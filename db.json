{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/images/demo_all_in.png","path":"images/demo_all_in.png","modified":1,"renderable":0},{"_id":"source/images/demo_greedy_fails.png","path":"images/demo_greedy_fails.png","modified":1,"renderable":0},{"_id":"source/images/demo_greedy_vs_km.png","path":"images/demo_greedy_vs_km.png","modified":1,"renderable":0},{"_id":"source/images/demo_timegap.png","path":"images/demo_timegap.png","modified":1,"renderable":0},{"_id":"source/images/density_distribute.png","path":"images/density_distribute.png","modified":1,"renderable":0},{"_id":"source/images/tsm_order.png","path":"images/tsm_order.png","modified":1,"renderable":0},{"_id":"source/images/tsm_wait.png","path":"images/tsm_wait.png","modified":1,"renderable":0},{"_id":"source/images/imitation_learning_paper/equation13.png","path":"images/imitation_learning_paper/equation13.png","modified":1,"renderable":0},{"_id":"source/images/imitation_learning_paper/title.jpg","path":"images/imitation_learning_paper/title.jpg","modified":1,"renderable":0},{"_id":"source/images/imitation_learning_paper/aggreg_error.png","path":"images/imitation_learning_paper/aggreg_error.png","modified":1,"renderable":0},{"_id":"source/images/imitation_learning_paper/iclr_imitation_learning_title.png","path":"images/imitation_learning_paper/iclr_imitation_learning_title.png","modified":1,"renderable":0},{"_id":"source/images/multi_agent_conversation/title.png","path":"images/multi_agent_conversation/title.png","modified":1,"renderable":0},{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":1,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":1,"renderable":1},{"_id":"source/images/imitation_learning_paper/iclr_imitation_learning_1.png","path":"images/imitation_learning_paper/iclr_imitation_learning_1.png","modified":1,"renderable":0},{"_id":"source/images/imitation_learning_paper/third_title.png","path":"images/imitation_learning_paper/third_title.png","modified":1,"renderable":0},{"_id":"source/images/km_algorithm.png","path":"images/km_algorithm.png","modified":1,"renderable":0},{"_id":"source/images/imitation_learning_paper/algorithm_gail.png","path":"images/imitation_learning_paper/algorithm_gail.png","modified":1,"renderable":0},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/hook-duoshuo.js","path":"js/src/hook-duoshuo.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/exturl.js","path":"js/src/exturl.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/js.cookie.js","path":"js/src/js.cookie.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/scroll-cookie.js","path":"js/src/scroll-cookie.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","path":"lib/algolia-instant-search/instantsearch.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","path":"lib/canvas-ribbon/canvas-ribbon.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","path":"lib/canvas-nest/canvas-nest.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/LICENSE","path":"lib/fastclick/LICENSE","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/bower.json","path":"lib/fastclick/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/README.md","path":"lib/fastclick/README.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","path":"lib/jquery_lazyload/CONTRIBUTING.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","path":"lib/jquery_lazyload/jquery.lazyload.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","path":"lib/jquery_lazyload/README.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","path":"lib/jquery_lazyload/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","path":"lib/jquery_lazyload/jquery.scrollstop.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/three-waves.min.js","path":"lib/three/three-waves.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/bower.json","path":"lib/velocity/bower.json","modified":1,"renderable":1},{"_id":"source/images/imitation_learning_paper/algorithm_third.png","path":"images/imitation_learning_paper/algorithm_third.png","modified":1,"renderable":0},{"_id":"source/images/multi_agent_conversation/architecture.png","path":"images/multi_agent_conversation/architecture.png","modified":1,"renderable":0},{"_id":"source/images/multi_agent_conversation/tsne.png","path":"images/multi_agent_conversation/tsne.png","modified":1,"renderable":0},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":1,"renderable":1},{"_id":"source/images/online_distribute_result.png","path":"images/online_distribute_result.png","modified":1,"renderable":0},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","path":"lib/fancybox/source/blank.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","path":"lib/fancybox/source/fancybox_loading.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","path":"lib/fancybox/source/fancybox_loading@2x.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","path":"lib/fancybox/source/fancybox_overlay.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","path":"lib/fancybox/source/fancybox_sprite.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","path":"lib/fancybox/source/fancybox_sprite@2x.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","path":"lib/fancybox/source/jquery.fancybox.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","path":"lib/fancybox/source/jquery.fancybox.pack.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","path":"lib/fancybox/source/jquery.fancybox.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","path":"lib/fastclick/lib/fastclick.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","path":"lib/fastclick/lib/fastclick.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":1,"renderable":1},{"_id":"source/images/timespace_model_vis.png","path":"images/timespace_model_vis.png","modified":1,"renderable":0},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","path":"lib/fancybox/source/helpers/fancybox_buttons.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","path":"lib/fancybox/source/helpers/jquery.fancybox-media.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","path":"lib/font-awesome/fonts/FontAwesome.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","path":"lib/font-awesome/fonts/fontawesome-webfont.ttf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","path":"lib/algolia-instant-search/instantsearch.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/three.min.js","path":"lib/three/three.min.js","modified":1,"renderable":1},{"_id":"source/raw_images/distribute1.pptx","path":"raw_images/distribute1.pptx","modified":1,"renderable":0},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","path":"lib/font-awesome/fonts/fontawesome-webfont.svg","modified":1,"renderable":1},{"_id":"source/blog_posts_and_images.zip","path":"blog_posts_and_images.zip","modified":1,"renderable":0}],"Cache":[{"_id":"source/.DS_Store","hash":"09ccc9d64de1dce6d7fd92e8ba49f0466a85e9d6","modified":1492259480000},{"_id":"source/_drafts/didi-distribute1.md","hash":"bad4ee8adfa3c8e266272ac622e7d2d0a8f235d3","modified":1492097394000},{"_id":"source/_drafts/didi-distribute2.md","hash":"01d169b19f9aef907af230792f53458c54aaa455","modified":1492097394000},{"_id":"source/_drafts/didi-distribute3.md","hash":"be6c82e6f9683f74d2e33c38d9a53571534983aa","modified":1492097394000},{"_id":"themes/next/.DS_Store","hash":"7f6083338524f77a6c25f4dc7895a8aaa31b969a","modified":1492000342000},{"_id":"source/_drafts/hello-world.md","hash":"2eaf889dc7a8f6b28520bd449ea5bc9ddb796cc8","modified":1492097394000},{"_id":"themes/next/.bowerrc","hash":"3228a58ed0ece9f85e1e3136352094080b8dece1","modified":1485336387000},{"_id":"themes/next/.editorconfig","hash":"792fd2bd8174ece1a75d5fd24ab16594886f3a7f","modified":1485336387000},{"_id":"themes/next/.gitattributes","hash":"44bd4729c74ccb88110804f41746fec07bf487d4","modified":1492099007000},{"_id":"themes/next/.hound.yml","hash":"b76daa84c9ca3ad292c78412603370a367cc2bc3","modified":1485336387000},{"_id":"themes/next/.gitignore","hash":"5f09fca02e030b7676c1d312cd88ce8fbccf381c","modified":1485336387000},{"_id":"themes/next/.javascript_ignore","hash":"f9ea3c5395f8feb225a24e2c32baa79afda30c16","modified":1485336387000},{"_id":"themes/next/.jshintrc","hash":"9928f81bd822f6a8d67fdbc909b517178533bca9","modified":1485336387000},{"_id":"themes/next/.stylintrc","hash":"b28e24704a5d8de08346c45286574c8e76cc109f","modified":1492099007000},{"_id":"themes/next/LICENSE","hash":"f293bcfcdc06c0b77ba13570bb8af55eb5c059fd","modified":1492099007000},{"_id":"themes/next/.travis.yml","hash":"c42d9608c8c7fe90de7b1581a8dc3886e90c179e","modified":1492099007000},{"_id":"themes/next/README.en.md","hash":"4ece25ee5f64447cd522e54cb0fffd9a375f0bd4","modified":1492099007000},{"_id":"themes/next/README.md","hash":"500b5606eb6a09c979d16128f8b00f4bf9bc95ac","modified":1485336387000},{"_id":"themes/next/_config.yml","hash":"4729f167ea79f2db26b6f16c466437f7aa486478","modified":1492255101000},{"_id":"themes/next/bower.json","hash":"5abc236d9cc2512f5457ed57c1fba76669eb7399","modified":1485336387000},{"_id":"themes/next/gulpfile.coffee","hash":"031bffc483e417b20e90eceb6cf358e7596d2e69","modified":1492099007000},{"_id":"themes/next/package.json","hash":"7e87b2621104b39a30488654c2a8a0c6a563574b","modified":1492099007000},{"_id":"source/_posts/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1492099825000},{"_id":"source/_posts/iclr17-multi-agent-conversation.md","hash":"8a1c1e30162cc6626fc1b6133b2402a158d31bf3","modified":1492264659000},{"_id":"source/_posts/iclr17-imitation-learning.md","hash":"8a363682bcfba49e2ebc079ef4f4e79876d10b7f","modified":1492255354000},{"_id":"source/about/index.md","hash":"7c6f9cd88836cd0ef6704801df51c68be3686964","modified":1492255289000},{"_id":"source/categories/index.md","hash":"fafb5c9f19e307b84299366d8f9b76188cbef1dc","modified":1492097394000},{"_id":"source/images/.DS_Store","hash":"865c02004389b1a73293a30fb60635a0320587d6","modified":1492259475000},{"_id":"source/images/demo_all_in.png","hash":"ef60d6ce9308f559afa461c109fd2c70a44e0c2d","modified":1492097394000},{"_id":"source/images/demo_greedy_fails.png","hash":"72e8ccba8fb9d705825eca5eb58301885e4b2a58","modified":1492097394000},{"_id":"source/images/demo_greedy_vs_km.png","hash":"beb7f010c2e347743f0ff3ccc06f086241116f83","modified":1492097394000},{"_id":"source/images/demo_timegap.png","hash":"4ad4381d25d04a9d92e313f5c73690e74c088fbc","modified":1492097394000},{"_id":"source/images/density_distribute.png","hash":"286cdcf7a4005f46bdfa79c1451f8e45818db1a6","modified":1492097394000},{"_id":"source/tags/index.md","hash":"3ddf43b71abb62e08ecd84cc3b4b4495a74dff6b","modified":1492097395000},{"_id":"themes/next/.git/FETCH_HEAD","hash":"de5238dd8a7926d54c15625385cb11e916fdb0be","modified":1492099007000},{"_id":"source/images/tsm_order.png","hash":"a0952e22c994dd5ff855c9981f48e09fb3307be5","modified":1492097395000},{"_id":"themes/next/.git/ORIG_HEAD","hash":"93f16a9f8033305545f80c1f65ec58c2bd38e6df","modified":1492099007000},{"_id":"themes/next/.git/description","hash":"9635f1b7e12c045212819dd934d809ef07efa2f4","modified":1485336371000},{"_id":"themes/next/.git/HEAD","hash":"acbaef275e46a7f14c1ef456fff2c8bbe8c84724","modified":1485336387000},{"_id":"themes/next/.git/COMMIT_EDITMSG","hash":"35daede4aafb0087f61abe6a55249a6fe4b1c4a7","modified":1492099212000},{"_id":"themes/next/.git/config","hash":"93ec11d93d9cf353c8725acf945588204b6be1d7","modified":1492099308000},{"_id":"themes/next/.git/index","hash":"b1ade032a10d6f7170a7f107f7aeab1a7a69b45c","modified":1492099212000},{"_id":"themes/next/.git/packed-refs","hash":"51b1e5920d5811d00d013083db36ed08a508f255","modified":1492099299000},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"3b5eafd32abb718e56ccf8d1cee0607ad8ce611d","modified":1492099007000},{"_id":"themes/next/.github/ISSUE_TEMPLATE.md","hash":"fdd63b77472612337309eb93ec415a059b90756b","modified":1492099007000},{"_id":"source/images/tsm_wait.png","hash":"e40802c07aad31748b4224b410c04e9ddee7456d","modified":1492097395000},{"_id":"themes/next/languages/default.yml","hash":"4cc6aeb1ac09a58330e494c8771773758ab354af","modified":1492099007000},{"_id":"themes/next/languages/de.yml","hash":"306db8c865630f32c6b6260ade9d3209fbec8011","modified":1492099007000},{"_id":"themes/next/languages/en.yml","hash":"e7def07a709ef55684490b700a06998c67f35f39","modified":1492099007000},{"_id":"themes/next/languages/fr-FR.yml","hash":"24180322c83587a153cea110e74e96eacc3355ad","modified":1492099007000},{"_id":"themes/next/languages/id.yml","hash":"2835ea80dadf093fcf47edd957680973f1fb6b85","modified":1492099007000},{"_id":"themes/next/languages/ja.yml","hash":"1c3a05ab80a6f8be63268b66da6f19da7aa2c638","modified":1492099007000},{"_id":"themes/next/languages/ko.yml","hash":"be150543379150f78329815af427bf152c0e9431","modified":1492099007000},{"_id":"themes/next/languages/pt-BR.yml","hash":"958e49571818a34fdf4af3232a07a024050f8f4e","modified":1492099007000},{"_id":"themes/next/languages/pt.yml","hash":"36c8f60dacbe5d27d84d0e0d6974d7679f928da0","modified":1492099007000},{"_id":"themes/next/languages/ru.yml","hash":"1549a7c2fe23caa7cbedcd0aa2b77c46e57caf27","modified":1492099007000},{"_id":"themes/next/languages/zh-Hans.yml","hash":"3c0c7dfd0256457ee24df9e9879226c58cb084b5","modified":1492099007000},{"_id":"themes/next/languages/zh-hk.yml","hash":"1c917997413bf566cb79e0975789f3c9c9128ccd","modified":1492099007000},{"_id":"themes/next/languages/zh-tw.yml","hash":"0b2c18aa76570364003c8d1cd429fa158ae89022","modified":1492099007000},{"_id":"themes/next/layout/_layout.swig","hash":"9d1a23a6add6f3d0f88c2d17979956f14aaa37a4","modified":1492099007000},{"_id":"themes/next/layout/archive.swig","hash":"b5b59d70fc1563f482fa07afd435752774ad5981","modified":1485336387000},{"_id":"themes/next/layout/category.swig","hash":"6422d196ceaff4220d54b8af770e7e957f3364ad","modified":1485336387000},{"_id":"themes/next/layout/index.swig","hash":"427d0b95b854e311ae363088ab39a393bf8fdc8b","modified":1485336387000},{"_id":"themes/next/layout/page.swig","hash":"3727fab9dadb967e9c2204edca787dc72264674a","modified":1485336387000},{"_id":"themes/next/layout/post.swig","hash":"e2e512142961ddfe77eba29eaa88f4a2ee43ae18","modified":1485336387000},{"_id":"themes/next/layout/schedule.swig","hash":"234dc8c3b9e276e7811c69011efd5d560519ef19","modified":1492099007000},{"_id":"themes/next/layout/tag.swig","hash":"07cf49c49c39a14dfbe9ce8e7d7eea3d4d0a4911","modified":1485336387000},{"_id":"themes/next/source/.DS_Store","hash":"d316b96fc793f4b24392a823ff8fe1927014291a","modified":1492000342000},{"_id":"themes/next/scripts/merge.js","hash":"9130dabe6a674c54b535f322b17d75fe6081472f","modified":1492099007000},{"_id":"themes/next/scripts/merge-configs.js","hash":"13c8b3a2d9fce06c2488820d9248d190c8100e0a","modified":1492099007000},{"_id":"themes/next/test/.jshintrc","hash":"19f93d13d1689fe033c82eb2d5f3ce30b6543cc0","modified":1485336387000},{"_id":"themes/next/test/helpers.js","hash":"a1f5de25154c3724ffc24a91ddc576cdbd60864f","modified":1485336387000},{"_id":"themes/next/test/intern.js","hash":"11fa8a4f5c3b4119a179ae0a2584c8187f907a73","modified":1485336387000},{"_id":"source/images/imitation_learning_paper/equation13.png","hash":"3ca632589b6b934d42a307228d4ad8384f05b3a9","modified":1492097394000},{"_id":"source/images/imitation_learning_paper/title.jpg","hash":"149bdd25f41676ef31c7a5f9ddc1023d62b038c4","modified":1492097394000},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1485336387000},{"_id":"source/images/imitation_learning_paper/aggreg_error.png","hash":"8f3148e70c0561f77b0d7e6b0f47291785401377","modified":1492097394000},{"_id":"source/images/imitation_learning_paper/iclr_imitation_learning_title.png","hash":"6d7c093a7f4b10f1f4071a31d780601822b11c92","modified":1492097394000},{"_id":"source/images/multi_agent_conversation/title.png","hash":"be2fe7d0f657b18b5bb371ba4df01c37327672cd","modified":1492259435000},{"_id":"themes/next/.git/hooks/commit-msg.sample","hash":"ee1ed5aad98a435f2020b6de35c173b75d9affac","modified":1485336371000},{"_id":"themes/next/.git/hooks/applypatch-msg.sample","hash":"4de88eb95a5e93fd27e78b5fb3b5231a8d8917dd","modified":1485336371000},{"_id":"themes/next/.git/hooks/post-update.sample","hash":"b614c2f63da7dca9f1db2e7ade61ef30448fc96c","modified":1485336371000},{"_id":"themes/next/.git/hooks/pre-applypatch.sample","hash":"f208287c1a92525de9f5462e905a9d31de1e2d75","modified":1485336371000},{"_id":"themes/next/.git/hooks/pre-commit.sample","hash":"36aed8976dcc08b5076844f0ec645b18bc37758f","modified":1485336371000},{"_id":"themes/next/.git/hooks/pre-push.sample","hash":"5c8518bfd1d1d3d2c1a7194994c0a16d8a313a41","modified":1485336371000},{"_id":"themes/next/.git/hooks/pre-rebase.sample","hash":"5885a56ab4fca8075a05a562d005e922cde9853b","modified":1485336371000},{"_id":"themes/next/.git/hooks/pre-receive.sample","hash":"705a17d259e7896f0082fe2e9f2c0c3b127be5ac","modified":1485336371000},{"_id":"themes/next/.git/hooks/prepare-commit-msg.sample","hash":"2b6275eda365cad50d167fe3a387c9bc9fedd54f","modified":1485336371000},{"_id":"themes/next/.git/hooks/update.sample","hash":"e729cd61b27c128951d139de8e7c63d1a3758dde","modified":1485336371000},{"_id":"themes/next/.git/info/exclude","hash":"c879df015d97615050afa7b9641e3352a1e701ac","modified":1485336371000},{"_id":"themes/next/.git/logs/HEAD","hash":"7e23118da18bef20095cbbc0d9aa18ef65adc5cf","modified":1492099212000},{"_id":"themes/next/layout/_custom/header.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1485336387000},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1485336387000},{"_id":"themes/next/layout/_macro/post-copyright.swig","hash":"b16fcbf0efd20c018d7545257a8533c497ea7647","modified":1492099007000},{"_id":"themes/next/layout/_macro/reward.swig","hash":"37e5b7c42ec17b9b6b786c5512bcc481a21c974e","modified":1485336387000},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"14e785adeb0e671ba0ff9a553e6f0d8def6c670c","modified":1485336387000},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"5864f5567ba5efeabcf6ea355013c0b603ee07f2","modified":1485336387000},{"_id":"themes/next/layout/_macro/post.swig","hash":"640b431eccbbd27f10c6781f33db5ea9a6e064de","modified":1492099007000},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"911b99ba0445b2c07373128d87a4ef2eb7de341a","modified":1492099007000},{"_id":"themes/next/layout/_partials/comments.swig","hash":"1c7d3c975e499b9aa3119d6724b030b7b00fc87e","modified":1492099007000},{"_id":"themes/next/layout/_partials/footer.swig","hash":"7172c6053118b7c291a56a7860128a652ae66b83","modified":1485336387000},{"_id":"themes/next/layout/_partials/head.swig","hash":"a0eafe24d1dae30c790ae35612154b3ffbbd5cce","modified":1492099007000},{"_id":"themes/next/layout/_partials/header.swig","hash":"a1ffbb691dfad3eaf2832a11766e58a179003b8b","modified":1492099007000},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"03aaebe9d50f6acb007ec38cc04acd1cfceb404d","modified":1485336387000},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"9e8e21d194ef44d271b1cca0bc1448c14d7edf4f","modified":1485336387000},{"_id":"themes/next/layout/_partials/page-header.swig","hash":"1efd925d34a5d4ba2dc0838d9c86ba911e705fc9","modified":1492099007000},{"_id":"themes/next/layout/_partials/search.swig","hash":"9dbd378e94abfcb3f864a5b8dbbf18d212ca2ee0","modified":1492099007000},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"766b2bdda29523ed6cd8d7aa197f996022f8fd94","modified":1485336387000},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"55491984964175176e054661a7326e85fa83a7a0","modified":1492099007000},{"_id":"themes/next/layout/_third-party/duoshuo-hot-articles.swig","hash":"5d4638c46aef65bf32a01681495b62416ccc98db","modified":1492099007000},{"_id":"themes/next/layout/_third-party/exturl.swig","hash":"7c04a42319d728be356746363aff8ea247791d24","modified":1492099007000},{"_id":"themes/next/layout/_third-party/mathjax.swig","hash":"6d25596d6a7c57700d37b607f8d9a62d89708683","modified":1492099007000},{"_id":"themes/next/layout/_third-party/scroll-cookie.swig","hash":"1ddb2336a1a19b47af3017047012c01ec5f54529","modified":1492099007000},{"_id":"themes/next/layout/_third-party/schedule.swig","hash":"22369026c87fc23893c35a7f250b42f3bb1b60f1","modified":1492099007000},{"_id":"themes/next/source/css/.DS_Store","hash":"cdfb1be57edb6f2445cba3a69b5aafb318350869","modified":1492000350000},{"_id":"themes/next/source/css/main.styl","hash":"20702c48d6053c92c5bcdbc68e8d0ef1369848a0","modified":1485336387000},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"90035272fa31a3f65b3c0e2cb8a633876ef457dc","modified":1485336387000},{"_id":"themes/next/source/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1485336387000},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1485336387000},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1485336387000},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1485336387000},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1485336387000},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1485336387000},{"_id":"themes/next/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1485336387000},{"_id":"themes/next/source/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1485336387000},{"_id":"themes/next/source/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1485336387000},{"_id":"themes/next/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1485336387000},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1485336387000},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1485336387000},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1485336387000},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"535fc542781021c4326dec24d8495cbb1387634a","modified":1485336387000},{"_id":"themes/next/scripts/tags/button.js","hash":"62e6dbeb53d07627a048132c79630b45d9a8f2cc","modified":1485336387000},{"_id":"themes/next/scripts/tags/exturl.js","hash":"8d7e60f60779bde050d20fd76f6fdc36fc85e06d","modified":1492099007000},{"_id":"themes/next/scripts/tags/full-image.js","hash":"8eeb3fb89540299bdbb799edfdfdac3743b50596","modified":1485336387000},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"49252824cd53184dc9b97b2f2d87ff28e1b3ef27","modified":1485336387000},{"_id":"themes/next/scripts/tags/note.js","hash":"6752925eedbdb939d8ec4d11bdfb75199f18dd70","modified":1485336387000},{"_id":"source/images/imitation_learning_paper/iclr_imitation_learning_1.png","hash":"19dc00bef87f4b4e77db5d31f7641edd61ad22af","modified":1492097394000},{"_id":"source/images/imitation_learning_paper/third_title.png","hash":"56b74040d56932626f897a4b22fb6c7b6a632ab3","modified":1492097394000},{"_id":"source/images/km_algorithm.png","hash":"786a3292ece7c7bfde5fd478c852a636ea3362d7","modified":1492097394000},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1485336387000},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1485336387000},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1485336387000},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1485336387000},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1485336387000},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1485336387000},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1485336387000},{"_id":"source/images/imitation_learning_paper/algorithm_gail.png","hash":"b46d294e1c61fd6253fbfbfc831dd15681bff1b2","modified":1492097394000},{"_id":"themes/next/.git/objects/13/5224e5a3fb4d930586225903daec5229d3692b","hash":"512c135587fbe456a11b1e0b661065e3e6365a4d","modified":1492098786000},{"_id":"themes/next/.git/objects/1a/7f049c17b21f7a6864b3ac48a13878812ddd2e","hash":"27906a862d3e0308fa8b9ad54404ba297db7aeec","modified":1492098786000},{"_id":"themes/next/.git/objects/1f/953aeebada7a9a8ba71d57889ba5c9cddec5c4","hash":"01c8c900843bdb2407dfeb5f9bced0c9f4461063","modified":1492098786000},{"_id":"themes/next/.git/objects/43/32d83200d4fc5c6524cba649c6e94176e51619","hash":"cffaca4f2724977a9887df47c5fb8ad03dd0e9c0","modified":1492098786000},{"_id":"themes/next/.git/objects/1e/ea2c4a749dccbf38e4da0ca442cee82063997e","hash":"c80d5dd2e9d0d89648442b1236efd9b8745345c7","modified":1492099212000},{"_id":"themes/next/.git/objects/49/e1e2122c44109ac68090e3c2e0e33980061f62","hash":"f6e518a75e31f1bf0b0f0e3e6d04cfc11c348f6d","modified":1492098771000},{"_id":"themes/next/.git/objects/54/67f9fbca8e3fc2a9e867506c34c3c2a222709b","hash":"0044f44783c746d77201e2527d2575ab2e6108cf","modified":1492098771000},{"_id":"themes/next/.git/objects/48/b76815990f78bed7dd9be8828f263f84cdb0f6","hash":"bda1266a724fc68a79baae47a9bc8537e3d919bf","modified":1492099212000},{"_id":"themes/next/.git/objects/5b/45bc0b5b4543905b0a3d1f7f790ebc50b8fe6b","hash":"5d93c5dc30c40383c514079753cdadcacd35dacd","modified":1492099007000},{"_id":"themes/next/.git/objects/79/fb1b76ff12937a246f7626cce346671e9f47d6","hash":"bbbf5ce902345c6c3523dd6d4331f8f5e94a145b","modified":1492099204000},{"_id":"themes/next/.git/objects/4a/b152d88314a159bec3a12e267abe6672da355d","hash":"6041676c079f13d4f9be8ec651dc6b4ba7240654","modified":1492099212000},{"_id":"themes/next/.git/objects/5d/cad34ed0ef26d7f58cf6bd020fc36f3afa08e6","hash":"671f07a902786206213f48dd83cc505025f471f9","modified":1492098786000},{"_id":"themes/next/.git/objects/87/6a47ea20cb09d3b6511b2af0cd15714eb94f35","hash":"8b0dbf4235a87309abbba7e12eda1945e055822f","modified":1492099212000},{"_id":"themes/next/.git/objects/7b/46d23fb0a06bab7866c6292de6696c38f8230f","hash":"835bec68ee2b9df38bcd7bf349dd1a4fc12f6576","modified":1492098771000},{"_id":"themes/next/.git/objects/8d/02a26f2e7175832bf8c2ed2b46efdd9c713efe","hash":"2e293ed472c87145217877612a7a9f0e76d89b05","modified":1492099212000},{"_id":"themes/next/.git/objects/93/394d447dedcafac5f26866aa324f8ddaba30e6","hash":"ef376c4eab6934527020a6c8276313a0fbf57d54","modified":1492098786000},{"_id":"themes/next/.git/objects/9e/8371b6ac1a01d342722437e0c0ad1e96b50032","hash":"3795ca480cfa0562b0c870be6b91c08d9f1095a0","modified":1492098771000},{"_id":"themes/next/.git/objects/bc/6824dec1ebd01d5a3d3e3dc5b495c2440957a2","hash":"67e0c53db08ffa0bee9771a986dc47b14021f77a","modified":1492099212000},{"_id":"themes/next/.git/objects/a7/15b6ed1bdb5ce9858adbfe981724f18a3da729","hash":"4544a0e8edc22a596c5ef91b559bf3adf8019b53","modified":1492099212000},{"_id":"themes/next/.git/objects/bf/2c6bba494de0acf5dbe0d3d0502b41128e4b19","hash":"c20d4c5d10d39d250ffbe007b78406535c9d4ecc","modified":1492098786000},{"_id":"themes/next/.git/objects/bf/e202deaf3a9a84cfe00702d7630c567e6f317a","hash":"fb6e2122be200b996ced34937ad6093ab9a16dc8","modified":1492099212000},{"_id":"themes/next/.git/objects/f7/5f3d4bb4d5744206228d0e642c7997225736c7","hash":"cb48ca7a6ea08efb567abd4bb39dfeaf6f0c9d40","modified":1492098786000},{"_id":"themes/next/.git/objects/eb/4cf4ebb4690cc911524ed5cf683128bb99e974","hash":"4edc0a8d7353acbb983053b5821e06236ffb953e","modified":1492099212000},{"_id":"themes/next/.git/objects/f6/c09a5f295b611be0485ebeaa50a3d3f36df74b","hash":"ee53b75bd2cc5d02e46275ddaab58ba4996d3265","modified":1492099212000},{"_id":"themes/next/.git/objects/fa/a27e57727a82e334fb5ec644c9d9895e0a98ac","hash":"f7de0891eb506a8073e0b7995ead3f2b18c2483c","modified":1492098786000},{"_id":"themes/next/.git/refs/tags/v5.1.1","hash":"3caf2cc30e2bc17ce7c8decb48064104d4845453","modified":1492099007000},{"_id":"themes/next/.git/refs/heads/master","hash":"1731d57daf6514caee50d49dc3487422aaf9e30f","modified":1492099212000},{"_id":"themes/next/.git/objects/pack/pack-da69df17c96c89b0b5fe139be1f04665859f5af8.idx","hash":"585c532da6f651ea60d827511d1412adee62ad64","modified":1492099007000},{"_id":"themes/next/layout/_partials/head/custom-head.swig","hash":"9e1b9666efa77f4cf8d8261bcfa445a9ac608e53","modified":1485336387000},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"7ce76358411184482bb0934e70037949dd0da8ca","modified":1485336387000},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"2d1075f4cabcb3956b7b84a8e210f5a66f0a5562","modified":1492099007000},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"959b7e04a96a5596056e4009b73b6489c117597e","modified":1485336387000},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"eefe2388ff3d424694045eda21346989b123977c","modified":1485336387000},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"23e23dc0f76ef3c631f24c65277adf7ea517b383","modified":1485336387000},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"1f1107468aaf03f7d0dcd7eb2b653e2813a675b4","modified":1485336387000},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"89c5a5240ecb223acfe1d12377df5562a943fd5d","modified":1485336387000},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"069d1357c717572256e5cdee09574ebce529cbae","modified":1485336387000},{"_id":"themes/next/layout/_partials/share/jiathis.swig","hash":"63315fcf210799f894208c9f512737096df84962","modified":1485336387000},{"_id":"themes/next/layout/_third-party/analytics/application-insights.swig","hash":"60426bf73f8a89ba61fb1be2df3ad5398e32c4ef","modified":1492099007000},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"deda6a814ed48debc694c4e0c466f06c127163d0","modified":1492099007000},{"_id":"themes/next/layout/_third-party/analytics/busuanzi-counter.swig","hash":"18e7bef8923d83ea42df6c97405e515a876cede4","modified":1492099007000},{"_id":"themes/next/layout/_third-party/analytics/cnzz-analytics.swig","hash":"8160b27bee0aa372c7dc7c8476c05bae57f58d0f","modified":1492099007000},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1485336387000},{"_id":"themes/next/layout/_third-party/analytics/facebook-sdk.swig","hash":"394d008e5e94575280407ad8a1607a028026cbc3","modified":1492099007000},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"5d9943d74cc2e0a91badcf4f755c6de77eab193a","modified":1492099007000},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"3358d11b9a26185a2d36c96049e4340e701646e4","modified":1492099007000},{"_id":"themes/next/layout/_third-party/analytics/lean-analytics.swig","hash":"92dc60821307fc9769bea9b2d60adaeb798342af","modified":1492099007000},{"_id":"themes/next/layout/_third-party/analytics/tencent-analytics.swig","hash":"3658414379e0e8a34c45c40feadc3edc8dc55f88","modified":1492099007000},{"_id":"themes/next/layout/_third-party/analytics/tencent-mta.swig","hash":"a652f202bd5b30c648c228ab8f0e997eb4928e44","modified":1492099007000},{"_id":"themes/next/layout/_third-party/analytics/vkontakte-api.swig","hash":"c3971fd154d781088e1cc665035f8561a4098f4c","modified":1492099007000},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"0e3378f7c39b2b0f69638290873ede6b6b6825c0","modified":1492099007000},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"c316758546dc9ba6c60cb4d852c17ca6bb6d6724","modified":1492099007000},{"_id":"themes/next/layout/_third-party/comments/duoshuo.swig","hash":"a356b2185d40914447fde817eb3d358ab6b3e4c3","modified":1492099007000},{"_id":"themes/next/layout/_third-party/comments/gentie.swig","hash":"03592d1d731592103a41ebb87437fe4b0a4c78ca","modified":1492099007000},{"_id":"themes/next/layout/_third-party/comments/hypercomments.swig","hash":"3e8dc5c6c912628a37e3b5f886bec7b2e5ed14ea","modified":1492099007000},{"_id":"themes/next/layout/_third-party/comments/index.swig","hash":"abb92620197a16ed2c0775edf18a0f044a82256e","modified":1492099007000},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"7240f2e5ec7115f8abbbc4c9ef73d4bed180fdc7","modified":1492099007000},{"_id":"themes/next/layout/_third-party/comments/youyan.swig","hash":"af9dd8a4aed7d06cf47b363eebff48850888566c","modified":1492099007000},{"_id":"themes/next/layout/_third-party/search/index.swig","hash":"c747fb5c6b1f500e8f0c583e44195878b66e4e29","modified":1492099007000},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"f4dbd4c896e6510ded8ebe05394c28f8a86e71bf","modified":1492099007000},{"_id":"themes/next/layout/_third-party/search/tinysou.swig","hash":"cb3a5d36dbe1630bab84e03a52733a46df7c219b","modified":1492099007000},{"_id":"themes/next/layout/_third-party/seo/baidu-push.swig","hash":"c057b17f79e8261680fbae8dc4e81317a127c799","modified":1492099007000},{"_id":"themes/next/source/css/_common/.DS_Store","hash":"e8768990ebc37a57860342f69b59c85f5cf5edd0","modified":1492000143000},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"c18e66ada72a21a8d1fcd8f4fc9d740952c6a68c","modified":1485778294000},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"78a83c38f69a8747bb74e420e6c9eeef1ea76525","modified":1485336387000},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"c8d35a6b9e3bff6d8fdb66de853065af9d37562d","modified":1485336387000},{"_id":"themes/next/source/css/_variables/base.styl","hash":"28a7f84242ca816a6452a0a79669ca963d824607","modified":1492099007000},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"715d5b40dc52f319fe4bff0325beb874774d9bd9","modified":1485336387000},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"06f432f328a5b8a9ef0dbd5301b002aba600b4ce","modified":1492099007000},{"_id":"themes/next/source/css/_schemes/.DS_Store","hash":"459c75ee776f556510fafc33db2843a4f743c935","modified":1492000372000},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"aab7be0a6e2724b3faa9338db93c19556c559625","modified":1492099007000},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1492099007000},{"_id":"themes/next/source/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1485336387000},{"_id":"themes/next/source/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1485336387000},{"_id":"themes/next/source/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1492099007000},{"_id":"themes/next/source/js/src/motion.js","hash":"269414e84df544a4ccb88519f6abae4943db3c67","modified":1485336387000},{"_id":"themes/next/source/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1492099007000},{"_id":"themes/next/source/js/src/post-details.js","hash":"af7a417dd1cb02465a7b98211653e7c6192e6d55","modified":1492099007000},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1485336387000},{"_id":"themes/next/source/js/src/utils.js","hash":"2041eae5ac81c392067bfbd1bba4399cb37beea1","modified":1492099007000},{"_id":"themes/next/source/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1492099007000},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1485336387000},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","hash":"b614b73ba49ca9136fed7c387ee8df72fe65118f","modified":1492099007000},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1485336387000},{"_id":"themes/next/source/lib/fancybox/.gitattributes","hash":"2db21acfbd457452462f71cc4048a943ee61b8e0","modified":1485336387000},{"_id":"themes/next/source/lib/fancybox/.bower.json","hash":"cc40a9b11e52348e554c84e4a5c058056f6b7aeb","modified":1485336387000},{"_id":"themes/next/source/lib/fastclick/.bower.json","hash":"93ebd5b35e632f714dcf1753e1f6db77ec74449b","modified":1485336387000},{"_id":"themes/next/source/lib/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1485336387000},{"_id":"themes/next/source/lib/jquery/.bower.json","hash":"91745c2cc6c946c7275f952b2b0760b880cea69e","modified":1485336387000},{"_id":"themes/next/source/lib/fastclick/bower.json","hash":"13379463c7463b4b96d13556b46faa4cc38d81e6","modified":1485336387000},{"_id":"themes/next/source/lib/fastclick/README.md","hash":"1decd8e1adad2cd6db0ab50cf56de6035156f4ea","modified":1485336387000},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"a2aaaf12378db56bd10596ba3daae30950eac051","modified":1492099007000},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"69d152fa46b517141ec3b1114dd6134724494d83","modified":1485336387000},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"dcf470ab3a358103bb896a539cc03caeda10fa8b","modified":1485336387000},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1485336387000},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","hash":"4891864c24c28efecd81a6a8d3f261145190f901","modified":1485336387000},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"279a8a718ab6c930a67c41237f0aac166c1b9440","modified":1485336387000},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1485336387000},{"_id":"themes/next/source/lib/jquery_lazyload/.bower.json","hash":"b7638afc93e9cd350d0783565ee9a7da6805ad8e","modified":1485336387000},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","hash":"895d50fa29759af7835256522e9dd7dac597765c","modified":1485336387000},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","hash":"65bc85d12197e71c40a55c0cd7f6823995a05222","modified":1485336387000},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1485336387000},{"_id":"themes/next/source/lib/three/three-waves.min.js","hash":"5b38ae00297ffc07f433c632c3dbf7bde4cdf39a","modified":1492099007000},{"_id":"themes/next/source/lib/velocity/.bower.json","hash":"05f960846f1c7a93dab1d3f9a1121e86812e8c88","modified":1485336387000},{"_id":"themes/next/source/lib/velocity/bower.json","hash":"2ec99573e84c7117368beccb9e94b6bf35d2db03","modified":1485336387000},{"_id":"source/images/imitation_learning_paper/algorithm_third.png","hash":"31cbbc7149f128571af1a3fed35e70ec0f9ca2e2","modified":1492097394000},{"_id":"source/images/multi_agent_conversation/architecture.png","hash":"a038ea2e09224bd1dcf68f74fe54d542e580d2f3","modified":1492262233000},{"_id":"source/images/multi_agent_conversation/tsne.png","hash":"a2c449650aabe4b7fcd8cb69636808ad07ad9d9c","modified":1492264173000},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1485336387000},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1485336387000},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1485336387000},{"_id":"themes/next/source/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1485336387000},{"_id":"source/images/online_distribute_result.png","hash":"ffcdc08ac57d163ae8132cba6d361a4f73d64d86","modified":1492097395000},{"_id":"themes/next/.git/logs/refs/heads/master","hash":"7e23118da18bef20095cbbc0d9aa18ef65adc5cf","modified":1492099212000},{"_id":"themes/next/.git/refs/remotes/upstream/HEAD","hash":"c6c0de63e788b3413f1822351289da67f50b159c","modified":1492099299000},{"_id":"themes/next/.git/refs/remotes/upstream/servant","hash":"ee903ee2112907f4498fdb13674a640c685e95fb","modified":1492099299000},{"_id":"themes/next/.git/refs/remotes/upstream/dev","hash":"41ab21bbe0d88c2cc439c0af33807af4e95aef31","modified":1492099299000},{"_id":"themes/next/.git/refs/remotes/upstream/master","hash":"481fa7b8f2c4ee5550cfc828764354aecad0325d","modified":1492099299000},{"_id":"themes/next/.git/refs/remotes/upstream/testing","hash":"15660a56eaab85fc3fd3324de8d61dbc2b09b42a","modified":1492099299000},{"_id":"themes/next/.git/refs/remotes/origin/master","hash":"1731d57daf6514caee50d49dc3487422aaf9e30f","modified":1492099308000},{"_id":"themes/next/layout/_third-party/search/algolia-search/dom.swig","hash":"ba698f49dd3a868c95b240d802f5b1b24ff287e4","modified":1492099007000},{"_id":"themes/next/layout/_third-party/search/algolia-search/assets.swig","hash":"28ff4ed6714c59124569ffcbd10f1173d53ca923","modified":1492099007000},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"ef089a407c90e58eca10c49bc47ec978f96e03ba","modified":1492099007000},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"0dfb4b3ba3180d7285e66f270e1d3fa0f132c3d2","modified":1485336387000},{"_id":"themes/next/source/css/_common/components/.DS_Store","hash":"90f586737b2776bdaf3ef73bc2a203fe82dc7d26","modified":1492000152000},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"471f1627891aca5c0e1973e09fbcb01e1510d193","modified":1485336387000},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"a6bb5256be6195e76addbda12f4ed7c662d65e7a","modified":1492099007000},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"59ad08bcc6fe9793594869ac2b4c525021453e78","modified":1492099007000},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"711c8830886619d4f4a0598b0cde5499dce50c62","modified":1485336387000},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"dd8a3b22fc2f222ac6e6c05bd8a773fb039169c0","modified":1485336387000},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"2186be20e317505cd31886f1291429cc21f76703","modified":1485336387000},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"7804e31c44717c9a9ddf0f8482b9b9c1a0f74538","modified":1492099007000},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"9c25c75311e1bd4d68df031d3f2ae6d141a90766","modified":1492099007000},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"ece571f38180febaf02ace8187ead8318a300ea7","modified":1485336387000},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"013619c472c7e4b08311c464fcbe9fcf5edde603","modified":1485336387000},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"64f5d56c08d74a338813df1265580ca0cbf0190b","modified":1485336387000},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"5ae7906dc7c1d9468c7f4b4a6feddddc555797a1","modified":1485336387000},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"c2d079788d6fc2e9a191ccdae94e50d55bf849dc","modified":1485336387000},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"38e5df90c8689a71c978fd83ba74af3d4e4e5386","modified":1485336387000},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"b0dcca862cd0cc6e732e33d975b476d744911742","modified":1485336387000},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"95b8e930e69486c609aa8ad804a246c285f54c09","modified":1492000378000},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1485336387000},{"_id":"themes/next/source/css/_schemes/Mist/.DS_Store","hash":"2a14cf6071caab3b9c8615f07c030df59230681a","modified":1485776434000},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"9a5581a770af8964064fef7afd3e16963e45547f","modified":1485336387000},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1485336387000},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"0efa036a15c18f5abb058b7c0fad1dd9ac5eed4c","modified":1485336387000},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"8829bc556ca38bfec4add4f15a2f028092ac6d46","modified":1485336387000},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"82bbaa6322764779a1ac2e2c8390ce901c7972e2","modified":1485336387000},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"a0e2030a606c934fb2c5c7373aaae04a1caac4c5","modified":1485336387000},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"c4ed249798296f60bda02351fe6404fb3ef2126f","modified":1492099007000},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"2f878213cb24c5ddc18877f6d15ec5c5f57745ac","modified":1485336387000},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"69ecd6c97e7cdfd822ac8102b45ad0ede85050db","modified":1485336387000},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"5b93958239d3d2bf9aeaede44eced2434d784462","modified":1492099007000},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"e3e23751d4ad24e8714b425d768cf68e37de7ded","modified":1492099007000},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"215de948be49bcf14f06d500cef9f7035e406a43","modified":1492099007000},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"79da92119bc246fe05d1626ac98426a83ec90a94","modified":1492099007000},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1485336387000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1485336387000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1485336387000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1485336387000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1485336387000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1485336387000},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1485336387000},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1485336387000},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1485336387000},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1485336387000},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1485336387000},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1492099007000},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1485336387000},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1492099007000},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1485336387000},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1485336387000},{"_id":"themes/next/.git/objects/pack/pack-70fa44f18a74f7390de80c3a31441723f4bc5273.idx","hash":"b4cf99299a1ffd0a310c392df8f4c5b4da218c50","modified":1485336387000},{"_id":"themes/next/.git/objects/pack/tmp_pack_fQ4jFa","hash":"e97eec3070242b5aae3e5a82dd6c76cf64259b51","modified":1492098879000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1492099007000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1492099007000},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1485336387000},{"_id":"source/images/timespace_model_vis.png","hash":"65878481c964e19812abe0d2d9f9d9e209771286","modified":1492097395000},{"_id":"themes/next/.git/logs/refs/remotes/origin/master","hash":"a1c5f4277f23a4bd2aa9a91871e58eafea8c672f","modified":1492099308000},{"_id":"themes/next/.git/logs/refs/remotes/upstream/dev","hash":"e80a4c88cdca67f62e00236b2ea88c001c3f80d2","modified":1492099299000},{"_id":"themes/next/.git/logs/refs/remotes/upstream/master","hash":"39536cff78e834833aa113013546d95f9803c51c","modified":1492099299000},{"_id":"themes/next/.git/logs/refs/remotes/upstream/testing","hash":"3697c83544bcd851b9071b1feaa131735c8c584c","modified":1492099299000},{"_id":"themes/next/.git/logs/refs/remotes/upstream/servant","hash":"52f29d6078a62c698608a0dc23a922198af723d9","modified":1492099299000},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"8994ffcce84deac0471532f270f97c44fea54dc0","modified":1485336387000},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"96f32ea6c3265a3889e6abe57587f6e2a2a40dfb","modified":1485336387000},{"_id":"themes/next/.git/logs/refs/remotes/upstream/HEAD","hash":"f4393c71cf65f93d776999bcc36b79339a7f2769","modified":1492099299000},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"740d37f428b8f4574a76fc95cc25e50e0565f45e","modified":1492099007000},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"b76387934fb6bb75212b23c1a194486892cc495e","modified":1485336387000},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"ae1ca14e51de67b07dba8f61ec79ee0e2e344574","modified":1485336387000},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d27448f199fc2f9980b601bc22b87f08b5d64dd1","modified":1485336387000},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"8a2421cb9005352905fae9d41a847ae56957247e","modified":1492099007000},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"6c00f6e0978f4d8f9a846a15579963728aaa6a17","modified":1485336387000},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"49c2b2c14a1e7fcc810c6be4b632975d0204c281","modified":1485336387000},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"7778920dd105fa4de3a7ab206eeba30b1a7bac45","modified":1485336387000},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"4eff5b252d7b614e500fc7d52c97ce325e57d3ab","modified":1485336387000},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"2039590632bba3943c39319d80ef630af7928185","modified":1485336387000},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"9bf4362a4d0ae151ada84b219d39fbe5bb8c790e","modified":1485336387000},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"a82afbb72d83ee394aedc7b37ac0008a9823b4f4","modified":1485336387000},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"beccb53dcd658136fb91a0c5678dea8f37d6e0b6","modified":1492099007000},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"8fae54591877a73dff0b29b2be2e8935e3c63575","modified":1485336387000},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"2cdc094ecf907a02fce25ad4a607cd5c40da0f2b","modified":1485336387000},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"c196401747019d389da09b7a0fe7f27e3a0ec01f","modified":1492099007000},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"387ce23bba52b22a586b2dfb4ec618fe1ffd3926","modified":1485336387000},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"b9a2e76f019a5941191f1263b54aef7b69c48789","modified":1492099007000},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"a5d8617a24d7cb6c5ad91ea621183ca2c0917331","modified":1492099007000},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"a352ae5b1f8857393bf770d2e638bf15f0c9585d","modified":1485336387000},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"f54367c0feda6986c030cc4d15a0ca6ceea14bcb","modified":1492099007000},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"963105a531403d7aad6d9e5e23e3bfabb8ec065a","modified":1485336387000},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"10251257aceecb117233c9554dcf8ecfef8e2104","modified":1485336387000},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"8c0276883398651336853d5ec0e9da267a00dd86","modified":1492099007000},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"e792c8dc41561c96d128e9b421187f1c3dc978a0","modified":1485336387000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"2e7ec9aaa3293941106b1bdd09055246aa3c3dc6","modified":1485336387000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"920343e41c124221a17f050bbb989494d44f7a24","modified":1485336387000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"5f6ea57aabfa30a437059bf8352f1ad829dbd4ff","modified":1492099007000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"9486ddd2cb255227db102d09a7df4cae0fabad72","modified":1485336387000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"45fa7193435a8eae9960267438750b4c9fa9587f","modified":1485336387000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"7690b9596ec3a49befbe529a5a2649abec0faf76","modified":1485336387000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"a2ec22ef4a6817bbb2abe8660fcd99fe4ca0cc5e","modified":1492099007000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"234facd038f144bd0fe09a31ed1357c5d74c517f","modified":1485336387000},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"3623e7fa4324ec1307370f33d8f287a9e20a5578","modified":1485336387000},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"e83e435abf3b823d85d0f9258dae1a5c766b8f29","modified":1485776651000},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"b8969e1654eec89a0fd10d88b337fee9cb03cd44","modified":1485336387000},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"4851b981020c5cbc354a1af9b831a2dcb3cf9d39","modified":1485336387000},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"74d0ba86f698165d13402670382a822c8736a556","modified":1485336387000},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"dd310c2d999185e881db007360176ee2f811df10","modified":1492099007000},{"_id":"themes/next/source/css/_common/components/third-party/baidushare.styl","hash":"93b08815c4d17e2b96fef8530ec1f1064dede6ef","modified":1485336387000},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"d4e6d8d7b34dc69994593c208f875ae8f7e8a3ae","modified":1492099007000},{"_id":"themes/next/source/css/_common/components/third-party/duoshuo.styl","hash":"2340dd9b3202c61d73cc708b790fac5adddbfc7f","modified":1485336387000},{"_id":"themes/next/source/css/_common/components/third-party/gentie.styl","hash":"586a3ec0f1015e7207cd6a2474362e068c341744","modified":1485336387000},{"_id":"themes/next/source/css/_common/components/tags/exturl.styl","hash":"1b3cc9f4e5a7f6e05b4100e9990b37b20d4a2005","modified":1492099007000},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"fd42777b9125fd8969dc39d4f15473e2b91b4142","modified":1492099007000},{"_id":"themes/next/source/css/_common/components/third-party/jiathis.styl","hash":"327b5f63d55ec26f7663185c1a778440588d9803","modified":1485336387000},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"173490e21bece35a34858e8e534cf86e34561350","modified":1492099007000},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"42348219db93a85d2ee23cb06cebd4d8ab121726","modified":1485336387000},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"5dc4859c66305f871e56cba78f64bfe3bf1b5f01","modified":1485336387000},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1492099007000},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1492099007000},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1485336387000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1485336387000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1485336387000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1485336387000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1485336387000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1485336387000},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1492099007000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1492099007000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1492099007000},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1485336387000},{"_id":"themes/next/source/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1492099007000},{"_id":"source/raw_images/distribute1.pptx","hash":"7998b0205f184b0ac6d5b4765c35c5cc4b1b09ab","modified":1492097395000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"98a8aa5cf7d62c2eff5f07ede8d844b874ef06ed","modified":1492099007000},{"_id":"themes/next/.git/objects/pack/pack-da69df17c96c89b0b5fe139be1f04665859f5af8.pack","hash":"882b62d97b4d4743847c6be4d50cc50618a0474a","modified":1492099212000},{"_id":"source/blog_posts_and_images.zip","hash":"34a6d6466b89baeab6c5f7c2e8a69c9d0d323239","modified":1492097394000},{"_id":"themes/next/.git/objects/pack/pack-70fa44f18a74f7390de80c3a31441723f4bc5273.pack","hash":"38a8ff9d0c26e00a6808a13fad2010aa0838e15e","modified":1492099212000}],"Category":[{"name":"didi","_id":"cj1jbutct00049qs61tk2nqqp"},{"name":"test","_id":"cj1jbutdq000i9qs6pcmgaz5c"},{"name":"drl","_id":"cj1jbutds000m9qs6qe54szej"}],"Data":[],"Page":[{"date":"2017-01-25T11:19:20.000Z","_content":"\n## 关于我\n  * 姓名：徐哲  \n  * 性别：男  \n  * 个性：听说是高冷+傲娇？总之一般人看起来还是一个异常认真、不太好交流的人吧。但其实很kind的=。=  \n  * 技能：世界上少数几个 __*鸟类专家*__ ，精通各种鸟（尤其是美国鸟）的识别😂\n\n![](http://image98.360doc.com/DownloadImg/2016/07/2905/76788528_141.gif)\n\n  * -- 博士期间主要是做了个鸟\\~\\~\\~类识别的工作，在细粒度识别(Fine-grained Visual Categorization)这个领域靠认鸟发了几篇paper，也算是正常毕业了。 当然了认一些其他的车啊人啊花啊草啊的也能勉强胜任，computer vision的其他recognition/detection/segmentation略知皮毛，不免俗的deep learning也玩，实验室的关系multimedia, data mining之类的也搞过一阵，还玩过一些图像艺术性分析这样的玄学，~~以上是吹逼~~。\n\n  * -- 最后是最近在看增强学习，我们在筹备着一些比较牛的东西，~~只不过不一定做的出来罢了~~┑(￣Д ￣)┍ 万一做出来了记得刚点个赞哈~\n\n  * Coding: 基本只熟悉python了，C++勉强写一些，其他会的但是现在用的少的像matlab, as3等等已经忘光了~  \n\n  * 休闲：目前最工作外常干的三件事就是 **看NBA，看ACG，和弹吉他（虽然很渣）** 基本各种娱乐活动来着不拒 \n \n  * 现状：于上交和悉尼科技大学博士毕业，目前在滴滴出行当算法攻城狮，主要负责分单系统的模型化 \n\n## 关于博客\n  * 这个Blog只负责记录我看的paper，之前做的一些工作和小实验，再加上现在准备在滴滴做的一些新东西。在可允许范围内我们会release一些滴滴可公开的数据，也请大家不要宣传->_->\n  * 此外我还会放一些之前依着个人恶趣味做的东西，不定期更新，请不要有所期待┑(￣Д ￣)┍\n\n---\n\n** 最近访问 ** \n\n<div class=\"ds-recent-visitors\"\n    data-num-items=\"36\"\n    data-avatar-size=\"42\"\n    id=\"ds-recent-visitors\">\n</div>\n\n\n","source":"about/index.md","raw":"---\n# title: about me\ndate: 2017-01-25 19:19:20\n---\n\n## 关于我\n  * 姓名：徐哲  \n  * 性别：男  \n  * 个性：听说是高冷+傲娇？总之一般人看起来还是一个异常认真、不太好交流的人吧。但其实很kind的=。=  \n  * 技能：世界上少数几个 __*鸟类专家*__ ，精通各种鸟（尤其是美国鸟）的识别😂\n\n![](http://image98.360doc.com/DownloadImg/2016/07/2905/76788528_141.gif)\n\n  * -- 博士期间主要是做了个鸟\\~\\~\\~类识别的工作，在细粒度识别(Fine-grained Visual Categorization)这个领域靠认鸟发了几篇paper，也算是正常毕业了。 当然了认一些其他的车啊人啊花啊草啊的也能勉强胜任，computer vision的其他recognition/detection/segmentation略知皮毛，不免俗的deep learning也玩，实验室的关系multimedia, data mining之类的也搞过一阵，还玩过一些图像艺术性分析这样的玄学，~~以上是吹逼~~。\n\n  * -- 最后是最近在看增强学习，我们在筹备着一些比较牛的东西，~~只不过不一定做的出来罢了~~┑(￣Д ￣)┍ 万一做出来了记得刚点个赞哈~\n\n  * Coding: 基本只熟悉python了，C++勉强写一些，其他会的但是现在用的少的像matlab, as3等等已经忘光了~  \n\n  * 休闲：目前最工作外常干的三件事就是 **看NBA，看ACG，和弹吉他（虽然很渣）** 基本各种娱乐活动来着不拒 \n \n  * 现状：于上交和悉尼科技大学博士毕业，目前在滴滴出行当算法攻城狮，主要负责分单系统的模型化 \n\n## 关于博客\n  * 这个Blog只负责记录我看的paper，之前做的一些工作和小实验，再加上现在准备在滴滴做的一些新东西。在可允许范围内我们会release一些滴滴可公开的数据，也请大家不要宣传->_->\n  * 此外我还会放一些之前依着个人恶趣味做的东西，不定期更新，请不要有所期待┑(￣Д ￣)┍\n\n---\n\n** 最近访问 ** \n\n<div class=\"ds-recent-visitors\"\n    data-num-items=\"36\"\n    data-avatar-size=\"42\"\n    id=\"ds-recent-visitors\">\n</div>\n\n\n","updated":"2017-04-15T11:21:29.000Z","path":"about/index.html","title":"","comments":1,"layout":"page","_id":"cj1jbutco00019qs6sah2ilzp","content":"<h2 id=\"关于我\"><a href=\"#关于我\" class=\"headerlink\" title=\"关于我\"></a>关于我</h2><ul>\n<li>姓名：徐哲  </li>\n<li>性别：男  </li>\n<li>个性：听说是高冷+傲娇？总之一般人看起来还是一个异常认真、不太好交流的人吧。但其实很kind的=。=  </li>\n<li>技能：世界上少数几个 <strong><em>鸟类专家</em></strong> ，精通各种鸟（尤其是美国鸟）的识别😂</li>\n</ul>\n<p><img src=\"http://image98.360doc.com/DownloadImg/2016/07/2905/76788528_141.gif\" alt=\"\"></p>\n<ul>\n<li><p>– 博士期间主要是做了个鸟~~~类识别的工作，在细粒度识别(Fine-grained Visual Categorization)这个领域靠认鸟发了几篇paper，也算是正常毕业了。 当然了认一些其他的车啊人啊花啊草啊的也能勉强胜任，computer vision的其他recognition/detection/segmentation略知皮毛，不免俗的deep learning也玩，实验室的关系multimedia, data mining之类的也搞过一阵，还玩过一些图像艺术性分析这样的玄学，<del>以上是吹逼</del>。</p>\n</li>\n<li><p>– 最后是最近在看增强学习，我们在筹备着一些比较牛的东西，<del>只不过不一定做的出来罢了</del>┑(￣Д ￣)┍ 万一做出来了记得刚点个赞哈~</p>\n</li>\n<li><p>Coding: 基本只熟悉python了，C++勉强写一些，其他会的但是现在用的少的像matlab, as3等等已经忘光了~  </p>\n</li>\n<li><p>休闲：目前最工作外常干的三件事就是 <strong>看NBA，看ACG，和弹吉他（虽然很渣）</strong> 基本各种娱乐活动来着不拒 </p>\n</li>\n<li><p>现状：于上交和悉尼科技大学博士毕业，目前在滴滴出行当算法攻城狮，主要负责分单系统的模型化 </p>\n</li>\n</ul>\n<h2 id=\"关于博客\"><a href=\"#关于博客\" class=\"headerlink\" title=\"关于博客\"></a>关于博客</h2><ul>\n<li>这个Blog只负责记录我看的paper，之前做的一些工作和小实验，再加上现在准备在滴滴做的一些新东西。在可允许范围内我们会release一些滴滴可公开的数据，也请大家不要宣传-&gt;_-&gt;</li>\n<li>此外我还会放一些之前依着个人恶趣味做的东西，不定期更新，请不要有所期待┑(￣Д ￣)┍</li>\n</ul>\n<hr>\n<p><strong> 最近访问 </strong> </p>\n<div class=\"ds-recent-visitors\" data-num-items=\"36\" data-avatar-size=\"42\" id=\"ds-recent-visitors\"><br></div>\n\n\n","excerpt":"","more":"<h2 id=\"关于我\"><a href=\"#关于我\" class=\"headerlink\" title=\"关于我\"></a>关于我</h2><ul>\n<li>姓名：徐哲  </li>\n<li>性别：男  </li>\n<li>个性：听说是高冷+傲娇？总之一般人看起来还是一个异常认真、不太好交流的人吧。但其实很kind的=。=  </li>\n<li>技能：世界上少数几个 <strong><em>鸟类专家</em></strong> ，精通各种鸟（尤其是美国鸟）的识别😂</li>\n</ul>\n<p><img src=\"http://image98.360doc.com/DownloadImg/2016/07/2905/76788528_141.gif\" alt=\"\"></p>\n<ul>\n<li><p>– 博士期间主要是做了个鸟~~~类识别的工作，在细粒度识别(Fine-grained Visual Categorization)这个领域靠认鸟发了几篇paper，也算是正常毕业了。 当然了认一些其他的车啊人啊花啊草啊的也能勉强胜任，computer vision的其他recognition/detection/segmentation略知皮毛，不免俗的deep learning也玩，实验室的关系multimedia, data mining之类的也搞过一阵，还玩过一些图像艺术性分析这样的玄学，<del>以上是吹逼</del>。</p>\n</li>\n<li><p>– 最后是最近在看增强学习，我们在筹备着一些比较牛的东西，<del>只不过不一定做的出来罢了</del>┑(￣Д ￣)┍ 万一做出来了记得刚点个赞哈~</p>\n</li>\n<li><p>Coding: 基本只熟悉python了，C++勉强写一些，其他会的但是现在用的少的像matlab, as3等等已经忘光了~  </p>\n</li>\n<li><p>休闲：目前最工作外常干的三件事就是 <strong>看NBA，看ACG，和弹吉他（虽然很渣）</strong> 基本各种娱乐活动来着不拒 </p>\n</li>\n<li><p>现状：于上交和悉尼科技大学博士毕业，目前在滴滴出行当算法攻城狮，主要负责分单系统的模型化 </p>\n</li>\n</ul>\n<h2 id=\"关于博客\"><a href=\"#关于博客\" class=\"headerlink\" title=\"关于博客\"></a>关于博客</h2><ul>\n<li>这个Blog只负责记录我看的paper，之前做的一些工作和小实验，再加上现在准备在滴滴做的一些新东西。在可允许范围内我们会release一些滴滴可公开的数据，也请大家不要宣传-&gt;_-&gt;</li>\n<li>此外我还会放一些之前依着个人恶趣味做的东西，不定期更新，请不要有所期待┑(￣Д ￣)┍</li>\n</ul>\n<hr>\n<p><strong> 最近访问 </strong> </p>\n<div class=\"ds-recent-visitors\"\n    data-num-items=\"36\"\n    data-avatar-size=\"42\"\n    id=\"ds-recent-visitors\"><br></div>\n\n\n"},{"title":"分类","date":"2017-01-26T06:14:14.000Z","type":"categories","comments":0,"_content":"","source":"categories/index.md","raw":"---\ntitle: 分类\ndate: 2017-01-26 14:14:14\ntype: \"categories\"\ncomments: false\n---\n","updated":"2017-04-13T15:29:54.000Z","path":"categories/index.html","layout":"page","_id":"cj1jbutcr00039qs6s7vzh339","content":"","excerpt":"","more":""},{"title":"标签","date":"2017-01-26T06:15:21.000Z","type":"tags","comments":0,"_content":"","source":"tags/index.md","raw":"---\ntitle: 标签\ndate: 2017-01-26 14:15:21\ntype: \"tags\"\ncomments: false\n---\n","updated":"2017-04-13T15:29:55.000Z","path":"tags/index.html","layout":"page","_id":"cj1jbutka00109qs6yovga4m9","content":"","excerpt":"","more":""}],"Post":[{"title":"滴滴派单算法解析（一）—— 订单分配问题初探","date":"2017-02-03T07:49:29.000Z","_content":"\n\n# 前言\n\n　　加入滴滴已经有超过半年的时间了。滴滴相关的新闻倒是一直不断，从补贴大战，到合并uber中国业务，再到新政、春节带来的新动荡，关注的焦点似乎一直都是在运营的手段以及政府关系等层面。除了零星的派单远、加价高的吐槽外，外界对公司中所用的技术与算法大多是漠不关心或一知半解的状态。  \n\n<center>![](http://research.xiaojukeji.com/images/index/case_2.png)\n</center>\n\n　　于是这里首先来解答一个大家都很好奇的问题：滴滴的派单算法是什么样的？一个看似简单的问题，实际上也是有万千玄妙的呢（笑）。如果在看过之后，能引发大家的思考，并重新审视我们的工作，那便是最大的欣慰了。\n\n　　全文共４部分。如果你只对吐槽滴滴有兴趣的话，第一、二部分就足够啦～当然如果你对技术方面，尤其是 **深度增强学习** 在滴滴派单中的应用有兴趣的话，希望你能坚持到第三、四部分的内容，我会讲一些比较深的理论知识和应用场景。\n\n<!--more-->\n\n---\n\n# 简单的问题？\n\n> ** 订单分配** 即是在系统平台中将 乘客发出的订单 分配给 在线的司机 的过程。 \n\n　　这看似是一个非常简单的问题。聪明的你肯定早有头绪：_把我的订单分给 **最近** 的司机不就好了嘛_？\n\n　　Bingo! 实际上目前滴滴的派单算法确实是基于 **就近分配** 的原则进行的。据我所知，目前世界上其他的竞品公司（包括Uber Global），也均是基于这个原则分单的。但是为什么又会出现派单后司机与乘客距离好几公里的情况呢？\n\n>     \n> 目前滴滴的派单算法原则是：就近分配。  \n>   \n\n---\n\n# 实际情况是...\n\n　　首先我们需要明确的一点是，在订单和司机 _充分密集_ 时，使用就近分配的算法得到的分配结果即为最优解。\n\n　　这里指的_充分密集_，即订单和司机在 *时间* 和 *空间* 维度上均存在足够多的样本。换句话说，当乘客于任何时刻呼叫订单时，均能有一个就在身边的司机在线并空闲。\n\n　　事实上，由于订单和司机在现实中并不可能充分密集，就近分单仍存在着一定的问题。当然，这也是为什么滴滴专车的平均接单距离要大于快车的原因（专车的司机和订单较快车均要少不止一个数量级）。\n\n> 在订单和司机 _充分密集_ 时，就近分配即可达到最优解。\n\n## 时序很重要\n\n![](/images/demo_all_in.png)\n\n　　好，现在让我们来举个简单的例子吧！假设上图是一个5x3的街区，你是图中帅气地招手的小红:D 显然，在这个有两名乘客和两位司机的情况下，将右下角的司机指派来接小红（就是你啦~），而让左边的司机去接左上角的乘客是最合理的选择，皆大欢喜！\n\n　　但现实往往比想象的更复杂，在乘客和司机的匹配问题中，通常时序是一个无法忽略的问题。实际上，上图中的两名乘客和两名司机往往并不是同时出现的。\n\n　　为了模拟现实中更容易发生的情况，我们做了一个时序分解，看一看在这段时间里发生了什么：\n![](/images/demo_timegap.png)\n\n- 在 **时间1** 上，小红打开了滴滴并叫车，形成了一个订单。此时小红周围其实 _并没有一个可以接单的司机_。\n- 过了几秒钟， **时间2** 时，左边的司机结束了上一个订单的服务，正式变为了空闲司机！此时，我们的分单系统按照 _最近匹配_ 的原则，将小红的订单分给了左边的司机，小红要等待司机5分钟才能上车。\n- 又过了几秒钟，右下角司机终于结束了上一单的服务姗姗来迟，然而系统已经指派了左边的司机来接单了。\n\n　　这里也许会有很多人有疑问：上面说的是否只是我捏造的例子呢？实际上这种情况的发生比例能有多少呀？\n\n　　很遗憾，上图所述的情况在现实中是普遍存在的。数据显示，在大城市高峰期热区地区，有超过50%的订单在产生时周围并没有空闲司机，而是在等待一段时间后分给刚刚结束服务的司机的。这种情况下，就近分配的方案可能不太好用哦T_T。\n\n> 高峰期时，有超过50%的订单是刚刚由结束服务的司机接单的哦！  \n> 在 大城市/热区/高峰期 ，这个比例甚至可能超过80%！\n\n## 为了更多人\n\n　　影响就近分单的第二个元素是个人需求（即 _贪婪法_ ）和更多人的需求（即 _全局最优_ ）的权衡。\n\n![](/images/demo_greedy_vs_km.png)\n\n　　回到刚才的例子，同样的5x3的街区，如果将靠左边的司机稍微向右移动一格，即会造成个人最优和全局最优的冲突。从小红的立场来看，自己不远处就有一辆车可以来接单。然而，若这辆车分配给了小红，另外一个乘客可能要忍受极长的接单时间，并很有可能只能 **取消订单** 。\n\n　　面对这个两难问题，你的选择会是什么呢？\n\n> 在供需系统中，个人利益的最优与全局最优常常是冲突的。\n\n## 看的更远\n\n　　相信大家一定有听说过或经历过出租车司机拒载的情况，其根本原因，在于订单确实有“肥瘦”之分。与个人乘客零星的打车需求不同，司机往往会在平台上连续接单好几个小时。这时，司机考虑的不只是当前的收益，还有了未来可能的收益。\n\n　　平台作为一个资源的调配方，其最终目的是 **服务更多的乘客/订单，使得司机的收入最高，从而提升平台收益，达到共赢的效果 **。\n\n　　为此，当一个在高峰期前从热区开到冷区的订单发出时，平台不得不在完成这一单的收益，和让司机之后一小时都要空驶，从而使得未来本可能让这个司机接单的两个订单无人接单之间做出选择。又是一个痛苦的抉择呐。\n\n> 春节期间由于司机减少的原因，大部分时间超过80%的快车司机都在服务中哦! 空闲的司机大多都是因为被订单派到了非常偏远的地方。\n\n---\n\n# 你会怎么做？\n\n　　当站在整个平台的角度看问题的时候，世界往往会和我们预想的发生微妙的偏差。也许在读这篇文章前，我们都无从了解原来一个“简单”的派单里还有如此多的问题。关于派单，你是否有自己的意见或建议给我们呢？\n\n　　在下一节中，我会简单地介绍目前我们对这些问题的思考以及当前的派单算法。\n","source":"_drafts/didi-distribute1.md","raw":"---\ntitle: 滴滴派单算法解析（一）—— 订单分配问题初探\ntags: didi\ncategories: didi\ndate: 2017-02-03 15:49:29\n---\n\n\n# 前言\n\n　　加入滴滴已经有超过半年的时间了。滴滴相关的新闻倒是一直不断，从补贴大战，到合并uber中国业务，再到新政、春节带来的新动荡，关注的焦点似乎一直都是在运营的手段以及政府关系等层面。除了零星的派单远、加价高的吐槽外，外界对公司中所用的技术与算法大多是漠不关心或一知半解的状态。  \n\n<center>![](http://research.xiaojukeji.com/images/index/case_2.png)\n</center>\n\n　　于是这里首先来解答一个大家都很好奇的问题：滴滴的派单算法是什么样的？一个看似简单的问题，实际上也是有万千玄妙的呢（笑）。如果在看过之后，能引发大家的思考，并重新审视我们的工作，那便是最大的欣慰了。\n\n　　全文共４部分。如果你只对吐槽滴滴有兴趣的话，第一、二部分就足够啦～当然如果你对技术方面，尤其是 **深度增强学习** 在滴滴派单中的应用有兴趣的话，希望你能坚持到第三、四部分的内容，我会讲一些比较深的理论知识和应用场景。\n\n<!--more-->\n\n---\n\n# 简单的问题？\n\n> ** 订单分配** 即是在系统平台中将 乘客发出的订单 分配给 在线的司机 的过程。 \n\n　　这看似是一个非常简单的问题。聪明的你肯定早有头绪：_把我的订单分给 **最近** 的司机不就好了嘛_？\n\n　　Bingo! 实际上目前滴滴的派单算法确实是基于 **就近分配** 的原则进行的。据我所知，目前世界上其他的竞品公司（包括Uber Global），也均是基于这个原则分单的。但是为什么又会出现派单后司机与乘客距离好几公里的情况呢？\n\n>     \n> 目前滴滴的派单算法原则是：就近分配。  \n>   \n\n---\n\n# 实际情况是...\n\n　　首先我们需要明确的一点是，在订单和司机 _充分密集_ 时，使用就近分配的算法得到的分配结果即为最优解。\n\n　　这里指的_充分密集_，即订单和司机在 *时间* 和 *空间* 维度上均存在足够多的样本。换句话说，当乘客于任何时刻呼叫订单时，均能有一个就在身边的司机在线并空闲。\n\n　　事实上，由于订单和司机在现实中并不可能充分密集，就近分单仍存在着一定的问题。当然，这也是为什么滴滴专车的平均接单距离要大于快车的原因（专车的司机和订单较快车均要少不止一个数量级）。\n\n> 在订单和司机 _充分密集_ 时，就近分配即可达到最优解。\n\n## 时序很重要\n\n![](/images/demo_all_in.png)\n\n　　好，现在让我们来举个简单的例子吧！假设上图是一个5x3的街区，你是图中帅气地招手的小红:D 显然，在这个有两名乘客和两位司机的情况下，将右下角的司机指派来接小红（就是你啦~），而让左边的司机去接左上角的乘客是最合理的选择，皆大欢喜！\n\n　　但现实往往比想象的更复杂，在乘客和司机的匹配问题中，通常时序是一个无法忽略的问题。实际上，上图中的两名乘客和两名司机往往并不是同时出现的。\n\n　　为了模拟现实中更容易发生的情况，我们做了一个时序分解，看一看在这段时间里发生了什么：\n![](/images/demo_timegap.png)\n\n- 在 **时间1** 上，小红打开了滴滴并叫车，形成了一个订单。此时小红周围其实 _并没有一个可以接单的司机_。\n- 过了几秒钟， **时间2** 时，左边的司机结束了上一个订单的服务，正式变为了空闲司机！此时，我们的分单系统按照 _最近匹配_ 的原则，将小红的订单分给了左边的司机，小红要等待司机5分钟才能上车。\n- 又过了几秒钟，右下角司机终于结束了上一单的服务姗姗来迟，然而系统已经指派了左边的司机来接单了。\n\n　　这里也许会有很多人有疑问：上面说的是否只是我捏造的例子呢？实际上这种情况的发生比例能有多少呀？\n\n　　很遗憾，上图所述的情况在现实中是普遍存在的。数据显示，在大城市高峰期热区地区，有超过50%的订单在产生时周围并没有空闲司机，而是在等待一段时间后分给刚刚结束服务的司机的。这种情况下，就近分配的方案可能不太好用哦T_T。\n\n> 高峰期时，有超过50%的订单是刚刚由结束服务的司机接单的哦！  \n> 在 大城市/热区/高峰期 ，这个比例甚至可能超过80%！\n\n## 为了更多人\n\n　　影响就近分单的第二个元素是个人需求（即 _贪婪法_ ）和更多人的需求（即 _全局最优_ ）的权衡。\n\n![](/images/demo_greedy_vs_km.png)\n\n　　回到刚才的例子，同样的5x3的街区，如果将靠左边的司机稍微向右移动一格，即会造成个人最优和全局最优的冲突。从小红的立场来看，自己不远处就有一辆车可以来接单。然而，若这辆车分配给了小红，另外一个乘客可能要忍受极长的接单时间，并很有可能只能 **取消订单** 。\n\n　　面对这个两难问题，你的选择会是什么呢？\n\n> 在供需系统中，个人利益的最优与全局最优常常是冲突的。\n\n## 看的更远\n\n　　相信大家一定有听说过或经历过出租车司机拒载的情况，其根本原因，在于订单确实有“肥瘦”之分。与个人乘客零星的打车需求不同，司机往往会在平台上连续接单好几个小时。这时，司机考虑的不只是当前的收益，还有了未来可能的收益。\n\n　　平台作为一个资源的调配方，其最终目的是 **服务更多的乘客/订单，使得司机的收入最高，从而提升平台收益，达到共赢的效果 **。\n\n　　为此，当一个在高峰期前从热区开到冷区的订单发出时，平台不得不在完成这一单的收益，和让司机之后一小时都要空驶，从而使得未来本可能让这个司机接单的两个订单无人接单之间做出选择。又是一个痛苦的抉择呐。\n\n> 春节期间由于司机减少的原因，大部分时间超过80%的快车司机都在服务中哦! 空闲的司机大多都是因为被订单派到了非常偏远的地方。\n\n---\n\n# 你会怎么做？\n\n　　当站在整个平台的角度看问题的时候，世界往往会和我们预想的发生微妙的偏差。也许在读这篇文章前，我们都无从了解原来一个“简单”的派单里还有如此多的问题。关于派单，你是否有自己的意见或建议给我们呢？\n\n　　在下一节中，我会简单地介绍目前我们对这些问题的思考以及当前的派单算法。\n","slug":"didi-distribute1","published":0,"updated":"2017-04-13T15:29:54.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1jbutcj00009qs6nenhcxjm","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><p>　　加入滴滴已经有超过半年的时间了。滴滴相关的新闻倒是一直不断，从补贴大战，到合并uber中国业务，再到新政、春节带来的新动荡，关注的焦点似乎一直都是在运营的手段以及政府关系等层面。除了零星的派单远、加价高的吐槽外，外界对公司中所用的技术与算法大多是漠不关心或一知半解的状态。  </p>\n<center><img src=\"http://research.xiaojukeji.com/images/index/case_2.png\" alt=\"\"><br></center>\n\n<p>　　于是这里首先来解答一个大家都很好奇的问题：滴滴的派单算法是什么样的？一个看似简单的问题，实际上也是有万千玄妙的呢（笑）。如果在看过之后，能引发大家的思考，并重新审视我们的工作，那便是最大的欣慰了。</p>\n<p>　　全文共４部分。如果你只对吐槽滴滴有兴趣的话，第一、二部分就足够啦～当然如果你对技术方面，尤其是 <strong>深度增强学习</strong> 在滴滴派单中的应用有兴趣的话，希望你能坚持到第三、四部分的内容，我会讲一些比较深的理论知识和应用场景。</p>\n<a id=\"more\"></a>\n<hr>\n<h1 id=\"简单的问题\"><a href=\"#简单的问题？\" class=\"headerlink\" title=\"简单的问题？\"></a>简单的问题？</h1><blockquote>\n<p><strong> 订单分配</strong> 即是在系统平台中将 乘客发出的订单 分配给 在线的司机 的过程。 </p>\n</blockquote>\n<p>　　这看似是一个非常简单的问题。聪明的你肯定早有头绪：_把我的订单分给 <strong>最近</strong> 的司机不就好了嘛_？</p>\n<p>　　Bingo! 实际上目前滴滴的派单算法确实是基于 <strong>就近分配</strong> 的原则进行的。据我所知，目前世界上其他的竞品公司（包括Uber Global），也均是基于这个原则分单的。但是为什么又会出现派单后司机与乘客距离好几公里的情况呢？</p>\n<blockquote>\n<p>目前滴滴的派单算法原则是：就近分配。  </p>\n</blockquote>\n<hr>\n<h1 id=\"实际情况是\"><a href=\"#实际情况是…\" class=\"headerlink\" title=\"实际情况是…\"></a>实际情况是…</h1><p>　　首先我们需要明确的一点是，在订单和司机 _充分密集_ 时，使用就近分配的算法得到的分配结果即为最优解。</p>\n<p>　　这里指的_充分密集_，即订单和司机在 <em>时间</em> 和 <em>空间</em> 维度上均存在足够多的样本。换句话说，当乘客于任何时刻呼叫订单时，均能有一个就在身边的司机在线并空闲。</p>\n<p>　　事实上，由于订单和司机在现实中并不可能充分密集，就近分单仍存在着一定的问题。当然，这也是为什么滴滴专车的平均接单距离要大于快车的原因（专车的司机和订单较快车均要少不止一个数量级）。</p>\n<blockquote>\n<p>在订单和司机 _充分密集_ 时，就近分配即可达到最优解。</p>\n</blockquote>\n<h2 id=\"时序很重要\"><a href=\"#时序很重要\" class=\"headerlink\" title=\"时序很重要\"></a>时序很重要</h2><p><img src=\"/images/demo_all_in.png\" alt=\"\"></p>\n<p>　　好，现在让我们来举个简单的例子吧！假设上图是一个5x3的街区，你是图中帅气地招手的小红:D 显然，在这个有两名乘客和两位司机的情况下，将右下角的司机指派来接小红（就是你啦~），而让左边的司机去接左上角的乘客是最合理的选择，皆大欢喜！</p>\n<p>　　但现实往往比想象的更复杂，在乘客和司机的匹配问题中，通常时序是一个无法忽略的问题。实际上，上图中的两名乘客和两名司机往往并不是同时出现的。</p>\n<p>　　为了模拟现实中更容易发生的情况，我们做了一个时序分解，看一看在这段时间里发生了什么：<br><img src=\"/images/demo_timegap.png\" alt=\"\"></p>\n<ul>\n<li>在 <strong>时间1</strong> 上，小红打开了滴滴并叫车，形成了一个订单。此时小红周围其实 _并没有一个可以接单的司机_。</li>\n<li>过了几秒钟， <strong>时间2</strong> 时，左边的司机结束了上一个订单的服务，正式变为了空闲司机！此时，我们的分单系统按照 _最近匹配_ 的原则，将小红的订单分给了左边的司机，小红要等待司机5分钟才能上车。</li>\n<li>又过了几秒钟，右下角司机终于结束了上一单的服务姗姗来迟，然而系统已经指派了左边的司机来接单了。</li>\n</ul>\n<p>　　这里也许会有很多人有疑问：上面说的是否只是我捏造的例子呢？实际上这种情况的发生比例能有多少呀？</p>\n<p>　　很遗憾，上图所述的情况在现实中是普遍存在的。数据显示，在大城市高峰期热区地区，有超过50%的订单在产生时周围并没有空闲司机，而是在等待一段时间后分给刚刚结束服务的司机的。这种情况下，就近分配的方案可能不太好用哦T_T。</p>\n<blockquote>\n<p>高峰期时，有超过50%的订单是刚刚由结束服务的司机接单的哦！<br>在 大城市/热区/高峰期 ，这个比例甚至可能超过80%！</p>\n</blockquote>\n<h2 id=\"为了更多人\"><a href=\"#为了更多人\" class=\"headerlink\" title=\"为了更多人\"></a>为了更多人</h2><p>　　影响就近分单的第二个元素是个人需求（即 _贪婪法_ ）和更多人的需求（即 _全局最优_ ）的权衡。</p>\n<p><img src=\"/images/demo_greedy_vs_km.png\" alt=\"\"></p>\n<p>　　回到刚才的例子，同样的5x3的街区，如果将靠左边的司机稍微向右移动一格，即会造成个人最优和全局最优的冲突。从小红的立场来看，自己不远处就有一辆车可以来接单。然而，若这辆车分配给了小红，另外一个乘客可能要忍受极长的接单时间，并很有可能只能 <strong>取消订单</strong> 。</p>\n<p>　　面对这个两难问题，你的选择会是什么呢？</p>\n<blockquote>\n<p>在供需系统中，个人利益的最优与全局最优常常是冲突的。</p>\n</blockquote>\n<h2 id=\"看的更远\"><a href=\"#看的更远\" class=\"headerlink\" title=\"看的更远\"></a>看的更远</h2><p>　　相信大家一定有听说过或经历过出租车司机拒载的情况，其根本原因，在于订单确实有“肥瘦”之分。与个人乘客零星的打车需求不同，司机往往会在平台上连续接单好几个小时。这时，司机考虑的不只是当前的收益，还有了未来可能的收益。</p>\n<p>　　平台作为一个资源的调配方，其最终目的是 <strong>服务更多的乘客/订单，使得司机的收入最高，从而提升平台收益，达到共赢的效果 </strong>。</p>\n<p>　　为此，当一个在高峰期前从热区开到冷区的订单发出时，平台不得不在完成这一单的收益，和让司机之后一小时都要空驶，从而使得未来本可能让这个司机接单的两个订单无人接单之间做出选择。又是一个痛苦的抉择呐。</p>\n<blockquote>\n<p>春节期间由于司机减少的原因，大部分时间超过80%的快车司机都在服务中哦! 空闲的司机大多都是因为被订单派到了非常偏远的地方。</p>\n</blockquote>\n<hr>\n<h1 id=\"你会怎么做\"><a href=\"#你会怎么做？\" class=\"headerlink\" title=\"你会怎么做？\"></a>你会怎么做？</h1><p>　　当站在整个平台的角度看问题的时候，世界往往会和我们预想的发生微妙的偏差。也许在读这篇文章前，我们都无从了解原来一个“简单”的派单里还有如此多的问题。关于派单，你是否有自己的意见或建议给我们呢？</p>\n<p>　　在下一节中，我会简单地介绍目前我们对这些问题的思考以及当前的派单算法。</p>\n","excerpt":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><p>　　加入滴滴已经有超过半年的时间了。滴滴相关的新闻倒是一直不断，从补贴大战，到合并uber中国业务，再到新政、春节带来的新动荡，关注的焦点似乎一直都是在运营的手段以及政府关系等层面。除了零星的派单远、加价高的吐槽外，外界对公司中所用的技术与算法大多是漠不关心或一知半解的状态。  </p>\n<center><img src=\"http://research.xiaojukeji.com/images/index/case_2.png\" alt=\"\"><br></center>\n\n<p>　　于是这里首先来解答一个大家都很好奇的问题：滴滴的派单算法是什么样的？一个看似简单的问题，实际上也是有万千玄妙的呢（笑）。如果在看过之后，能引发大家的思考，并重新审视我们的工作，那便是最大的欣慰了。</p>\n<p>　　全文共４部分。如果你只对吐槽滴滴有兴趣的话，第一、二部分就足够啦～当然如果你对技术方面，尤其是 <strong>深度增强学习</strong> 在滴滴派单中的应用有兴趣的话，希望你能坚持到第三、四部分的内容，我会讲一些比较深的理论知识和应用场景。</p>","more":"<hr>\n<h1 id=\"简单的问题？\"><a href=\"#简单的问题？\" class=\"headerlink\" title=\"简单的问题？\"></a>简单的问题？</h1><blockquote>\n<p><strong> 订单分配</strong> 即是在系统平台中将 乘客发出的订单 分配给 在线的司机 的过程。 </p>\n</blockquote>\n<p>　　这看似是一个非常简单的问题。聪明的你肯定早有头绪：_把我的订单分给 <strong>最近</strong> 的司机不就好了嘛_？</p>\n<p>　　Bingo! 实际上目前滴滴的派单算法确实是基于 <strong>就近分配</strong> 的原则进行的。据我所知，目前世界上其他的竞品公司（包括Uber Global），也均是基于这个原则分单的。但是为什么又会出现派单后司机与乘客距离好几公里的情况呢？</p>\n<blockquote>\n<p>目前滴滴的派单算法原则是：就近分配。  </p>\n</blockquote>\n<hr>\n<h1 id=\"实际情况是…\"><a href=\"#实际情况是…\" class=\"headerlink\" title=\"实际情况是…\"></a>实际情况是…</h1><p>　　首先我们需要明确的一点是，在订单和司机 _充分密集_ 时，使用就近分配的算法得到的分配结果即为最优解。</p>\n<p>　　这里指的_充分密集_，即订单和司机在 <em>时间</em> 和 <em>空间</em> 维度上均存在足够多的样本。换句话说，当乘客于任何时刻呼叫订单时，均能有一个就在身边的司机在线并空闲。</p>\n<p>　　事实上，由于订单和司机在现实中并不可能充分密集，就近分单仍存在着一定的问题。当然，这也是为什么滴滴专车的平均接单距离要大于快车的原因（专车的司机和订单较快车均要少不止一个数量级）。</p>\n<blockquote>\n<p>在订单和司机 _充分密集_ 时，就近分配即可达到最优解。</p>\n</blockquote>\n<h2 id=\"时序很重要\"><a href=\"#时序很重要\" class=\"headerlink\" title=\"时序很重要\"></a>时序很重要</h2><p><img src=\"/images/demo_all_in.png\" alt=\"\"></p>\n<p>　　好，现在让我们来举个简单的例子吧！假设上图是一个5x3的街区，你是图中帅气地招手的小红:D 显然，在这个有两名乘客和两位司机的情况下，将右下角的司机指派来接小红（就是你啦~），而让左边的司机去接左上角的乘客是最合理的选择，皆大欢喜！</p>\n<p>　　但现实往往比想象的更复杂，在乘客和司机的匹配问题中，通常时序是一个无法忽略的问题。实际上，上图中的两名乘客和两名司机往往并不是同时出现的。</p>\n<p>　　为了模拟现实中更容易发生的情况，我们做了一个时序分解，看一看在这段时间里发生了什么：<br><img src=\"/images/demo_timegap.png\" alt=\"\"></p>\n<ul>\n<li>在 <strong>时间1</strong> 上，小红打开了滴滴并叫车，形成了一个订单。此时小红周围其实 _并没有一个可以接单的司机_。</li>\n<li>过了几秒钟， <strong>时间2</strong> 时，左边的司机结束了上一个订单的服务，正式变为了空闲司机！此时，我们的分单系统按照 _最近匹配_ 的原则，将小红的订单分给了左边的司机，小红要等待司机5分钟才能上车。</li>\n<li>又过了几秒钟，右下角司机终于结束了上一单的服务姗姗来迟，然而系统已经指派了左边的司机来接单了。</li>\n</ul>\n<p>　　这里也许会有很多人有疑问：上面说的是否只是我捏造的例子呢？实际上这种情况的发生比例能有多少呀？</p>\n<p>　　很遗憾，上图所述的情况在现实中是普遍存在的。数据显示，在大城市高峰期热区地区，有超过50%的订单在产生时周围并没有空闲司机，而是在等待一段时间后分给刚刚结束服务的司机的。这种情况下，就近分配的方案可能不太好用哦T_T。</p>\n<blockquote>\n<p>高峰期时，有超过50%的订单是刚刚由结束服务的司机接单的哦！<br>在 大城市/热区/高峰期 ，这个比例甚至可能超过80%！</p>\n</blockquote>\n<h2 id=\"为了更多人\"><a href=\"#为了更多人\" class=\"headerlink\" title=\"为了更多人\"></a>为了更多人</h2><p>　　影响就近分单的第二个元素是个人需求（即 _贪婪法_ ）和更多人的需求（即 _全局最优_ ）的权衡。</p>\n<p><img src=\"/images/demo_greedy_vs_km.png\" alt=\"\"></p>\n<p>　　回到刚才的例子，同样的5x3的街区，如果将靠左边的司机稍微向右移动一格，即会造成个人最优和全局最优的冲突。从小红的立场来看，自己不远处就有一辆车可以来接单。然而，若这辆车分配给了小红，另外一个乘客可能要忍受极长的接单时间，并很有可能只能 <strong>取消订单</strong> 。</p>\n<p>　　面对这个两难问题，你的选择会是什么呢？</p>\n<blockquote>\n<p>在供需系统中，个人利益的最优与全局最优常常是冲突的。</p>\n</blockquote>\n<h2 id=\"看的更远\"><a href=\"#看的更远\" class=\"headerlink\" title=\"看的更远\"></a>看的更远</h2><p>　　相信大家一定有听说过或经历过出租车司机拒载的情况，其根本原因，在于订单确实有“肥瘦”之分。与个人乘客零星的打车需求不同，司机往往会在平台上连续接单好几个小时。这时，司机考虑的不只是当前的收益，还有了未来可能的收益。</p>\n<p>　　平台作为一个资源的调配方，其最终目的是 <strong>服务更多的乘客/订单，使得司机的收入最高，从而提升平台收益，达到共赢的效果 </strong>。</p>\n<p>　　为此，当一个在高峰期前从热区开到冷区的订单发出时，平台不得不在完成这一单的收益，和让司机之后一小时都要空驶，从而使得未来本可能让这个司机接单的两个订单无人接单之间做出选择。又是一个痛苦的抉择呐。</p>\n<blockquote>\n<p>春节期间由于司机减少的原因，大部分时间超过80%的快车司机都在服务中哦! 空闲的司机大多都是因为被订单派到了非常偏远的地方。</p>\n</blockquote>\n<hr>\n<h1 id=\"你会怎么做？\"><a href=\"#你会怎么做？\" class=\"headerlink\" title=\"你会怎么做？\"></a>你会怎么做？</h1><p>　　当站在整个平台的角度看问题的时候，世界往往会和我们预想的发生微妙的偏差。也许在读这篇文章前，我们都无从了解原来一个“简单”的派单里还有如此多的问题。关于派单，你是否有自己的意见或建议给我们呢？</p>\n<p>　　在下一节中，我会简单地介绍目前我们对这些问题的思考以及当前的派单算法。</p>"},{"title":"滴滴派单算法解析（二）—— 线上策略","date":"2017-02-03T07:50:32.000Z","_content":"\n\n# 前言\n\n　　在上一节的内容中，我们描述了派单过程中可能出现的问题。关于这些问题的应对方案，或多或少都在目前线上使用的派单策略中有所体现，并获得了一定的效果。这一节里我们就来聊聊线上的策略到底是什么样的～ (_公司政策所致，这里不能放具体的数据和太详细的策略执行情况，所以还是以motivation为主_)。\n\n<!--more-->\n\n---\n\n# 总原则\n> 当前派单策略的原则是 从全局出发，保证用户的需求尽可能多的被满足。\n\n　　如上一节所说，在设计派单算法时，会涉及到一些原则或是哲学问题。在当前线上的策略中，我们的总原则是：**站在全局的角度，尽可能多服务一个用户，并尽力保证用户表达的每个叫车需求可以确定性的执行**。如何理解这个原则呢？\n- 策略是站在 **全局** 的角度，争取达到全局最优。这样，对每个用户自身来说，派单可能不是“局部最优”的，但大多数用户的需求可以得到保证。\n- 叫车的确定性，即用户在平台中点击呼叫后，我们希望绝大部分的用户都能有司机来响应，并最终完成订单。其分别对应 _应答率_ 和 _成交率_ 两个概念。目前，正常情况下，平台上快车的应答率超过95%，成交率超过80%。\n\n---\n\n# 线上策略\n　　基于我们的总原则，下面一一讲一下现在线上使用（或进行过实验）的派单策略。\n\n## 规则过滤\n> 如果身边的滴滴司机没有听到你发的订单，那么他很有可能是命中了反作弊、限号等过滤规则了。若不是，可能是司机手机的网络状况出了问题，由于种种原因没有听到播单的情况也有超过1%哦。\n\n　　最基本的策略其实是人工设定的规则过滤。必须澄清的一点是这里的规则并不会造成分单时不公平的效果，而完全是为了业务能正常运行而设立的。举几个最基础的例子：\n- 规则A: 快车司机不能接专车订单\n- 规则B: 保证司机接单后不会通过限号区域\n- 规则C: 为设定实时目的地的司机过滤不顺路订单\n- 规则D: 为只听预约单的司机过滤实时订单\n- 规则E: 同一个订单只会发给一个司机一次\n- 。。。\n\n　　目前这些类似的为了保证业务正确性的人工规则已经有了超过60条，也同时承担着反作弊、拼车等业务的入口功能。当然，这种人工规则并不涉及到具体的派单策略，故这里就不展开讲了。\n\n\n## 延迟集中分单\n> 当前线上的派单系统，是将乘客订单和空闲司机的信息收集起来，每一秒钟进行一次全局最优的分单匹配。\n\n　　派单策略中最基础的部分，是为了解决上一节提到的时序问题。\n\n　　由于用户订单的产生和司机的出现往往并不在同一时间点，在时间维度上贪婪的分单方式（即每个订单出现时即选择附近最近的司机派单）并不能获得全局最优的效果。一个自然的想法就是先让乘客和司机稍等一会，待收集了一段时间的订单和司机信息后，再集中分配。这样，有了相对较多、较密集的订单、司机后，派单策略即可找到更近更合理的派单方式了。\n\n　　找寻司机和订单分配的全局最优是一个 _二分图匹配问题_ (bipartite graph matching) ，可用组合优化中知名的 [_匈牙利算法_](https://en.wikipedia.org/wiki/Hungarian_algorithm) 求解。见下图。实际上，我们线上使用的是KM算法：[_Kuhn–Munkres (KM) 算法_](https://www.topcoder.com/community/data-science/data-science-tutorials/assignment-problem-and-hungarian-algorithm/)。\n\n![](/images/km_algorithm.png)\n\n\n## 基于供需预测分单\n> 如果有先知告诉我们每个订单的生成时间和地点，派单就会变成一件很轻松的事情。\n\n　　KM算法得到的匹配结果，理论上保证是全局最优的。所以问题圆满解决，可喜可贺~~~~真的是这样吗？\n\n　　很遗憾，以上所述的延迟集中分单的策略只能解决部分的问题，仍不是一个完全的方案。其最大的问题，在于用户对系统派单的 *响应时间* 容忍度有限，很多情况下短短的几秒钟即会使用户对平台丧失信心，从而取消订单。故实际线上我们只累积了一秒钟的订单和司机信息进行集中分单，而这在大局上来说仍可近似看做时间维度上的贪婪策略。\n\n　　若想即时的获得最优派单结果，唯一的方法是利用对未来的预测，即进行基于供需预测的分单。这种想法说来玄妙，其实核心内容也很简单：如果我们预测出未来一个区域更有可能有更多的订单/司机，那么就让这个区域的司机/订单先暂缓接单。\n\n![](/images/density_distribute.png)\n\n　　重新回到上一节的例子。这次我们加入了供需预测，对应于上图方框的颜色，颜色越深，代表越有可能形成新的订单。在时间1上，乘客的订单有左边和右下角两辆车均可接单，左边的司机相对来说要更近一些。但由于考虑到了未来可能产生的订单的影响，我们认为将左边的司机留下更有利于未来的订单分配，故在时间2上我们即做出决策，让右下角的司机接单。这个派单决策，直接造成了时间3上新的乘客的需求得到了响应，达到了更合理的分配效果。\n\n## 连环派单\n　　基于供需预测的分单有很大意义，但由于预测的不确定性，其实际效果很难得到保证。为此，我们使用了一种更有确定性的预测方式来进行派单，即 _连环派单_。\n\n> 连环派单，即将订单指派给 _即将结束服务_ 的司机，如果司机的终点与订单位置很相近。\n\n　　与预测订单的分布相反，连环派单预测的是下一时刻空闲司机的所在位置。由于高峰期空闲司机多为司机完成订单后转换而来，预测司机的位置就变成了一个相对确定性的问题，即监测司机到目的地的距离和时间。当服务中的司机距终点很近，且终点离乘客新产生的订单也很近时，便会命中连环派单逻辑。司机在结束上一单服务后，会立刻进入新订单的接单过程中，有效地压缩了订单的应答时间、以及司机的接单距离。\n\n　　连环派单策略上线后获得了很好的效果，也说明了供需预测对订单分配的重要性。\n\n---\n\n# 看看真实数据是怎么样的？\n　　理论上的策略终究是虚的，很容易看的一头雾水。不如现在我们来探一下线上的真实数据是怎么样的！\n\n　　下图是北京某天早高峰 一个KM分单周期（如上文所述，为一秒钟）的订单与空闲司机分布情况。为隐私考虑，图中隐去了所有乘客和司机的个人信息和地图，目的为大致地观察一下线上的真实场景。\n- 每一格为约2kmx2km的区域；\n- 蓝色的圆圈表示这一轮派单中，完成分单  的订单；\n- 红色的圆圈表示这一轮派单中，未完成分单 的订单；\n- 绿色的小圆圈表示在这一轮派单中，所有周围有订单的空闲司机；\n- 蓝色大圆圈和绿色小圆圈的连线 代表 匹配结果。\n- 白色圆圈表示系统中存在的订单，但周围没有可接单的空闲司机。\n\n![](/images/online_distribute_result.png)\n\n　　从图中我们可以发现一些有意思的现象：\n- 由于业务上过滤规则的缘故，不是所有订单都可以和所有司机匹配；\n- 订单和司机的分布很不均匀，会出现大量司机很少订单（如左上角）和大量订单很少司机（如正下方和左下角）的情况；\n- 即使单独在某一时刻看，分配成功的订单比例只占很低的部分，但由于不断有载客司机结束服务，实际上的订单分配成功率（即应答率）仍可超过90%。\n\n> 现实中订单和司机的分布不均匀是普遍存在的。\n\n---\n\n# 总结\n　　基于订单分配中的很多问题，以服务更多人为目标，数据科学家们设计了多种派单策略，并在线上实验获得了可观的效果。\n\n　　然而，当前的派单策略更多的有种各自为政，缺乏整体感和模型化的感觉。为此，我们设计了考虑未来的增强学习模型化的派单策略，将在下两节和大家分享。\n\n","source":"_drafts/didi-distribute2.md","raw":"---\ntitle: 滴滴派单算法解析（二）—— 线上策略\ntags: didi\ncategories: didi\ndate: 2017-02-03 15:50:32\n---\n\n\n# 前言\n\n　　在上一节的内容中，我们描述了派单过程中可能出现的问题。关于这些问题的应对方案，或多或少都在目前线上使用的派单策略中有所体现，并获得了一定的效果。这一节里我们就来聊聊线上的策略到底是什么样的～ (_公司政策所致，这里不能放具体的数据和太详细的策略执行情况，所以还是以motivation为主_)。\n\n<!--more-->\n\n---\n\n# 总原则\n> 当前派单策略的原则是 从全局出发，保证用户的需求尽可能多的被满足。\n\n　　如上一节所说，在设计派单算法时，会涉及到一些原则或是哲学问题。在当前线上的策略中，我们的总原则是：**站在全局的角度，尽可能多服务一个用户，并尽力保证用户表达的每个叫车需求可以确定性的执行**。如何理解这个原则呢？\n- 策略是站在 **全局** 的角度，争取达到全局最优。这样，对每个用户自身来说，派单可能不是“局部最优”的，但大多数用户的需求可以得到保证。\n- 叫车的确定性，即用户在平台中点击呼叫后，我们希望绝大部分的用户都能有司机来响应，并最终完成订单。其分别对应 _应答率_ 和 _成交率_ 两个概念。目前，正常情况下，平台上快车的应答率超过95%，成交率超过80%。\n\n---\n\n# 线上策略\n　　基于我们的总原则，下面一一讲一下现在线上使用（或进行过实验）的派单策略。\n\n## 规则过滤\n> 如果身边的滴滴司机没有听到你发的订单，那么他很有可能是命中了反作弊、限号等过滤规则了。若不是，可能是司机手机的网络状况出了问题，由于种种原因没有听到播单的情况也有超过1%哦。\n\n　　最基本的策略其实是人工设定的规则过滤。必须澄清的一点是这里的规则并不会造成分单时不公平的效果，而完全是为了业务能正常运行而设立的。举几个最基础的例子：\n- 规则A: 快车司机不能接专车订单\n- 规则B: 保证司机接单后不会通过限号区域\n- 规则C: 为设定实时目的地的司机过滤不顺路订单\n- 规则D: 为只听预约单的司机过滤实时订单\n- 规则E: 同一个订单只会发给一个司机一次\n- 。。。\n\n　　目前这些类似的为了保证业务正确性的人工规则已经有了超过60条，也同时承担着反作弊、拼车等业务的入口功能。当然，这种人工规则并不涉及到具体的派单策略，故这里就不展开讲了。\n\n\n## 延迟集中分单\n> 当前线上的派单系统，是将乘客订单和空闲司机的信息收集起来，每一秒钟进行一次全局最优的分单匹配。\n\n　　派单策略中最基础的部分，是为了解决上一节提到的时序问题。\n\n　　由于用户订单的产生和司机的出现往往并不在同一时间点，在时间维度上贪婪的分单方式（即每个订单出现时即选择附近最近的司机派单）并不能获得全局最优的效果。一个自然的想法就是先让乘客和司机稍等一会，待收集了一段时间的订单和司机信息后，再集中分配。这样，有了相对较多、较密集的订单、司机后，派单策略即可找到更近更合理的派单方式了。\n\n　　找寻司机和订单分配的全局最优是一个 _二分图匹配问题_ (bipartite graph matching) ，可用组合优化中知名的 [_匈牙利算法_](https://en.wikipedia.org/wiki/Hungarian_algorithm) 求解。见下图。实际上，我们线上使用的是KM算法：[_Kuhn–Munkres (KM) 算法_](https://www.topcoder.com/community/data-science/data-science-tutorials/assignment-problem-and-hungarian-algorithm/)。\n\n![](/images/km_algorithm.png)\n\n\n## 基于供需预测分单\n> 如果有先知告诉我们每个订单的生成时间和地点，派单就会变成一件很轻松的事情。\n\n　　KM算法得到的匹配结果，理论上保证是全局最优的。所以问题圆满解决，可喜可贺~~~~真的是这样吗？\n\n　　很遗憾，以上所述的延迟集中分单的策略只能解决部分的问题，仍不是一个完全的方案。其最大的问题，在于用户对系统派单的 *响应时间* 容忍度有限，很多情况下短短的几秒钟即会使用户对平台丧失信心，从而取消订单。故实际线上我们只累积了一秒钟的订单和司机信息进行集中分单，而这在大局上来说仍可近似看做时间维度上的贪婪策略。\n\n　　若想即时的获得最优派单结果，唯一的方法是利用对未来的预测，即进行基于供需预测的分单。这种想法说来玄妙，其实核心内容也很简单：如果我们预测出未来一个区域更有可能有更多的订单/司机，那么就让这个区域的司机/订单先暂缓接单。\n\n![](/images/density_distribute.png)\n\n　　重新回到上一节的例子。这次我们加入了供需预测，对应于上图方框的颜色，颜色越深，代表越有可能形成新的订单。在时间1上，乘客的订单有左边和右下角两辆车均可接单，左边的司机相对来说要更近一些。但由于考虑到了未来可能产生的订单的影响，我们认为将左边的司机留下更有利于未来的订单分配，故在时间2上我们即做出决策，让右下角的司机接单。这个派单决策，直接造成了时间3上新的乘客的需求得到了响应，达到了更合理的分配效果。\n\n## 连环派单\n　　基于供需预测的分单有很大意义，但由于预测的不确定性，其实际效果很难得到保证。为此，我们使用了一种更有确定性的预测方式来进行派单，即 _连环派单_。\n\n> 连环派单，即将订单指派给 _即将结束服务_ 的司机，如果司机的终点与订单位置很相近。\n\n　　与预测订单的分布相反，连环派单预测的是下一时刻空闲司机的所在位置。由于高峰期空闲司机多为司机完成订单后转换而来，预测司机的位置就变成了一个相对确定性的问题，即监测司机到目的地的距离和时间。当服务中的司机距终点很近，且终点离乘客新产生的订单也很近时，便会命中连环派单逻辑。司机在结束上一单服务后，会立刻进入新订单的接单过程中，有效地压缩了订单的应答时间、以及司机的接单距离。\n\n　　连环派单策略上线后获得了很好的效果，也说明了供需预测对订单分配的重要性。\n\n---\n\n# 看看真实数据是怎么样的？\n　　理论上的策略终究是虚的，很容易看的一头雾水。不如现在我们来探一下线上的真实数据是怎么样的！\n\n　　下图是北京某天早高峰 一个KM分单周期（如上文所述，为一秒钟）的订单与空闲司机分布情况。为隐私考虑，图中隐去了所有乘客和司机的个人信息和地图，目的为大致地观察一下线上的真实场景。\n- 每一格为约2kmx2km的区域；\n- 蓝色的圆圈表示这一轮派单中，完成分单  的订单；\n- 红色的圆圈表示这一轮派单中，未完成分单 的订单；\n- 绿色的小圆圈表示在这一轮派单中，所有周围有订单的空闲司机；\n- 蓝色大圆圈和绿色小圆圈的连线 代表 匹配结果。\n- 白色圆圈表示系统中存在的订单，但周围没有可接单的空闲司机。\n\n![](/images/online_distribute_result.png)\n\n　　从图中我们可以发现一些有意思的现象：\n- 由于业务上过滤规则的缘故，不是所有订单都可以和所有司机匹配；\n- 订单和司机的分布很不均匀，会出现大量司机很少订单（如左上角）和大量订单很少司机（如正下方和左下角）的情况；\n- 即使单独在某一时刻看，分配成功的订单比例只占很低的部分，但由于不断有载客司机结束服务，实际上的订单分配成功率（即应答率）仍可超过90%。\n\n> 现实中订单和司机的分布不均匀是普遍存在的。\n\n---\n\n# 总结\n　　基于订单分配中的很多问题，以服务更多人为目标，数据科学家们设计了多种派单策略，并在线上实验获得了可观的效果。\n\n　　然而，当前的派单策略更多的有种各自为政，缺乏整体感和模型化的感觉。为此，我们设计了考虑未来的增强学习模型化的派单策略，将在下两节和大家分享。\n\n","slug":"didi-distribute2","published":0,"updated":"2017-04-13T15:29:54.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1jbutcp00029qs6g6sdv3x0","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><p>　　在上一节的内容中，我们描述了派单过程中可能出现的问题。关于这些问题的应对方案，或多或少都在目前线上使用的派单策略中有所体现，并获得了一定的效果。这一节里我们就来聊聊线上的策略到底是什么样的～ (_公司政策所致，这里不能放具体的数据和太详细的策略执行情况，所以还是以motivation为主_)。</p>\n<a id=\"more\"></a>\n<hr>\n<h1 id=\"总原则\"><a href=\"#总原则\" class=\"headerlink\" title=\"总原则\"></a>总原则</h1><blockquote>\n<p>当前派单策略的原则是 从全局出发，保证用户的需求尽可能多的被满足。</p>\n</blockquote>\n<p>　　如上一节所说，在设计派单算法时，会涉及到一些原则或是哲学问题。在当前线上的策略中，我们的总原则是：<strong>站在全局的角度，尽可能多服务一个用户，并尽力保证用户表达的每个叫车需求可以确定性的执行</strong>。如何理解这个原则呢？</p>\n<ul>\n<li>策略是站在 <strong>全局</strong> 的角度，争取达到全局最优。这样，对每个用户自身来说，派单可能不是“局部最优”的，但大多数用户的需求可以得到保证。</li>\n<li>叫车的确定性，即用户在平台中点击呼叫后，我们希望绝大部分的用户都能有司机来响应，并最终完成订单。其分别对应 _应答率_ 和 _成交率_ 两个概念。目前，正常情况下，平台上快车的应答率超过95%，成交率超过80%。</li>\n</ul>\n<hr>\n<h1 id=\"线上策略\"><a href=\"#线上策略\" class=\"headerlink\" title=\"线上策略\"></a>线上策略</h1><p>　　基于我们的总原则，下面一一讲一下现在线上使用（或进行过实验）的派单策略。</p>\n<h2 id=\"规则过滤\"><a href=\"#规则过滤\" class=\"headerlink\" title=\"规则过滤\"></a>规则过滤</h2><blockquote>\n<p>如果身边的滴滴司机没有听到你发的订单，那么他很有可能是命中了反作弊、限号等过滤规则了。若不是，可能是司机手机的网络状况出了问题，由于种种原因没有听到播单的情况也有超过1%哦。</p>\n</blockquote>\n<p>　　最基本的策略其实是人工设定的规则过滤。必须澄清的一点是这里的规则并不会造成分单时不公平的效果，而完全是为了业务能正常运行而设立的。举几个最基础的例子：</p>\n<ul>\n<li>规则A: 快车司机不能接专车订单</li>\n<li>规则B: 保证司机接单后不会通过限号区域</li>\n<li>规则C: 为设定实时目的地的司机过滤不顺路订单</li>\n<li>规则D: 为只听预约单的司机过滤实时订单</li>\n<li>规则E: 同一个订单只会发给一个司机一次</li>\n<li>。。。</li>\n</ul>\n<p>　　目前这些类似的为了保证业务正确性的人工规则已经有了超过60条，也同时承担着反作弊、拼车等业务的入口功能。当然，这种人工规则并不涉及到具体的派单策略，故这里就不展开讲了。</p>\n<h2 id=\"延迟集中分单\"><a href=\"#延迟集中分单\" class=\"headerlink\" title=\"延迟集中分单\"></a>延迟集中分单</h2><blockquote>\n<p>当前线上的派单系统，是将乘客订单和空闲司机的信息收集起来，每一秒钟进行一次全局最优的分单匹配。</p>\n</blockquote>\n<p>　　派单策略中最基础的部分，是为了解决上一节提到的时序问题。</p>\n<p>　　由于用户订单的产生和司机的出现往往并不在同一时间点，在时间维度上贪婪的分单方式（即每个订单出现时即选择附近最近的司机派单）并不能获得全局最优的效果。一个自然的想法就是先让乘客和司机稍等一会，待收集了一段时间的订单和司机信息后，再集中分配。这样，有了相对较多、较密集的订单、司机后，派单策略即可找到更近更合理的派单方式了。</p>\n<p>　　找寻司机和订单分配的全局最优是一个 _二分图匹配问题_ (bipartite graph matching) ，可用组合优化中知名的 <a href=\"https://en.wikipedia.org/wiki/Hungarian_algorithm\" target=\"_blank\" rel=\"external\">_匈牙利算法_</a> 求解。见下图。实际上，我们线上使用的是KM算法：<a href=\"https://www.topcoder.com/community/data-science/data-science-tutorials/assignment-problem-and-hungarian-algorithm/\" target=\"_blank\" rel=\"external\">_Kuhn–Munkres (KM) 算法_</a>。</p>\n<p><img src=\"/images/km_algorithm.png\" alt=\"\"></p>\n<h2 id=\"基于供需预测分单\"><a href=\"#基于供需预测分单\" class=\"headerlink\" title=\"基于供需预测分单\"></a>基于供需预测分单</h2><blockquote>\n<p>如果有先知告诉我们每个订单的生成时间和地点，派单就会变成一件很轻松的事情。</p>\n</blockquote>\n<p>　　KM算法得到的匹配结果，理论上保证是全局最优的。所以问题圆满解决，可喜可贺~~~~真的是这样吗？</p>\n<p>　　很遗憾，以上所述的延迟集中分单的策略只能解决部分的问题，仍不是一个完全的方案。其最大的问题，在于用户对系统派单的 <em>响应时间</em> 容忍度有限，很多情况下短短的几秒钟即会使用户对平台丧失信心，从而取消订单。故实际线上我们只累积了一秒钟的订单和司机信息进行集中分单，而这在大局上来说仍可近似看做时间维度上的贪婪策略。</p>\n<p>　　若想即时的获得最优派单结果，唯一的方法是利用对未来的预测，即进行基于供需预测的分单。这种想法说来玄妙，其实核心内容也很简单：如果我们预测出未来一个区域更有可能有更多的订单/司机，那么就让这个区域的司机/订单先暂缓接单。</p>\n<p><img src=\"/images/density_distribute.png\" alt=\"\"></p>\n<p>　　重新回到上一节的例子。这次我们加入了供需预测，对应于上图方框的颜色，颜色越深，代表越有可能形成新的订单。在时间1上，乘客的订单有左边和右下角两辆车均可接单，左边的司机相对来说要更近一些。但由于考虑到了未来可能产生的订单的影响，我们认为将左边的司机留下更有利于未来的订单分配，故在时间2上我们即做出决策，让右下角的司机接单。这个派单决策，直接造成了时间3上新的乘客的需求得到了响应，达到了更合理的分配效果。</p>\n<h2 id=\"连环派单\"><a href=\"#连环派单\" class=\"headerlink\" title=\"连环派单\"></a>连环派单</h2><p>　　基于供需预测的分单有很大意义，但由于预测的不确定性，其实际效果很难得到保证。为此，我们使用了一种更有确定性的预测方式来进行派单，即 _连环派单_。</p>\n<blockquote>\n<p>连环派单，即将订单指派给 _即将结束服务_ 的司机，如果司机的终点与订单位置很相近。</p>\n</blockquote>\n<p>　　与预测订单的分布相反，连环派单预测的是下一时刻空闲司机的所在位置。由于高峰期空闲司机多为司机完成订单后转换而来，预测司机的位置就变成了一个相对确定性的问题，即监测司机到目的地的距离和时间。当服务中的司机距终点很近，且终点离乘客新产生的订单也很近时，便会命中连环派单逻辑。司机在结束上一单服务后，会立刻进入新订单的接单过程中，有效地压缩了订单的应答时间、以及司机的接单距离。</p>\n<p>　　连环派单策略上线后获得了很好的效果，也说明了供需预测对订单分配的重要性。</p>\n<hr>\n<h1 id=\"看看真实数据是怎么样的\"><a href=\"#看看真实数据是怎么样的？\" class=\"headerlink\" title=\"看看真实数据是怎么样的？\"></a>看看真实数据是怎么样的？</h1><p>　　理论上的策略终究是虚的，很容易看的一头雾水。不如现在我们来探一下线上的真实数据是怎么样的！</p>\n<p>　　下图是北京某天早高峰 一个KM分单周期（如上文所述，为一秒钟）的订单与空闲司机分布情况。为隐私考虑，图中隐去了所有乘客和司机的个人信息和地图，目的为大致地观察一下线上的真实场景。</p>\n<ul>\n<li>每一格为约2kmx2km的区域；</li>\n<li>蓝色的圆圈表示这一轮派单中，完成分单  的订单；</li>\n<li>红色的圆圈表示这一轮派单中，未完成分单 的订单；</li>\n<li>绿色的小圆圈表示在这一轮派单中，所有周围有订单的空闲司机；</li>\n<li>蓝色大圆圈和绿色小圆圈的连线 代表 匹配结果。</li>\n<li>白色圆圈表示系统中存在的订单，但周围没有可接单的空闲司机。</li>\n</ul>\n<p><img src=\"/images/online_distribute_result.png\" alt=\"\"></p>\n<p>　　从图中我们可以发现一些有意思的现象：</p>\n<ul>\n<li>由于业务上过滤规则的缘故，不是所有订单都可以和所有司机匹配；</li>\n<li>订单和司机的分布很不均匀，会出现大量司机很少订单（如左上角）和大量订单很少司机（如正下方和左下角）的情况；</li>\n<li>即使单独在某一时刻看，分配成功的订单比例只占很低的部分，但由于不断有载客司机结束服务，实际上的订单分配成功率（即应答率）仍可超过90%。</li>\n</ul>\n<blockquote>\n<p>现实中订单和司机的分布不均匀是普遍存在的。</p>\n</blockquote>\n<hr>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>　　基于订单分配中的很多问题，以服务更多人为目标，数据科学家们设计了多种派单策略，并在线上实验获得了可观的效果。</p>\n<p>　　然而，当前的派单策略更多的有种各自为政，缺乏整体感和模型化的感觉。为此，我们设计了考虑未来的增强学习模型化的派单策略，将在下两节和大家分享。</p>\n","excerpt":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><p>　　在上一节的内容中，我们描述了派单过程中可能出现的问题。关于这些问题的应对方案，或多或少都在目前线上使用的派单策略中有所体现，并获得了一定的效果。这一节里我们就来聊聊线上的策略到底是什么样的～ (_公司政策所致，这里不能放具体的数据和太详细的策略执行情况，所以还是以motivation为主_)。</p>","more":"<hr>\n<h1 id=\"总原则\"><a href=\"#总原则\" class=\"headerlink\" title=\"总原则\"></a>总原则</h1><blockquote>\n<p>当前派单策略的原则是 从全局出发，保证用户的需求尽可能多的被满足。</p>\n</blockquote>\n<p>　　如上一节所说，在设计派单算法时，会涉及到一些原则或是哲学问题。在当前线上的策略中，我们的总原则是：<strong>站在全局的角度，尽可能多服务一个用户，并尽力保证用户表达的每个叫车需求可以确定性的执行</strong>。如何理解这个原则呢？</p>\n<ul>\n<li>策略是站在 <strong>全局</strong> 的角度，争取达到全局最优。这样，对每个用户自身来说，派单可能不是“局部最优”的，但大多数用户的需求可以得到保证。</li>\n<li>叫车的确定性，即用户在平台中点击呼叫后，我们希望绝大部分的用户都能有司机来响应，并最终完成订单。其分别对应 _应答率_ 和 _成交率_ 两个概念。目前，正常情况下，平台上快车的应答率超过95%，成交率超过80%。</li>\n</ul>\n<hr>\n<h1 id=\"线上策略\"><a href=\"#线上策略\" class=\"headerlink\" title=\"线上策略\"></a>线上策略</h1><p>　　基于我们的总原则，下面一一讲一下现在线上使用（或进行过实验）的派单策略。</p>\n<h2 id=\"规则过滤\"><a href=\"#规则过滤\" class=\"headerlink\" title=\"规则过滤\"></a>规则过滤</h2><blockquote>\n<p>如果身边的滴滴司机没有听到你发的订单，那么他很有可能是命中了反作弊、限号等过滤规则了。若不是，可能是司机手机的网络状况出了问题，由于种种原因没有听到播单的情况也有超过1%哦。</p>\n</blockquote>\n<p>　　最基本的策略其实是人工设定的规则过滤。必须澄清的一点是这里的规则并不会造成分单时不公平的效果，而完全是为了业务能正常运行而设立的。举几个最基础的例子：</p>\n<ul>\n<li>规则A: 快车司机不能接专车订单</li>\n<li>规则B: 保证司机接单后不会通过限号区域</li>\n<li>规则C: 为设定实时目的地的司机过滤不顺路订单</li>\n<li>规则D: 为只听预约单的司机过滤实时订单</li>\n<li>规则E: 同一个订单只会发给一个司机一次</li>\n<li>。。。</li>\n</ul>\n<p>　　目前这些类似的为了保证业务正确性的人工规则已经有了超过60条，也同时承担着反作弊、拼车等业务的入口功能。当然，这种人工规则并不涉及到具体的派单策略，故这里就不展开讲了。</p>\n<h2 id=\"延迟集中分单\"><a href=\"#延迟集中分单\" class=\"headerlink\" title=\"延迟集中分单\"></a>延迟集中分单</h2><blockquote>\n<p>当前线上的派单系统，是将乘客订单和空闲司机的信息收集起来，每一秒钟进行一次全局最优的分单匹配。</p>\n</blockquote>\n<p>　　派单策略中最基础的部分，是为了解决上一节提到的时序问题。</p>\n<p>　　由于用户订单的产生和司机的出现往往并不在同一时间点，在时间维度上贪婪的分单方式（即每个订单出现时即选择附近最近的司机派单）并不能获得全局最优的效果。一个自然的想法就是先让乘客和司机稍等一会，待收集了一段时间的订单和司机信息后，再集中分配。这样，有了相对较多、较密集的订单、司机后，派单策略即可找到更近更合理的派单方式了。</p>\n<p>　　找寻司机和订单分配的全局最优是一个 _二分图匹配问题_ (bipartite graph matching) ，可用组合优化中知名的 <a href=\"https://en.wikipedia.org/wiki/Hungarian_algorithm\">_匈牙利算法_</a> 求解。见下图。实际上，我们线上使用的是KM算法：<a href=\"https://www.topcoder.com/community/data-science/data-science-tutorials/assignment-problem-and-hungarian-algorithm/\">_Kuhn–Munkres (KM) 算法_</a>。</p>\n<p><img src=\"/images/km_algorithm.png\" alt=\"\"></p>\n<h2 id=\"基于供需预测分单\"><a href=\"#基于供需预测分单\" class=\"headerlink\" title=\"基于供需预测分单\"></a>基于供需预测分单</h2><blockquote>\n<p>如果有先知告诉我们每个订单的生成时间和地点，派单就会变成一件很轻松的事情。</p>\n</blockquote>\n<p>　　KM算法得到的匹配结果，理论上保证是全局最优的。所以问题圆满解决，可喜可贺~~~~真的是这样吗？</p>\n<p>　　很遗憾，以上所述的延迟集中分单的策略只能解决部分的问题，仍不是一个完全的方案。其最大的问题，在于用户对系统派单的 <em>响应时间</em> 容忍度有限，很多情况下短短的几秒钟即会使用户对平台丧失信心，从而取消订单。故实际线上我们只累积了一秒钟的订单和司机信息进行集中分单，而这在大局上来说仍可近似看做时间维度上的贪婪策略。</p>\n<p>　　若想即时的获得最优派单结果，唯一的方法是利用对未来的预测，即进行基于供需预测的分单。这种想法说来玄妙，其实核心内容也很简单：如果我们预测出未来一个区域更有可能有更多的订单/司机，那么就让这个区域的司机/订单先暂缓接单。</p>\n<p><img src=\"/images/density_distribute.png\" alt=\"\"></p>\n<p>　　重新回到上一节的例子。这次我们加入了供需预测，对应于上图方框的颜色，颜色越深，代表越有可能形成新的订单。在时间1上，乘客的订单有左边和右下角两辆车均可接单，左边的司机相对来说要更近一些。但由于考虑到了未来可能产生的订单的影响，我们认为将左边的司机留下更有利于未来的订单分配，故在时间2上我们即做出决策，让右下角的司机接单。这个派单决策，直接造成了时间3上新的乘客的需求得到了响应，达到了更合理的分配效果。</p>\n<h2 id=\"连环派单\"><a href=\"#连环派单\" class=\"headerlink\" title=\"连环派单\"></a>连环派单</h2><p>　　基于供需预测的分单有很大意义，但由于预测的不确定性，其实际效果很难得到保证。为此，我们使用了一种更有确定性的预测方式来进行派单，即 _连环派单_。</p>\n<blockquote>\n<p>连环派单，即将订单指派给 _即将结束服务_ 的司机，如果司机的终点与订单位置很相近。</p>\n</blockquote>\n<p>　　与预测订单的分布相反，连环派单预测的是下一时刻空闲司机的所在位置。由于高峰期空闲司机多为司机完成订单后转换而来，预测司机的位置就变成了一个相对确定性的问题，即监测司机到目的地的距离和时间。当服务中的司机距终点很近，且终点离乘客新产生的订单也很近时，便会命中连环派单逻辑。司机在结束上一单服务后，会立刻进入新订单的接单过程中，有效地压缩了订单的应答时间、以及司机的接单距离。</p>\n<p>　　连环派单策略上线后获得了很好的效果，也说明了供需预测对订单分配的重要性。</p>\n<hr>\n<h1 id=\"看看真实数据是怎么样的？\"><a href=\"#看看真实数据是怎么样的？\" class=\"headerlink\" title=\"看看真实数据是怎么样的？\"></a>看看真实数据是怎么样的？</h1><p>　　理论上的策略终究是虚的，很容易看的一头雾水。不如现在我们来探一下线上的真实数据是怎么样的！</p>\n<p>　　下图是北京某天早高峰 一个KM分单周期（如上文所述，为一秒钟）的订单与空闲司机分布情况。为隐私考虑，图中隐去了所有乘客和司机的个人信息和地图，目的为大致地观察一下线上的真实场景。</p>\n<ul>\n<li>每一格为约2kmx2km的区域；</li>\n<li>蓝色的圆圈表示这一轮派单中，完成分单  的订单；</li>\n<li>红色的圆圈表示这一轮派单中，未完成分单 的订单；</li>\n<li>绿色的小圆圈表示在这一轮派单中，所有周围有订单的空闲司机；</li>\n<li>蓝色大圆圈和绿色小圆圈的连线 代表 匹配结果。</li>\n<li>白色圆圈表示系统中存在的订单，但周围没有可接单的空闲司机。</li>\n</ul>\n<p><img src=\"/images/online_distribute_result.png\" alt=\"\"></p>\n<p>　　从图中我们可以发现一些有意思的现象：</p>\n<ul>\n<li>由于业务上过滤规则的缘故，不是所有订单都可以和所有司机匹配；</li>\n<li>订单和司机的分布很不均匀，会出现大量司机很少订单（如左上角）和大量订单很少司机（如正下方和左下角）的情况；</li>\n<li>即使单独在某一时刻看，分配成功的订单比例只占很低的部分，但由于不断有载客司机结束服务，实际上的订单分配成功率（即应答率）仍可超过90%。</li>\n</ul>\n<blockquote>\n<p>现实中订单和司机的分布不均匀是普遍存在的。</p>\n</blockquote>\n<hr>\n<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>　　基于订单分配中的很多问题，以服务更多人为目标，数据科学家们设计了多种派单策略，并在线上实验获得了可观的效果。</p>\n<p>　　然而，当前的派单策略更多的有种各自为政，缺乏整体感和模型化的感觉。为此，我们设计了考虑未来的增强学习模型化的派单策略，将在下两节和大家分享。</p>"},{"title":"滴滴派单算法解析（三）—— 模型化派单雏形","date":"2017-02-03T07:50:37.000Z","_content":"\n\n# 前言\n　　模型化派单是我们近期刚刚开展的一项创新性项目。做模型化的意义很明显：这可以使我们从繁琐的人工参数设置中解放出来，真正的以 _数据驱动_ 的方式来理解并指导派单行为。\n\n　　模型化派单的雏形，要从时空价值模型讲起。\n\n![某时刻北京专车的时空价值示例](/images/timespace_model_vis.png)\n\n<!--more-->\n\n---\n\n# 时空价值模型的来由\n　　在滴滴的交易引擎中，共存在着三种形态的个体：乘客、司机、以及平台。平台的作用，即站在宏观的角度，通过调控使得更多的订单可以完成。其中，司机作为长时间存在于平台中提供服务的个体，可认为是资源的提供方。从这个角度，平台的作用即最大限度的利用已有资源（司机），使其可服务更多的订单（乘客）。\n\n　　换句话说，平台的作用即使得平台上的司机们“忙起来”，指导其接哪些订单，向哪里调度，可以获得最多的收益。这里指的收益，不只是当前接到或未接到订单的收益，而是从长远考虑，对司机未来可能获得的总收入的一种衡量。\n\n> 时空价值模型衡量的是平台上的某个司机，若在某个时刻（如上午8点）出现在某个区域（如中关村），那么其在一个自然天内预期可产生的总流水值（如600元）。\n\n　　那么司机在某个时刻出现在哪个位置才更有可能获得更多收益呢？服务多年的老司机们常常会有自己的独家绝学，比如多去机场趴活跑长单，或是地铁站小区来回短单，亦或是夜高峰去商业区接晚下班的白领等等。计算每个区域/时间的价值是一个很复杂的问题，它至少与以下因素相关：\n- 区域的订单量\n- 区域的司机数\n- 是否是高峰期\n- 市中心的话是否拥堵\n- 是否是机场等长单多的地方\n- 是否有大企业可报销的情况\n- 是否有大型活动\n- 等等\n\n　　一个完全考虑了以上所有因素的模型几乎是不可能存在的。但是如果我们换个思路，从过去所有司机的接单记录出发，是否可以绕过以上因素，直接得到司机在某个时间/区域出现的长期价值呢？答案是肯定的。\n\n---\n\n# 时空价值模型的计算\n　　时空价值模型的计算涉及到 **增强学习(Reinforcement Learning)** 的一些基础知识，不熟悉的读者可以参考 [David Silver的公开课](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html) 以及 [Richard S. Sutton的书](http://webdocs.cs.ualberta.ca/~sutton/book/the-book.html)。\n\n> 时空价值，即将司机在平台上的行为建模成一个马尔科夫决策过程(MDP)，采用动态规划(DP)的算法，计算得到的价值(Value Function)。\n\n　　这里直观的解释一下时空价值的计算：为了衡量司机出现在某个时间/区域的价值，我们首先要考虑 _当前价值_ ，即当前状态下接单的概率及预期接单的收入；此外，我们还需考虑 _未来价值_ ，即司机接单后达到的目的地的时空价值。换句话说，我们不光希望司机当前可以更容易接单、赚钱，还会考虑这一单结束后，下一单，再下一单，再下一单，直到一天的结束的总收入。\n\n　　聪明的读者可能已经发现了，这是一个典型的动态规划问题(Dynamic Programming)，可以高效的求解。为此，我们将平台上过去三个月的司机接单（和空闲）行为进行了精细的处理，并将其封装为了半马尔科夫决策过程(semi-Markov Decision Process)的动作(action)的四元组输入：(起始状态$S$，终止状态$S'$，收益$R$，持续时间$D$)。\n\n　　我们首先将时间和空间进行量化，分别以十分钟和地理区域格子（半径约1.5km）为最小单位。假设司机在$T_0$时刻处在区域X，对应的状态为$S_0=(T_0,X)$。\n\n* 情况1：假设司机在10分钟内未接单。\n这种情况下，认为司机从$S_0$状态，经过了一个时间点，转移到了 $S_1=(T_1, X)$ 状态，并且没有得到任何收益( $R=0$ )，如下图所示。\n![](/images/tsm_wait.png)\n故状态$S_0$的价值会受到一定的损失，使其更接近于一个时间点后的状态($S_1$)的价值：\n$$ V(S_0) \\leftarrow V(S_1) $$\n\n* 情况2：假设司机接到了订单$O$，订单的终点在区域$Y$，行驶了30分钟，于时间$T_3$到达终点。\n这种情况下，司机从$S_0$状态出发，发生了到$S_2=(T_3, Y)$状态的转移，并获得了订单金额$R=R_O$的收益。\n![](/images/tsm_order.png)\n状态$S_0$的价值同样会发生变化，会更接近于订单固有价值$R_O$和订单终点价值$V(S_2)$的和。这里需要注意的一点是，即使司机接到了订单，但由于订单的终点可能是冷区（价值很低），状态$S_0$的价值反而也有可能会降低。\n$$ V(S_0) \\leftarrow V(S_2) + R_O $$\n\n　　考虑了空闲和接单两种情况，以及排除了司机出车/收车以及预约单的影响，我们得到了百万级别的出行数据，并使用动态规划的算法算出了每个时间/空间状态的价值。题图即显示了北京某时刻所有区域的价值分布。\n\n\n---\n\n# 时空价值模型的应用\n　　时空价值模型可用来指导订单分配、司机调度等任务。直观地，我们希望司机向着更容易接单，即时空价值较高的区域集中。同时，使用计算出的时空价值，我们可以量化的设定任何时间/区域的最优播单距离（即司机和乘客间距离的最远阈值）、平衡距离和供需间的不平衡等。\n\n　　目前时空价值模型已经成功的应用于快车订单升舱（即快车单会有一定概率免费享受专车司机的服务哦）中，使得完成订单后专车司机的位置更佳，从而提高了专车司机的收入。\n\n> 时空价值模型首先应用于智能升舱中，为平台节省了部分补贴花费，使更多乘客平价享受到专车的品质服务，同时提升了专车司机的收入，最高的城市有接近10%的幅度哦！\n\n---\n\n# 结论\n　　时空价值模型是向模型化分单转化的第一步，它将各种因素统一转变为了司机的收入，从而使得长时间（如一整天）的全局最优化策略变为可行。\n\n　　然而，虽然时空价值模型从真实的历史数据出发，使用了全部司机的接单记录，但其考虑的是单个司机的价值最优化，与整体派单系统的目标稍有偏差。在下一节中，我们将抛砖引玉，提出全局考虑多达上万个司机整体效率最优化算法的主要思路。\n\n","source":"_drafts/didi-distribute3.md","raw":"---\ntitle: 滴滴派单算法解析（三）—— 模型化派单雏形\ntags: didi\ncategories: didi\ndate: 2017-02-03 15:50:37\n---\n\n\n# 前言\n　　模型化派单是我们近期刚刚开展的一项创新性项目。做模型化的意义很明显：这可以使我们从繁琐的人工参数设置中解放出来，真正的以 _数据驱动_ 的方式来理解并指导派单行为。\n\n　　模型化派单的雏形，要从时空价值模型讲起。\n\n![某时刻北京专车的时空价值示例](/images/timespace_model_vis.png)\n\n<!--more-->\n\n---\n\n# 时空价值模型的来由\n　　在滴滴的交易引擎中，共存在着三种形态的个体：乘客、司机、以及平台。平台的作用，即站在宏观的角度，通过调控使得更多的订单可以完成。其中，司机作为长时间存在于平台中提供服务的个体，可认为是资源的提供方。从这个角度，平台的作用即最大限度的利用已有资源（司机），使其可服务更多的订单（乘客）。\n\n　　换句话说，平台的作用即使得平台上的司机们“忙起来”，指导其接哪些订单，向哪里调度，可以获得最多的收益。这里指的收益，不只是当前接到或未接到订单的收益，而是从长远考虑，对司机未来可能获得的总收入的一种衡量。\n\n> 时空价值模型衡量的是平台上的某个司机，若在某个时刻（如上午8点）出现在某个区域（如中关村），那么其在一个自然天内预期可产生的总流水值（如600元）。\n\n　　那么司机在某个时刻出现在哪个位置才更有可能获得更多收益呢？服务多年的老司机们常常会有自己的独家绝学，比如多去机场趴活跑长单，或是地铁站小区来回短单，亦或是夜高峰去商业区接晚下班的白领等等。计算每个区域/时间的价值是一个很复杂的问题，它至少与以下因素相关：\n- 区域的订单量\n- 区域的司机数\n- 是否是高峰期\n- 市中心的话是否拥堵\n- 是否是机场等长单多的地方\n- 是否有大企业可报销的情况\n- 是否有大型活动\n- 等等\n\n　　一个完全考虑了以上所有因素的模型几乎是不可能存在的。但是如果我们换个思路，从过去所有司机的接单记录出发，是否可以绕过以上因素，直接得到司机在某个时间/区域出现的长期价值呢？答案是肯定的。\n\n---\n\n# 时空价值模型的计算\n　　时空价值模型的计算涉及到 **增强学习(Reinforcement Learning)** 的一些基础知识，不熟悉的读者可以参考 [David Silver的公开课](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html) 以及 [Richard S. Sutton的书](http://webdocs.cs.ualberta.ca/~sutton/book/the-book.html)。\n\n> 时空价值，即将司机在平台上的行为建模成一个马尔科夫决策过程(MDP)，采用动态规划(DP)的算法，计算得到的价值(Value Function)。\n\n　　这里直观的解释一下时空价值的计算：为了衡量司机出现在某个时间/区域的价值，我们首先要考虑 _当前价值_ ，即当前状态下接单的概率及预期接单的收入；此外，我们还需考虑 _未来价值_ ，即司机接单后达到的目的地的时空价值。换句话说，我们不光希望司机当前可以更容易接单、赚钱，还会考虑这一单结束后，下一单，再下一单，再下一单，直到一天的结束的总收入。\n\n　　聪明的读者可能已经发现了，这是一个典型的动态规划问题(Dynamic Programming)，可以高效的求解。为此，我们将平台上过去三个月的司机接单（和空闲）行为进行了精细的处理，并将其封装为了半马尔科夫决策过程(semi-Markov Decision Process)的动作(action)的四元组输入：(起始状态$S$，终止状态$S'$，收益$R$，持续时间$D$)。\n\n　　我们首先将时间和空间进行量化，分别以十分钟和地理区域格子（半径约1.5km）为最小单位。假设司机在$T_0$时刻处在区域X，对应的状态为$S_0=(T_0,X)$。\n\n* 情况1：假设司机在10分钟内未接单。\n这种情况下，认为司机从$S_0$状态，经过了一个时间点，转移到了 $S_1=(T_1, X)$ 状态，并且没有得到任何收益( $R=0$ )，如下图所示。\n![](/images/tsm_wait.png)\n故状态$S_0$的价值会受到一定的损失，使其更接近于一个时间点后的状态($S_1$)的价值：\n$$ V(S_0) \\leftarrow V(S_1) $$\n\n* 情况2：假设司机接到了订单$O$，订单的终点在区域$Y$，行驶了30分钟，于时间$T_3$到达终点。\n这种情况下，司机从$S_0$状态出发，发生了到$S_2=(T_3, Y)$状态的转移，并获得了订单金额$R=R_O$的收益。\n![](/images/tsm_order.png)\n状态$S_0$的价值同样会发生变化，会更接近于订单固有价值$R_O$和订单终点价值$V(S_2)$的和。这里需要注意的一点是，即使司机接到了订单，但由于订单的终点可能是冷区（价值很低），状态$S_0$的价值反而也有可能会降低。\n$$ V(S_0) \\leftarrow V(S_2) + R_O $$\n\n　　考虑了空闲和接单两种情况，以及排除了司机出车/收车以及预约单的影响，我们得到了百万级别的出行数据，并使用动态规划的算法算出了每个时间/空间状态的价值。题图即显示了北京某时刻所有区域的价值分布。\n\n\n---\n\n# 时空价值模型的应用\n　　时空价值模型可用来指导订单分配、司机调度等任务。直观地，我们希望司机向着更容易接单，即时空价值较高的区域集中。同时，使用计算出的时空价值，我们可以量化的设定任何时间/区域的最优播单距离（即司机和乘客间距离的最远阈值）、平衡距离和供需间的不平衡等。\n\n　　目前时空价值模型已经成功的应用于快车订单升舱（即快车单会有一定概率免费享受专车司机的服务哦）中，使得完成订单后专车司机的位置更佳，从而提高了专车司机的收入。\n\n> 时空价值模型首先应用于智能升舱中，为平台节省了部分补贴花费，使更多乘客平价享受到专车的品质服务，同时提升了专车司机的收入，最高的城市有接近10%的幅度哦！\n\n---\n\n# 结论\n　　时空价值模型是向模型化分单转化的第一步，它将各种因素统一转变为了司机的收入，从而使得长时间（如一整天）的全局最优化策略变为可行。\n\n　　然而，虽然时空价值模型从真实的历史数据出发，使用了全部司机的接单记录，但其考虑的是单个司机的价值最优化，与整体派单系统的目标稍有偏差。在下一节中，我们将抛砖引玉，提出全局考虑多达上万个司机整体效率最优化算法的主要思路。\n\n","slug":"didi-distribute3","published":0,"updated":"2017-04-13T15:29:54.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1jbutcv00069qs62fcdhslk","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><p>　　模型化派单是我们近期刚刚开展的一项创新性项目。做模型化的意义很明显：这可以使我们从繁琐的人工参数设置中解放出来，真正的以 _数据驱动_ 的方式来理解并指导派单行为。</p>\n<p>　　模型化派单的雏形，要从时空价值模型讲起。</p>\n<p><img src=\"/images/timespace_model_vis.png\" alt=\"某时刻北京专车的时空价值示例\"></p>\n<a id=\"more\"></a>\n<hr>\n<h1 id=\"时空价值模型的来由\"><a href=\"#时空价值模型的来由\" class=\"headerlink\" title=\"时空价值模型的来由\"></a>时空价值模型的来由</h1><p>　　在滴滴的交易引擎中，共存在着三种形态的个体：乘客、司机、以及平台。平台的作用，即站在宏观的角度，通过调控使得更多的订单可以完成。其中，司机作为长时间存在于平台中提供服务的个体，可认为是资源的提供方。从这个角度，平台的作用即最大限度的利用已有资源（司机），使其可服务更多的订单（乘客）。</p>\n<p>　　换句话说，平台的作用即使得平台上的司机们“忙起来”，指导其接哪些订单，向哪里调度，可以获得最多的收益。这里指的收益，不只是当前接到或未接到订单的收益，而是从长远考虑，对司机未来可能获得的总收入的一种衡量。</p>\n<blockquote>\n<p>时空价值模型衡量的是平台上的某个司机，若在某个时刻（如上午8点）出现在某个区域（如中关村），那么其在一个自然天内预期可产生的总流水值（如600元）。</p>\n</blockquote>\n<p>　　那么司机在某个时刻出现在哪个位置才更有可能获得更多收益呢？服务多年的老司机们常常会有自己的独家绝学，比如多去机场趴活跑长单，或是地铁站小区来回短单，亦或是夜高峰去商业区接晚下班的白领等等。计算每个区域/时间的价值是一个很复杂的问题，它至少与以下因素相关：</p>\n<ul>\n<li>区域的订单量</li>\n<li>区域的司机数</li>\n<li>是否是高峰期</li>\n<li>市中心的话是否拥堵</li>\n<li>是否是机场等长单多的地方</li>\n<li>是否有大企业可报销的情况</li>\n<li>是否有大型活动</li>\n<li>等等</li>\n</ul>\n<p>　　一个完全考虑了以上所有因素的模型几乎是不可能存在的。但是如果我们换个思路，从过去所有司机的接单记录出发，是否可以绕过以上因素，直接得到司机在某个时间/区域出现的长期价值呢？答案是肯定的。</p>\n<hr>\n<h1 id=\"时空价值模型的计算\"><a href=\"#时空价值模型的计算\" class=\"headerlink\" title=\"时空价值模型的计算\"></a>时空价值模型的计算</h1><p>　　时空价值模型的计算涉及到 <strong>增强学习(Reinforcement Learning)</strong> 的一些基础知识，不熟悉的读者可以参考 <a href=\"http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html\" target=\"_blank\" rel=\"external\">David Silver的公开课</a> 以及 <a href=\"http://webdocs.cs.ualberta.ca/~sutton/book/the-book.html\" target=\"_blank\" rel=\"external\">Richard S. Sutton的书</a>。</p>\n<blockquote>\n<p>时空价值，即将司机在平台上的行为建模成一个马尔科夫决策过程(MDP)，采用动态规划(DP)的算法，计算得到的价值(Value Function)。</p>\n</blockquote>\n<p>　　这里直观的解释一下时空价值的计算：为了衡量司机出现在某个时间/区域的价值，我们首先要考虑 _当前价值_ ，即当前状态下接单的概率及预期接单的收入；此外，我们还需考虑 _未来价值_ ，即司机接单后达到的目的地的时空价值。换句话说，我们不光希望司机当前可以更容易接单、赚钱，还会考虑这一单结束后，下一单，再下一单，再下一单，直到一天的结束的总收入。</p>\n<p>　　聪明的读者可能已经发现了，这是一个典型的动态规划问题(Dynamic Programming)，可以高效的求解。为此，我们将平台上过去三个月的司机接单（和空闲）行为进行了精细的处理，并将其封装为了半马尔科夫决策过程(semi-Markov Decision Process)的动作(action)的四元组输入：(起始状态$S$，终止状态$S’$，收益$R$，持续时间$D$)。</p>\n<p>　　我们首先将时间和空间进行量化，分别以十分钟和地理区域格子（半径约1.5km）为最小单位。假设司机在$T_0$时刻处在区域X，对应的状态为$S_0=(T_0,X)$。</p>\n<ul>\n<li><p>情况1：假设司机在10分钟内未接单。<br>这种情况下，认为司机从$S_0$状态，经过了一个时间点，转移到了 $S_1=(T_1, X)$ 状态，并且没有得到任何收益( $R=0$ )，如下图所示。<br><img src=\"/images/tsm_wait.png\" alt=\"\"><br>故状态$S_0$的价值会受到一定的损失，使其更接近于一个时间点后的状态($S_1$)的价值：<br>$$ V(S_0) \\leftarrow V(S_1) $$</p>\n</li>\n<li><p>情况2：假设司机接到了订单$O$，订单的终点在区域$Y$，行驶了30分钟，于时间$T_3$到达终点。<br>这种情况下，司机从$S_0$状态出发，发生了到$S_2=(T_3, Y)$状态的转移，并获得了订单金额$R=R_O$的收益。<br><img src=\"/images/tsm_order.png\" alt=\"\"><br>状态$S_0$的价值同样会发生变化，会更接近于订单固有价值$R_O$和订单终点价值$V(S_2)$的和。这里需要注意的一点是，即使司机接到了订单，但由于订单的终点可能是冷区（价值很低），状态$S_0$的价值反而也有可能会降低。<br>$$ V(S_0) \\leftarrow V(S_2) + R_O $$</p>\n</li>\n</ul>\n<p>　　考虑了空闲和接单两种情况，以及排除了司机出车/收车以及预约单的影响，我们得到了百万级别的出行数据，并使用动态规划的算法算出了每个时间/空间状态的价值。题图即显示了北京某时刻所有区域的价值分布。</p>\n<hr>\n<h1 id=\"时空价值模型的应用\"><a href=\"#时空价值模型的应用\" class=\"headerlink\" title=\"时空价值模型的应用\"></a>时空价值模型的应用</h1><p>　　时空价值模型可用来指导订单分配、司机调度等任务。直观地，我们希望司机向着更容易接单，即时空价值较高的区域集中。同时，使用计算出的时空价值，我们可以量化的设定任何时间/区域的最优播单距离（即司机和乘客间距离的最远阈值）、平衡距离和供需间的不平衡等。</p>\n<p>　　目前时空价值模型已经成功的应用于快车订单升舱（即快车单会有一定概率免费享受专车司机的服务哦）中，使得完成订单后专车司机的位置更佳，从而提高了专车司机的收入。</p>\n<blockquote>\n<p>时空价值模型首先应用于智能升舱中，为平台节省了部分补贴花费，使更多乘客平价享受到专车的品质服务，同时提升了专车司机的收入，最高的城市有接近10%的幅度哦！</p>\n</blockquote>\n<hr>\n<h1 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h1><p>　　时空价值模型是向模型化分单转化的第一步，它将各种因素统一转变为了司机的收入，从而使得长时间（如一整天）的全局最优化策略变为可行。</p>\n<p>　　然而，虽然时空价值模型从真实的历史数据出发，使用了全部司机的接单记录，但其考虑的是单个司机的价值最优化，与整体派单系统的目标稍有偏差。在下一节中，我们将抛砖引玉，提出全局考虑多达上万个司机整体效率最优化算法的主要思路。</p>\n","excerpt":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><p>　　模型化派单是我们近期刚刚开展的一项创新性项目。做模型化的意义很明显：这可以使我们从繁琐的人工参数设置中解放出来，真正的以 _数据驱动_ 的方式来理解并指导派单行为。</p>\n<p>　　模型化派单的雏形，要从时空价值模型讲起。</p>\n<p><img src=\"/images/timespace_model_vis.png\" alt=\"某时刻北京专车的时空价值示例\"></p>","more":"<hr>\n<h1 id=\"时空价值模型的来由\"><a href=\"#时空价值模型的来由\" class=\"headerlink\" title=\"时空价值模型的来由\"></a>时空价值模型的来由</h1><p>　　在滴滴的交易引擎中，共存在着三种形态的个体：乘客、司机、以及平台。平台的作用，即站在宏观的角度，通过调控使得更多的订单可以完成。其中，司机作为长时间存在于平台中提供服务的个体，可认为是资源的提供方。从这个角度，平台的作用即最大限度的利用已有资源（司机），使其可服务更多的订单（乘客）。</p>\n<p>　　换句话说，平台的作用即使得平台上的司机们“忙起来”，指导其接哪些订单，向哪里调度，可以获得最多的收益。这里指的收益，不只是当前接到或未接到订单的收益，而是从长远考虑，对司机未来可能获得的总收入的一种衡量。</p>\n<blockquote>\n<p>时空价值模型衡量的是平台上的某个司机，若在某个时刻（如上午8点）出现在某个区域（如中关村），那么其在一个自然天内预期可产生的总流水值（如600元）。</p>\n</blockquote>\n<p>　　那么司机在某个时刻出现在哪个位置才更有可能获得更多收益呢？服务多年的老司机们常常会有自己的独家绝学，比如多去机场趴活跑长单，或是地铁站小区来回短单，亦或是夜高峰去商业区接晚下班的白领等等。计算每个区域/时间的价值是一个很复杂的问题，它至少与以下因素相关：</p>\n<ul>\n<li>区域的订单量</li>\n<li>区域的司机数</li>\n<li>是否是高峰期</li>\n<li>市中心的话是否拥堵</li>\n<li>是否是机场等长单多的地方</li>\n<li>是否有大企业可报销的情况</li>\n<li>是否有大型活动</li>\n<li>等等</li>\n</ul>\n<p>　　一个完全考虑了以上所有因素的模型几乎是不可能存在的。但是如果我们换个思路，从过去所有司机的接单记录出发，是否可以绕过以上因素，直接得到司机在某个时间/区域出现的长期价值呢？答案是肯定的。</p>\n<hr>\n<h1 id=\"时空价值模型的计算\"><a href=\"#时空价值模型的计算\" class=\"headerlink\" title=\"时空价值模型的计算\"></a>时空价值模型的计算</h1><p>　　时空价值模型的计算涉及到 <strong>增强学习(Reinforcement Learning)</strong> 的一些基础知识，不熟悉的读者可以参考 <a href=\"http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html\">David Silver的公开课</a> 以及 <a href=\"http://webdocs.cs.ualberta.ca/~sutton/book/the-book.html\">Richard S. Sutton的书</a>。</p>\n<blockquote>\n<p>时空价值，即将司机在平台上的行为建模成一个马尔科夫决策过程(MDP)，采用动态规划(DP)的算法，计算得到的价值(Value Function)。</p>\n</blockquote>\n<p>　　这里直观的解释一下时空价值的计算：为了衡量司机出现在某个时间/区域的价值，我们首先要考虑 _当前价值_ ，即当前状态下接单的概率及预期接单的收入；此外，我们还需考虑 _未来价值_ ，即司机接单后达到的目的地的时空价值。换句话说，我们不光希望司机当前可以更容易接单、赚钱，还会考虑这一单结束后，下一单，再下一单，再下一单，直到一天的结束的总收入。</p>\n<p>　　聪明的读者可能已经发现了，这是一个典型的动态规划问题(Dynamic Programming)，可以高效的求解。为此，我们将平台上过去三个月的司机接单（和空闲）行为进行了精细的处理，并将其封装为了半马尔科夫决策过程(semi-Markov Decision Process)的动作(action)的四元组输入：(起始状态$S$，终止状态$S’$，收益$R$，持续时间$D$)。</p>\n<p>　　我们首先将时间和空间进行量化，分别以十分钟和地理区域格子（半径约1.5km）为最小单位。假设司机在$T_0$时刻处在区域X，对应的状态为$S_0=(T_0,X)$。</p>\n<ul>\n<li><p>情况1：假设司机在10分钟内未接单。<br>这种情况下，认为司机从$S_0$状态，经过了一个时间点，转移到了 $S_1=(T_1, X)$ 状态，并且没有得到任何收益( $R=0$ )，如下图所示。<br><img src=\"/images/tsm_wait.png\" alt=\"\"><br>故状态$S_0$的价值会受到一定的损失，使其更接近于一个时间点后的状态($S_1$)的价值：<br>$$ V(S_0) \\leftarrow V(S_1) $$</p>\n</li>\n<li><p>情况2：假设司机接到了订单$O$，订单的终点在区域$Y$，行驶了30分钟，于时间$T_3$到达终点。<br>这种情况下，司机从$S_0$状态出发，发生了到$S_2=(T_3, Y)$状态的转移，并获得了订单金额$R=R_O$的收益。<br><img src=\"/images/tsm_order.png\" alt=\"\"><br>状态$S_0$的价值同样会发生变化，会更接近于订单固有价值$R_O$和订单终点价值$V(S_2)$的和。这里需要注意的一点是，即使司机接到了订单，但由于订单的终点可能是冷区（价值很低），状态$S_0$的价值反而也有可能会降低。<br>$$ V(S_0) \\leftarrow V(S_2) + R_O $$</p>\n</li>\n</ul>\n<p>　　考虑了空闲和接单两种情况，以及排除了司机出车/收车以及预约单的影响，我们得到了百万级别的出行数据，并使用动态规划的算法算出了每个时间/空间状态的价值。题图即显示了北京某时刻所有区域的价值分布。</p>\n<hr>\n<h1 id=\"时空价值模型的应用\"><a href=\"#时空价值模型的应用\" class=\"headerlink\" title=\"时空价值模型的应用\"></a>时空价值模型的应用</h1><p>　　时空价值模型可用来指导订单分配、司机调度等任务。直观地，我们希望司机向着更容易接单，即时空价值较高的区域集中。同时，使用计算出的时空价值，我们可以量化的设定任何时间/区域的最优播单距离（即司机和乘客间距离的最远阈值）、平衡距离和供需间的不平衡等。</p>\n<p>　　目前时空价值模型已经成功的应用于快车订单升舱（即快车单会有一定概率免费享受专车司机的服务哦）中，使得完成订单后专车司机的位置更佳，从而提高了专车司机的收入。</p>\n<blockquote>\n<p>时空价值模型首先应用于智能升舱中，为平台节省了部分补贴花费，使更多乘客平价享受到专车的品质服务，同时提升了专车司机的收入，最高的城市有接近10%的幅度哦！</p>\n</blockquote>\n<hr>\n<h1 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h1><p>　　时空价值模型是向模型化分单转化的第一步，它将各种因素统一转变为了司机的收入，从而使得长时间（如一整天）的全局最优化策略变为可行。</p>\n<p>　　然而，虽然时空价值模型从真实的历史数据出发，使用了全部司机的接单记录，但其考虑的是单个司机的价值最优化，与整体派单系统的目标稍有偏差。在下一节中，我们将抛砖引玉，提出全局考虑多达上万个司机整体效率最优化算法的主要思路。</p>"},{"title":"Hello World","_content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n<!-- More -->\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n\n## Unit Tests\n### Try to insert an equation\n$$ X = \\frac{1}{Y} \\mathcal{Q}(S) $$\n\n### Quote test\n{% blockquote Andrej Karparphy http://karpathy.github.io/2016/05/31/rl/ %}\nRL is hot!\n{% endblockquote %}\n\n### Code test\n{% codeblock lang:python %}\nimport pandas as pd\nimport gym\n{% endcodeblock %}\n\n### Quote test2\n>I've said that it needs more practice.\n","source":"_drafts/hello-world.md","raw":"---\ntitle: Hello World\ntags: [test, RL, code]\ncategories: [test]\n---\nWelcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n<!-- More -->\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n\n## Unit Tests\n### Try to insert an equation\n$$ X = \\frac{1}{Y} \\mathcal{Q}(S) $$\n\n### Quote test\n{% blockquote Andrej Karparphy http://karpathy.github.io/2016/05/31/rl/ %}\nRL is hot!\n{% endblockquote %}\n\n### Code test\n{% codeblock lang:python %}\nimport pandas as pd\nimport gym\n{% endcodeblock %}\n\n### Quote test2\n>I've said that it needs more practice.\n","slug":"hello-world","published":0,"date":"2017-04-13T15:29:54.000Z","updated":"2017-04-13T15:29:54.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1jbutcz00079qs6nra6q4fq","content":"<p>Welcome to <a href=\"https://hexo.io/\" target=\"_blank\" rel=\"external\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\" target=\"_blank\" rel=\"external\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\" target=\"_blank\" rel=\"external\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\" target=\"_blank\" rel=\"external\">GitHub</a>.</p>\n<a id=\"more\"></a>\n<h2 id=\"quick-start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo new <span class=\"string\">\"My New Post\"</span></div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\" target=\"_blank\" rel=\"external\">Writing</a></p>\n<h3 id=\"run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo server</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\" target=\"_blank\" rel=\"external\">Server</a></p>\n<h3 id=\"generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo generate</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\" target=\"_blank\" rel=\"external\">Generating</a></p>\n<h3 id=\"deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo deploy</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/deployment.html\" target=\"_blank\" rel=\"external\">Deployment</a></p>\n<h2 id=\"unit-tests\"><a href=\"#Unit-Tests\" class=\"headerlink\" title=\"Unit Tests\"></a>Unit Tests</h2><h3 id=\"try-to-insert-an-equation\"><a href=\"#Try-to-insert-an-equation\" class=\"headerlink\" title=\"Try to insert an equation\"></a>Try to insert an equation</h3><p>$$ X = \\frac{1}{Y} \\mathcal{Q}(S) $$</p>\n<h3 id=\"quote-test\"><a href=\"#Quote-test\" class=\"headerlink\" title=\"Quote test\"></a>Quote test</h3><blockquote><p>RL is hot!</p>\n<footer><strong>Andrej Karparphy</strong><cite><a href=\"http://karpathy.github.io/2016/05/31/rl/\" target=\"_blank\" rel=\"external\">karpathy.github.io/2016/05/31/rl</a></cite></footer></blockquote>\n<h3 id=\"code-test\"><a href=\"#Code-test\" class=\"headerlink\" title=\"Code test\"></a>Code test</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</div><div class=\"line\"><span class=\"keyword\">import</span> gym</div></pre></td></tr></table></figure>\n<h3 id=\"quote-test2\"><a href=\"#Quote-test2\" class=\"headerlink\" title=\"Quote test2\"></a>Quote test2</h3><blockquote>\n<p>I’ve said that it needs more practice.</p>\n</blockquote>\n","excerpt":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>","more":"<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo new <span class=\"string\">\"My New Post\"</span></div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo server</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo generate</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo deploy</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/deployment.html\">Deployment</a></p>\n<h2 id=\"Unit-Tests\"><a href=\"#Unit-Tests\" class=\"headerlink\" title=\"Unit Tests\"></a>Unit Tests</h2><h3 id=\"Try-to-insert-an-equation\"><a href=\"#Try-to-insert-an-equation\" class=\"headerlink\" title=\"Try to insert an equation\"></a>Try to insert an equation</h3><p>$$ X = \\frac{1}{Y} \\mathcal{Q}(S) $$</p>\n<h3 id=\"Quote-test\"><a href=\"#Quote-test\" class=\"headerlink\" title=\"Quote test\"></a>Quote test</h3><blockquote><p>RL is hot!</p>\n<footer><strong>Andrej Karparphy</strong><cite><a href=\"http://karpathy.github.io/2016/05/31/rl/\">karpathy.github.io/2016/05/31/rl</a></cite></footer></blockquote>\n<h3 id=\"Code-test\"><a href=\"#Code-test\" class=\"headerlink\" title=\"Code test\"></a>Code test</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</div><div class=\"line\"><span class=\"keyword\">import</span> gym</div></pre></td></tr></table></figure>\n<h3 id=\"Quote-test2\"><a href=\"#Quote-test2\" class=\"headerlink\" title=\"Quote test2\"></a>Quote test2</h3><blockquote>\n<p>I’ve said that it needs more practice.</p>\n</blockquote>"},{"title":"【DRL论文阅读】Multi-Agent Cooperation and the Emergence of (Natural) Language Learning","date":"2017-04-15T13:57:39.000Z","_content":"\n\n![](/images/multi_agent_conversation/title.png)\n\n> 解决的问题：两个机器人之间玩“你画我猜”\n使用方法：RL (Reinforce), t-SNE可视化分析\n亮点：很有意思的应用，让两个agent玩游戏，观察agents间的交互方式\n\n<!--more-->\n\n---\n\n# 文章简介\n\n　　这两天尝试啃了几篇非常理论的Paper之后，发现还是找一些更简单、应用更有意思的paper看起来比较轻松~ 于是今天来聊一下一篇很有意思的论文：[\"Multi-Agent Cooperation and the Emergence of (Natural) Language\"](https://arxiv.org/pdf/1612.07182.pdf)，来自Deepmind和Facebook AI Research的合作。\n\n　　这篇文章大致做了这么一件事情：让两个agent分别扮演 *描述者* 和 *猜题者* 两个角色共同玩一个游戏。描述者看到两张不同concept的图片（分别是目标图片和干扰图片），并得知哪张图是目标图片。游戏的目标是让描述者提供一个词（词库由两个agent提前商量好，类似于暗号），使得猜题者可以通过这个词成功的分别两张图片，找到目标图片。设定有点像“你画我猜”或者是“看动作猜词语”这种游戏。\n\n　　之前我们在研究中更多地都是focus在supervised learning上，而却忽视了通往通用AI之路的一个重要问题：AI和他人协同、交互的能力。这篇文章用很简单的方法，测试了AI间协作交互的能力，并试图分析这种交互，乃至干预AI间交互的形式变成人类可理解的语言，非常有意思。\n\n> AI能否学到如何与同伴通信并互相理解？\n\n---\n\n# 游戏流程与算法\n\n　　这篇文章的想法非常简单，就是一个常用的Reinforce搞定。于是我们重点来看一下作者是如何构造的游戏使得AI间可以协同合作。文章里使用了一种叫做 *referential games* 的设定：\n\n- 首先建立一个图片集$i_1,...,i_N$，它们分别来自于463个基础概念（concept，如猫、苹果、汽车等）、20个大类，每个concept从ImageNet库里拉了100张图片（*论万能的ImageNet...*）。每一次游戏开始时，系统随机的选择两个concept，并分别选择一张图片 $(i_L,i_R)$，把其中的一个作为目标 $t\\in $\\{$ L,R $\\}，另一个作为伪装。\n\n- 游戏中共有两个角色：*描述者* 和 *猜题者* 。他们都能看到两张输入图片，但是只有描述者知道哪个是目标图片，即收到了输入 $\\theta_S(i_L,i_R,t)$。\n\n- 描述者和猜题者共同维护一个暗号集 $V$（size为$K$）。描述者选择 $V$ 中的一个暗号 $s$ 传给猜题者。描述者的策略记为：$s(\\theta_S(i_L,i_R,t)) \\in V$。\n\n- 猜题者并不清楚哪张图是目标图片，于是他根据描述者传来的信息和两张图片本身来猜测目标图片。猜题者的策略记为：$r\\big(i_L,i_R,s(\\theta_S(i_L,i_R,t))\\big) \\in $\\{$ L,R $\\} 。\n\n- 如果猜题者猜对了目标图片，即$r\\big(i_L,i_R,s(\\theta_S(i_L,i_R,t))\\big) = t$，描述者和猜题者同时加一分（胜利）；否则都不加分（失败）。\n\n　　作者建立了两种简单的图像embedding方式，分别是（如下图所示）\n1) 直接将 VGG的softmax层输出（或fc层输出）embedding。称为 *agnostic* 方法。\n2) 将输出再过一层2x1的卷积层降维，把目标图和伪装图的输出混合起来，最后再embedding。称为 *informed* 方法。\n\n![](/images/multi_agent_conversation/architecture.png)\n\n　　建立了policy和reward，玩游戏的问题自然地被转化成了RL过程，作者使用了Reinforce（最简单的policy gradient方法之一）算法来求解。结果很愉快，所有的agent都在10k轮之内成功收敛了，且答对率接近100%！其中，*informed* 方法相比于 *agnostic* 方法由于使用了更多的信息，可以收敛更快并更准。\n\n　　当然了，这个游戏本身其实挺简单的（concept之间差的挺大的，imagenet的vgg特征也足够强），所以performance倒并不是最重要的，更有意义的是去分析 agent之间到底是如何(chuan)交(an)互(hao)的，以为这种交互方式是否能被我们人类所理解？\n\n> 试想如果有一天 AI 之间发明了一种特殊的暗号，人类完全破译不能。。。 orz \n\n---\n\n# 了解AI间的交互方式\n\n　　为了潜入AI的脑内，分析其与同伴打招呼的方式，作者设计了几种trick来证明或可视化他们的假设。下面我们简单看看作者们的发现：\n\n> AI 有丰富的描述语言，并能抓住重点。\n\n　　作者的第一个发现，就是 *informed sender* 使用了较多种的词语作为暗号来通信，且这些暗号中至少有很大一部分代表着不同的意义。同时，这些暗号的意义与人类定义的20种大类有着一定的相关性（最高的一种算法的聚类purity有46%）。\n　　另一方面，*agnostic sender* 弱弱地只使用过两个词作为暗号。作者肉眼看的结果是这个暗号类似于 “能喘气的 vs. 不动换的” (living-vs-non-living)，也确实符合imagenet“动物园”的特性。\n\n> 可以通过“去常识化”使得 AI 的词语与人类的认知进一步相似\n\n　　用博弈论的调调来说，常识指所有人都知道的，以及所有人都知道所有人都知道的。。。把输入中的常识分量去掉，可以使得agent学会更本质的东西。作者的实现方法很tricky，给描述者和猜题者看了两张很相近但确不完全相同的图片（比如目标concept是狗，两个agent一个看到的是吉娃娃，另一个则看到的是波士顿小狗）。结果是agent的暗号与人类定义的类别更相近了。这点其实我没太理解。。。\n\n![](/images/multi_agent_conversation/tsne.png \"去常识化之前（左）和之后（右）的AI暗号聚类示意图，同样颜色对应着人类标识的相同大类\")\n\n> 可以通过加入监督学习的方式使得 AI 用人类的词语通信\n\n　　最后，当然我们可以用监督学习的方式，显式地让 AI 学会人类的语言来通信。作者在最后一个实验中，迭代地进行增强学习来优化agent和监督学习来让agent学会人类语言。评价时，作者雇了一群真正的人来当猜题者，并由AI生成的词语（监督学习中对应着imagenet的label）作为提示。结果是68%的人能猜到正确的图片（心疼这些人几秒钟。。。），似乎比预想的要低一些，但至少能大幅超过random guess。\n\n---\n\n# 结论\n\n　　总体来说，这篇文章提出了一个很有意思的应用，依此研究了AI间交互的可能性与其形式。理论算法和实验的结果其实都并不是很强，相信还有很大的提升空间。\n\n---\n\n** 最近访问 ** \n\n<div class=\"ds-recent-visitors\"\n    data-num-items=\"36\"\n    data-avatar-size=\"42\"\n    id=\"ds-recent-visitors\">\n</div>","source":"_posts/iclr17-multi-agent-conversation.md","raw":"---\ntitle: >-\n  【DRL论文阅读】Multi-Agent Cooperation and the Emergence of (Natural) Language\n  Learning\ntags: drl\ncategories: drl\ndate: 2017-04-15 21:57:39\n---\n\n\n![](/images/multi_agent_conversation/title.png)\n\n> 解决的问题：两个机器人之间玩“你画我猜”\n使用方法：RL (Reinforce), t-SNE可视化分析\n亮点：很有意思的应用，让两个agent玩游戏，观察agents间的交互方式\n\n<!--more-->\n\n---\n\n# 文章简介\n\n　　这两天尝试啃了几篇非常理论的Paper之后，发现还是找一些更简单、应用更有意思的paper看起来比较轻松~ 于是今天来聊一下一篇很有意思的论文：[\"Multi-Agent Cooperation and the Emergence of (Natural) Language\"](https://arxiv.org/pdf/1612.07182.pdf)，来自Deepmind和Facebook AI Research的合作。\n\n　　这篇文章大致做了这么一件事情：让两个agent分别扮演 *描述者* 和 *猜题者* 两个角色共同玩一个游戏。描述者看到两张不同concept的图片（分别是目标图片和干扰图片），并得知哪张图是目标图片。游戏的目标是让描述者提供一个词（词库由两个agent提前商量好，类似于暗号），使得猜题者可以通过这个词成功的分别两张图片，找到目标图片。设定有点像“你画我猜”或者是“看动作猜词语”这种游戏。\n\n　　之前我们在研究中更多地都是focus在supervised learning上，而却忽视了通往通用AI之路的一个重要问题：AI和他人协同、交互的能力。这篇文章用很简单的方法，测试了AI间协作交互的能力，并试图分析这种交互，乃至干预AI间交互的形式变成人类可理解的语言，非常有意思。\n\n> AI能否学到如何与同伴通信并互相理解？\n\n---\n\n# 游戏流程与算法\n\n　　这篇文章的想法非常简单，就是一个常用的Reinforce搞定。于是我们重点来看一下作者是如何构造的游戏使得AI间可以协同合作。文章里使用了一种叫做 *referential games* 的设定：\n\n- 首先建立一个图片集$i_1,...,i_N$，它们分别来自于463个基础概念（concept，如猫、苹果、汽车等）、20个大类，每个concept从ImageNet库里拉了100张图片（*论万能的ImageNet...*）。每一次游戏开始时，系统随机的选择两个concept，并分别选择一张图片 $(i_L,i_R)$，把其中的一个作为目标 $t\\in $\\{$ L,R $\\}，另一个作为伪装。\n\n- 游戏中共有两个角色：*描述者* 和 *猜题者* 。他们都能看到两张输入图片，但是只有描述者知道哪个是目标图片，即收到了输入 $\\theta_S(i_L,i_R,t)$。\n\n- 描述者和猜题者共同维护一个暗号集 $V$（size为$K$）。描述者选择 $V$ 中的一个暗号 $s$ 传给猜题者。描述者的策略记为：$s(\\theta_S(i_L,i_R,t)) \\in V$。\n\n- 猜题者并不清楚哪张图是目标图片，于是他根据描述者传来的信息和两张图片本身来猜测目标图片。猜题者的策略记为：$r\\big(i_L,i_R,s(\\theta_S(i_L,i_R,t))\\big) \\in $\\{$ L,R $\\} 。\n\n- 如果猜题者猜对了目标图片，即$r\\big(i_L,i_R,s(\\theta_S(i_L,i_R,t))\\big) = t$，描述者和猜题者同时加一分（胜利）；否则都不加分（失败）。\n\n　　作者建立了两种简单的图像embedding方式，分别是（如下图所示）\n1) 直接将 VGG的softmax层输出（或fc层输出）embedding。称为 *agnostic* 方法。\n2) 将输出再过一层2x1的卷积层降维，把目标图和伪装图的输出混合起来，最后再embedding。称为 *informed* 方法。\n\n![](/images/multi_agent_conversation/architecture.png)\n\n　　建立了policy和reward，玩游戏的问题自然地被转化成了RL过程，作者使用了Reinforce（最简单的policy gradient方法之一）算法来求解。结果很愉快，所有的agent都在10k轮之内成功收敛了，且答对率接近100%！其中，*informed* 方法相比于 *agnostic* 方法由于使用了更多的信息，可以收敛更快并更准。\n\n　　当然了，这个游戏本身其实挺简单的（concept之间差的挺大的，imagenet的vgg特征也足够强），所以performance倒并不是最重要的，更有意义的是去分析 agent之间到底是如何(chuan)交(an)互(hao)的，以为这种交互方式是否能被我们人类所理解？\n\n> 试想如果有一天 AI 之间发明了一种特殊的暗号，人类完全破译不能。。。 orz \n\n---\n\n# 了解AI间的交互方式\n\n　　为了潜入AI的脑内，分析其与同伴打招呼的方式，作者设计了几种trick来证明或可视化他们的假设。下面我们简单看看作者们的发现：\n\n> AI 有丰富的描述语言，并能抓住重点。\n\n　　作者的第一个发现，就是 *informed sender* 使用了较多种的词语作为暗号来通信，且这些暗号中至少有很大一部分代表着不同的意义。同时，这些暗号的意义与人类定义的20种大类有着一定的相关性（最高的一种算法的聚类purity有46%）。\n　　另一方面，*agnostic sender* 弱弱地只使用过两个词作为暗号。作者肉眼看的结果是这个暗号类似于 “能喘气的 vs. 不动换的” (living-vs-non-living)，也确实符合imagenet“动物园”的特性。\n\n> 可以通过“去常识化”使得 AI 的词语与人类的认知进一步相似\n\n　　用博弈论的调调来说，常识指所有人都知道的，以及所有人都知道所有人都知道的。。。把输入中的常识分量去掉，可以使得agent学会更本质的东西。作者的实现方法很tricky，给描述者和猜题者看了两张很相近但确不完全相同的图片（比如目标concept是狗，两个agent一个看到的是吉娃娃，另一个则看到的是波士顿小狗）。结果是agent的暗号与人类定义的类别更相近了。这点其实我没太理解。。。\n\n![](/images/multi_agent_conversation/tsne.png \"去常识化之前（左）和之后（右）的AI暗号聚类示意图，同样颜色对应着人类标识的相同大类\")\n\n> 可以通过加入监督学习的方式使得 AI 用人类的词语通信\n\n　　最后，当然我们可以用监督学习的方式，显式地让 AI 学会人类的语言来通信。作者在最后一个实验中，迭代地进行增强学习来优化agent和监督学习来让agent学会人类语言。评价时，作者雇了一群真正的人来当猜题者，并由AI生成的词语（监督学习中对应着imagenet的label）作为提示。结果是68%的人能猜到正确的图片（心疼这些人几秒钟。。。），似乎比预想的要低一些，但至少能大幅超过random guess。\n\n---\n\n# 结论\n\n　　总体来说，这篇文章提出了一个很有意思的应用，依此研究了AI间交互的可能性与其形式。理论算法和实验的结果其实都并不是很强，相信还有很大的提升空间。\n\n---\n\n** 最近访问 ** \n\n<div class=\"ds-recent-visitors\"\n    data-num-items=\"36\"\n    data-avatar-size=\"42\"\n    id=\"ds-recent-visitors\">\n</div>","slug":"iclr17-multi-agent-conversation","published":1,"updated":"2017-04-15T13:57:39.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1jbutd100089qs6mxj50ssp","content":"<p><img src=\"/images/multi_agent_conversation/title.png\" alt=\"\"></p>\n<blockquote>\n<p>解决的问题：两个机器人之间玩“你画我猜”<br>使用方法：RL (Reinforce), t-SNE可视化分析<br>亮点：很有意思的应用，让两个agent玩游戏，观察agents间的交互方式</p>\n</blockquote>\n<a id=\"more\"></a>\n<hr>\n<h1 id=\"文章简介\"><a href=\"#文章简介\" class=\"headerlink\" title=\"文章简介\"></a>文章简介</h1><p>　　这两天尝试啃了几篇非常理论的Paper之后，发现还是找一些更简单、应用更有意思的paper看起来比较轻松~ 于是今天来聊一下一篇很有意思的论文：<a href=\"https://arxiv.org/pdf/1612.07182.pdf\" target=\"_blank\" rel=\"external\">“Multi-Agent Cooperation and the Emergence of (Natural) Language”</a>，来自Deepmind和Facebook AI Research的合作。</p>\n<p>　　这篇文章大致做了这么一件事情：让两个agent分别扮演 <em>描述者</em> 和 <em>猜题者</em> 两个角色共同玩一个游戏。描述者看到两张不同concept的图片（分别是目标图片和干扰图片），并得知哪张图是目标图片。游戏的目标是让描述者提供一个词（词库由两个agent提前商量好，类似于暗号），使得猜题者可以通过这个词成功的分别两张图片，找到目标图片。设定有点像“你画我猜”或者是“看动作猜词语”这种游戏。</p>\n<p>　　之前我们在研究中更多地都是focus在supervised learning上，而却忽视了通往通用AI之路的一个重要问题：AI和他人协同、交互的能力。这篇文章用很简单的方法，测试了AI间协作交互的能力，并试图分析这种交互，乃至干预AI间交互的形式变成人类可理解的语言，非常有意思。</p>\n<blockquote>\n<p>AI能否学到如何与同伴通信并互相理解？</p>\n</blockquote>\n<hr>\n<h1 id=\"游戏流程与算法\"><a href=\"#游戏流程与算法\" class=\"headerlink\" title=\"游戏流程与算法\"></a>游戏流程与算法</h1><p>　　这篇文章的想法非常简单，就是一个常用的Reinforce搞定。于是我们重点来看一下作者是如何构造的游戏使得AI间可以协同合作。文章里使用了一种叫做 <em>referential games</em> 的设定：</p>\n<ul>\n<li><p>首先建立一个图片集$i_1,…,i_N$，它们分别来自于463个基础概念（concept，如猫、苹果、汽车等）、20个大类，每个concept从ImageNet库里拉了100张图片（<em>论万能的ImageNet…</em>）。每一次游戏开始时，系统随机的选择两个concept，并分别选择一张图片 $(i_L,i_R)$，把其中的一个作为目标 $t\\in ${$ L,R $}，另一个作为伪装。</p>\n</li>\n<li><p>游戏中共有两个角色：<em>描述者</em> 和 <em>猜题者</em> 。他们都能看到两张输入图片，但是只有描述者知道哪个是目标图片，即收到了输入 $\\theta_S(i_L,i_R,t)$。</p>\n</li>\n<li><p>描述者和猜题者共同维护一个暗号集 $V$（size为$K$）。描述者选择 $V$ 中的一个暗号 $s$ 传给猜题者。描述者的策略记为：$s(\\theta_S(i_L,i_R,t)) \\in V$。</p>\n</li>\n<li><p>猜题者并不清楚哪张图是目标图片，于是他根据描述者传来的信息和两张图片本身来猜测目标图片。猜题者的策略记为：$r\\big(i_L,i_R,s(\\theta_S(i_L,i_R,t))\\big) \\in ${$ L,R $} 。</p>\n</li>\n<li><p>如果猜题者猜对了目标图片，即$r\\big(i_L,i_R,s(\\theta_S(i_L,i_R,t))\\big) = t$，描述者和猜题者同时加一分（胜利）；否则都不加分（失败）。</p>\n</li>\n</ul>\n<p>　　作者建立了两种简单的图像embedding方式，分别是（如下图所示）<br>1) 直接将 VGG的softmax层输出（或fc层输出）embedding。称为 <em>agnostic</em> 方法。<br>2) 将输出再过一层2x1的卷积层降维，把目标图和伪装图的输出混合起来，最后再embedding。称为 <em>informed</em> 方法。</p>\n<p><img src=\"/images/multi_agent_conversation/architecture.png\" alt=\"\"></p>\n<p>　　建立了policy和reward，玩游戏的问题自然地被转化成了RL过程，作者使用了Reinforce（最简单的policy gradient方法之一）算法来求解。结果很愉快，所有的agent都在10k轮之内成功收敛了，且答对率接近100%！其中，<em>informed</em> 方法相比于 <em>agnostic</em> 方法由于使用了更多的信息，可以收敛更快并更准。</p>\n<p>　　当然了，这个游戏本身其实挺简单的（concept之间差的挺大的，imagenet的vgg特征也足够强），所以performance倒并不是最重要的，更有意义的是去分析 agent之间到底是如何(chuan)交(an)互(hao)的，以为这种交互方式是否能被我们人类所理解？</p>\n<blockquote>\n<p>试想如果有一天 AI 之间发明了一种特殊的暗号，人类完全破译不能。。。 orz </p>\n</blockquote>\n<hr>\n<h1 id=\"了解ai间的交互方式\"><a href=\"#了解AI间的交互方式\" class=\"headerlink\" title=\"了解AI间的交互方式\"></a>了解AI间的交互方式</h1><p>　　为了潜入AI的脑内，分析其与同伴打招呼的方式，作者设计了几种trick来证明或可视化他们的假设。下面我们简单看看作者们的发现：</p>\n<blockquote>\n<p>AI 有丰富的描述语言，并能抓住重点。</p>\n</blockquote>\n<p>　　作者的第一个发现，就是 <em>informed sender</em> 使用了较多种的词语作为暗号来通信，且这些暗号中至少有很大一部分代表着不同的意义。同时，这些暗号的意义与人类定义的20种大类有着一定的相关性（最高的一种算法的聚类purity有46%）。<br>　　另一方面，<em>agnostic sender</em> 弱弱地只使用过两个词作为暗号。作者肉眼看的结果是这个暗号类似于 “能喘气的 vs. 不动换的” (living-vs-non-living)，也确实符合imagenet“动物园”的特性。</p>\n<blockquote>\n<p>可以通过“去常识化”使得 AI 的词语与人类的认知进一步相似</p>\n</blockquote>\n<p>　　用博弈论的调调来说，常识指所有人都知道的，以及所有人都知道所有人都知道的。。。把输入中的常识分量去掉，可以使得agent学会更本质的东西。作者的实现方法很tricky，给描述者和猜题者看了两张很相近但确不完全相同的图片（比如目标concept是狗，两个agent一个看到的是吉娃娃，另一个则看到的是波士顿小狗）。结果是agent的暗号与人类定义的类别更相近了。这点其实我没太理解。。。</p>\n<p><img src=\"/images/multi_agent_conversation/tsne.png\" alt=\"\" title=\"去常识化之前（左）和之后（右）的AI暗号聚类示意图，同样颜色对应着人类标识的相同大类\"></p>\n<blockquote>\n<p>可以通过加入监督学习的方式使得 AI 用人类的词语通信</p>\n</blockquote>\n<p>　　最后，当然我们可以用监督学习的方式，显式地让 AI 学会人类的语言来通信。作者在最后一个实验中，迭代地进行增强学习来优化agent和监督学习来让agent学会人类语言。评价时，作者雇了一群真正的人来当猜题者，并由AI生成的词语（监督学习中对应着imagenet的label）作为提示。结果是68%的人能猜到正确的图片（心疼这些人几秒钟。。。），似乎比预想的要低一些，但至少能大幅超过random guess。</p>\n<hr>\n<h1 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h1><p>　　总体来说，这篇文章提出了一个很有意思的应用，依此研究了AI间交互的可能性与其形式。理论算法和实验的结果其实都并不是很强，相信还有很大的提升空间。</p>\n<hr>\n<p><strong> 最近访问 </strong> </p>\n<div class=\"ds-recent-visitors\" data-num-items=\"36\" data-avatar-size=\"42\" id=\"ds-recent-visitors\"><br></div>","excerpt":"<p><img src=\"/images/multi_agent_conversation/title.png\" alt=\"\"></p>\n<blockquote>\n<p>解决的问题：两个机器人之间玩“你画我猜”<br>使用方法：RL (Reinforce), t-SNE可视化分析<br>亮点：很有意思的应用，让两个agent玩游戏，观察agents间的交互方式</p>\n</blockquote>","more":"<hr>\n<h1 id=\"文章简介\"><a href=\"#文章简介\" class=\"headerlink\" title=\"文章简介\"></a>文章简介</h1><p>　　这两天尝试啃了几篇非常理论的Paper之后，发现还是找一些更简单、应用更有意思的paper看起来比较轻松~ 于是今天来聊一下一篇很有意思的论文：<a href=\"https://arxiv.org/pdf/1612.07182.pdf\">“Multi-Agent Cooperation and the Emergence of (Natural) Language”</a>，来自Deepmind和Facebook AI Research的合作。</p>\n<p>　　这篇文章大致做了这么一件事情：让两个agent分别扮演 <em>描述者</em> 和 <em>猜题者</em> 两个角色共同玩一个游戏。描述者看到两张不同concept的图片（分别是目标图片和干扰图片），并得知哪张图是目标图片。游戏的目标是让描述者提供一个词（词库由两个agent提前商量好，类似于暗号），使得猜题者可以通过这个词成功的分别两张图片，找到目标图片。设定有点像“你画我猜”或者是“看动作猜词语”这种游戏。</p>\n<p>　　之前我们在研究中更多地都是focus在supervised learning上，而却忽视了通往通用AI之路的一个重要问题：AI和他人协同、交互的能力。这篇文章用很简单的方法，测试了AI间协作交互的能力，并试图分析这种交互，乃至干预AI间交互的形式变成人类可理解的语言，非常有意思。</p>\n<blockquote>\n<p>AI能否学到如何与同伴通信并互相理解？</p>\n</blockquote>\n<hr>\n<h1 id=\"游戏流程与算法\"><a href=\"#游戏流程与算法\" class=\"headerlink\" title=\"游戏流程与算法\"></a>游戏流程与算法</h1><p>　　这篇文章的想法非常简单，就是一个常用的Reinforce搞定。于是我们重点来看一下作者是如何构造的游戏使得AI间可以协同合作。文章里使用了一种叫做 <em>referential games</em> 的设定：</p>\n<ul>\n<li><p>首先建立一个图片集$i_1,…,i_N$，它们分别来自于463个基础概念（concept，如猫、苹果、汽车等）、20个大类，每个concept从ImageNet库里拉了100张图片（<em>论万能的ImageNet…</em>）。每一次游戏开始时，系统随机的选择两个concept，并分别选择一张图片 $(i_L,i_R)$，把其中的一个作为目标 $t\\in ${$ L,R $}，另一个作为伪装。</p>\n</li>\n<li><p>游戏中共有两个角色：<em>描述者</em> 和 <em>猜题者</em> 。他们都能看到两张输入图片，但是只有描述者知道哪个是目标图片，即收到了输入 $\\theta_S(i_L,i_R,t)$。</p>\n</li>\n<li><p>描述者和猜题者共同维护一个暗号集 $V$（size为$K$）。描述者选择 $V$ 中的一个暗号 $s$ 传给猜题者。描述者的策略记为：$s(\\theta_S(i_L,i_R,t)) \\in V$。</p>\n</li>\n<li><p>猜题者并不清楚哪张图是目标图片，于是他根据描述者传来的信息和两张图片本身来猜测目标图片。猜题者的策略记为：$r\\big(i_L,i_R,s(\\theta_S(i_L,i_R,t))\\big) \\in ${$ L,R $} 。</p>\n</li>\n<li><p>如果猜题者猜对了目标图片，即$r\\big(i_L,i_R,s(\\theta_S(i_L,i_R,t))\\big) = t$，描述者和猜题者同时加一分（胜利）；否则都不加分（失败）。</p>\n</li>\n</ul>\n<p>　　作者建立了两种简单的图像embedding方式，分别是（如下图所示）<br>1) 直接将 VGG的softmax层输出（或fc层输出）embedding。称为 <em>agnostic</em> 方法。<br>2) 将输出再过一层2x1的卷积层降维，把目标图和伪装图的输出混合起来，最后再embedding。称为 <em>informed</em> 方法。</p>\n<p><img src=\"/images/multi_agent_conversation/architecture.png\" alt=\"\"></p>\n<p>　　建立了policy和reward，玩游戏的问题自然地被转化成了RL过程，作者使用了Reinforce（最简单的policy gradient方法之一）算法来求解。结果很愉快，所有的agent都在10k轮之内成功收敛了，且答对率接近100%！其中，<em>informed</em> 方法相比于 <em>agnostic</em> 方法由于使用了更多的信息，可以收敛更快并更准。</p>\n<p>　　当然了，这个游戏本身其实挺简单的（concept之间差的挺大的，imagenet的vgg特征也足够强），所以performance倒并不是最重要的，更有意义的是去分析 agent之间到底是如何(chuan)交(an)互(hao)的，以为这种交互方式是否能被我们人类所理解？</p>\n<blockquote>\n<p>试想如果有一天 AI 之间发明了一种特殊的暗号，人类完全破译不能。。。 orz </p>\n</blockquote>\n<hr>\n<h1 id=\"了解AI间的交互方式\"><a href=\"#了解AI间的交互方式\" class=\"headerlink\" title=\"了解AI间的交互方式\"></a>了解AI间的交互方式</h1><p>　　为了潜入AI的脑内，分析其与同伴打招呼的方式，作者设计了几种trick来证明或可视化他们的假设。下面我们简单看看作者们的发现：</p>\n<blockquote>\n<p>AI 有丰富的描述语言，并能抓住重点。</p>\n</blockquote>\n<p>　　作者的第一个发现，就是 <em>informed sender</em> 使用了较多种的词语作为暗号来通信，且这些暗号中至少有很大一部分代表着不同的意义。同时，这些暗号的意义与人类定义的20种大类有着一定的相关性（最高的一种算法的聚类purity有46%）。<br>　　另一方面，<em>agnostic sender</em> 弱弱地只使用过两个词作为暗号。作者肉眼看的结果是这个暗号类似于 “能喘气的 vs. 不动换的” (living-vs-non-living)，也确实符合imagenet“动物园”的特性。</p>\n<blockquote>\n<p>可以通过“去常识化”使得 AI 的词语与人类的认知进一步相似</p>\n</blockquote>\n<p>　　用博弈论的调调来说，常识指所有人都知道的，以及所有人都知道所有人都知道的。。。把输入中的常识分量去掉，可以使得agent学会更本质的东西。作者的实现方法很tricky，给描述者和猜题者看了两张很相近但确不完全相同的图片（比如目标concept是狗，两个agent一个看到的是吉娃娃，另一个则看到的是波士顿小狗）。结果是agent的暗号与人类定义的类别更相近了。这点其实我没太理解。。。</p>\n<p><img src=\"/images/multi_agent_conversation/tsne.png\" alt=\"\" title=\"去常识化之前（左）和之后（右）的AI暗号聚类示意图，同样颜色对应着人类标识的相同大类\"></p>\n<blockquote>\n<p>可以通过加入监督学习的方式使得 AI 用人类的词语通信</p>\n</blockquote>\n<p>　　最后，当然我们可以用监督学习的方式，显式地让 AI 学会人类的语言来通信。作者在最后一个实验中，迭代地进行增强学习来优化agent和监督学习来让agent学会人类语言。评价时，作者雇了一群真正的人来当猜题者，并由AI生成的词语（监督学习中对应着imagenet的label）作为提示。结果是68%的人能猜到正确的图片（心疼这些人几秒钟。。。），似乎比预想的要低一些，但至少能大幅超过random guess。</p>\n<hr>\n<h1 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h1><p>　　总体来说，这篇文章提出了一个很有意思的应用，依此研究了AI间交互的可能性与其形式。理论算法和实验的结果其实都并不是很强，相信还有很大的提升空间。</p>\n<hr>\n<p><strong> 最近访问 </strong> </p>\n<div class=\"ds-recent-visitors\"\n    data-num-items=\"36\"\n    data-avatar-size=\"42\"\n    id=\"ds-recent-visitors\"><br></div>"},{"title":"【DRL论文阅读】Third-Person Imitation Learning 和前作Generative Adversarial Imitation Learning","date":"2017-04-13T16:10:56.000Z","_content":"\n\n# 前言\n\n　　最近终于项目有一定进展，可以有时间看paper啦，撒花~  这个系列扫一下ICLR2017关于DRL的文章，希望能有些insights。\n\n　　首先说个题外话，ICLR的审稿流程真是可怕，reviewer和author来回好几轮，工作量足够赶上一个journal，没有金刚钻是不敢揽这个瓷器活的。DRL的圈子被Deepmind和OpenAI把持，外人想进去很难，不过现阶段用DRL去做一些其他应用的道路还是很宽的，无论是写paper还是工业界都有着很大的价值，值得follow。\n\n---\n\n![](/images/imitation_learning_paper/iclr_imitation_learning_title.png)\n> 解决的问题：第三人称视角模仿学习\n使用方法：GAN、RL (TRPO)\n亮点：GAN与RL结合、模仿学习应用场景扩展\n\n<!--more-->\n\n---\n\n# 文章简介\n\n　　第一篇文章讲一下 [\"Third-Person Imitation Learning\"](https://arxiv.org/pdf/1703.01703.pdf) [1]，出自OpenAI，挂着Pieter Abbeel和Ilya Sutskever两座大神的名字。正如题目所说，这篇文章讲的是以第三人称视角进行模仿学习(imitation learning)，即学习者通过观察老师的行为，进行模仿并最终实现任务，此过程中不需要学习者以第一人称的视角实际体验该任务，这个设定实际上很符合人类婴儿的学习过程。以后imitation learning可能就不需要agent亲自上场试验啦，想想看机器人仅靠眼睛看人类的行为即可做到模仿、学习，既感到fancy又觉得可怕。\n\n![](/images/imitation_learning_paper/title.jpg \"模仿学习\")\n\n　　推荐这篇文章的另外一个原因，其实是延续自其前作的一个很有意思的思想，即将Generative Adversarial Network (GAN)的思想引入Imitation Learning，把目前大火的 <font color=\"#FF0000\">**GAN**</font> 和 <font color=\"#FF0000\">**RL**</font> 有机的结合在一起。个人浅见，这是目前GAN在实际工业应用中最可能实现突破的一点。\n\n　　理解这篇paper首先需要解释两个问题：1) 什么是imitation learning; 2) 理解其前作：[\"Generative Adversarial Imitation Learning\"](http://papers.nips.cc/paper/6391-generative-adversarial-imitation-learning.pdf) (NIPS 16，作者是OpenAI的Jonathan Ho和Stefano Ermon) [2]。\n\n---\n\n# 模仿学习（Imitation Learning）\n\n> 模仿学习，指学习者通过模仿专家的示范动作来完成任务的一种算法。\n\n　　根据[2]的描述，其特点是：\n* 学习者(learner)可以得到专家(expert)行为的轨迹或历史记录(trajactory)；\n* 在学习过程中，learner无法从expert处追加查询更多的数据；\n* Learner在学习过程中无法获得任何显式的奖励信号(reward signal)。\n\n　　公式上，模仿学习一般提供专家的决策数据 {$\\tau_E$}，每个决策包含着状态和动作序列 \n$< s_1^i,a_1^i,s_2^i,a_2^2,...,s_n^i>$。将所有的状态-动作对抽离出来，以状态作为feature，动作作为label进行学习，并使得模型生成的状态-动作与输入轨迹尽可能相近。\n\n　　实现imitation learning的方式主要有两种。最简单直接可以想到的，即用supervised learning的方法直接去拟合专家轨迹的状态-动作对，作为learner的策略(policy)。这种算法被称为 **Behavioral Cloning**。相关的supervised learning算法，可以想到的例子如RNN (LSTM)，Structural SVM, CRF等。由于传统的supervised learning不考虑agent和环境间的交互，Behavioral Cloning存在着序列行为中累积误差逐渐增大的问题。\n\n![](/images/imitation_learning_paper/aggreg_error.png \"误差累积问题\")\n\n　　另一种算法，即 ** IRL (Inverse Reinforcement Learning) **，根据专家的轨迹拟合出其做决策时的cost function，并根据这个cost function进行强化学习(Reinforcement Learning)以实现行为。该算法引入了agent和environment的交互，更适用于时间序列的学习问题。[2]中提到，由于IRL算法在内层循环中需要运行Reinforcement learning这一花费大量时间的过程，其scalability能力很有限，换句话说就是跑的太慢啦，不能解大型问题。\n\n---\n\n# GAIL (Generative Adversarial Imitation Learning)\n　　这篇前作[2]解决的问题，即调过IRL，直接使用RL中的policy gradient算法学习专家的策略。\n\n## IRL 目标函数解释\n　　Inverse Reinforcement Learning，顾名思义，是RL的反过程，即通过行为体某策略的动作轨迹，反推行为体做决策时使用的奖励函数reward或代价函数cost。首先回顾一下符号定义：\n- $\\mathcal{S}$表示状态集，$\\mathcal{A}$表示动作集；\n- 状态转移$s\\rightarrow s'$由环境中的转移概率$P(s'|s,a)$决定；\n- 专家策略由$\\pi_E$表示，学习者策略由$\\pi$表示；\n- 定义cost function $c(s,a)$，其目的是区分专家决策和学习者的决策，也就是说$c(s,a)$会倾向于给learner的策略比较高的cost，而给专家的策略低的cost；\n- 定义 $ \\mathbb{E}_{\\pi}[c(s,a)] \\triangleq \\mathbb{E}[\\sum_{t=0}^\\infty \\gamma^t c(s_t,a_t)] $，描述在策略$\\pi$下的total discounted cost之和的期望，类似于RL里的value function.\n\n　　在此定一下，IRL的目标函数可以定义为:\n\n> $$ \\max_{c\\in \\mathbb{C}} \\big(\\min_{\\pi \\in \\Pi} -H(\\pi) + \\mathbb{E}_{\\pi} [c(s,a)] \\big) - \\mathbb{E}_{\\pi_E} [c(s,a)] $$\n\n　　OK，下面我们来解释一下这个objective function。从外向内看，\n- 首先，cost function $c$的作用是建立一种reward(cost)机制，使得其可以分别expert trajactories和learner trajactories。故公式的最外层是对c求max cost，即Inverse RL里由行为轨迹反求reward(cost)的过程；\n- 向内一层，在第一个括号之内，是优化learner策略的过程，即求策略$\\pi$，使其可最小化长期cost function；\n- 最后，$H(\\pi)$ 定义为 * $\\gamma$-discounted casusal entropy*，$H(\\pi)\\triangleq \\mathbb{E}\\pi [-\\log \\pi(a|s)]$，其作用类似于supervised learning中s为feature，a为label的学习过程。\n\n当代价函数$c$确定后，IRL在inner loop中就可以使用RL算法，利用学习到的cost function做为依据来得到最优策略：\n$$ RL(c)=\\arg\\min_{\\pi \\in \\Pi} -H(\\pi)+\\mathbb{E}_{\\pi}[c(s,a)] $$\n\n> 综上，IRL的求解过程为：计算learner的策略$\\pi$使其尽量模仿expert的行为，并建立代价函数$c$使其尽量区分expert的真实轨迹和learner的模仿轨迹，重复迭代以上两步骤至收敛。\n\n　　*等等，这是不是看起来很熟悉？很像GAN里面的印钞机和验钞机有没有？！* 没错！GAIL这篇文章的想法，就是将IRL改写成GAN的形式，使用neural network的强大特征表征能力来提升Imitation Learning的性能。当然，这篇文章的另一个贡献是将GAN以及 Abbeel and Ng 和 Syed之前的两篇学徒学习(Apprenticeship learning)的算法从数学上统一在了一个框架中，得到了很漂亮的公式表达。\n\n## 公式推导与重要结论\n\n介绍完IRL的主要思路后，文章里做了一系列的数学推导，得到了一个很general的公式来表征模仿学习的问题。我们这里就只提一些重要结论啦，感兴趣的读者可以啃啃原文。\n\n- 由于专家轨迹的量有限，IRL学习cost function $c$时很容易overfit，于是没什么可说的，加个regularizer $\\psi$吧！于是IRL目标函数变为了\n$$ IRL_{\\psi}(\\pi_E) = \\arg\\max_{c\\in\\mathbb{R}^{\\mathcal{S}\\times \\mathcal{A}}} -\\psi(c) + \\big(\\min_{\\pi \\in \\Pi} -H(\\pi) + \\mathbb{E}_{\\pi} [c(s,a)] \\big) - \\mathbb{E}_{\\pi_E}[c(s,a)]$$\n\n- 定义occupancy measure $\\rho_{\\pi}: \\mathcal{S}\\times\\mathcal{A}\\rightarrow\\mathbb{R}$ 为 $\\rho_{\\pi}(s,a)=\\pi(a|s)\\sum_{t=0}^{\\infty}\\gamma^t P(s_t=s|\\pi)$，表示策略$\\pi$下状态-动作对$< s,a>$出现的频率。则使用IRL进行Imitation learning的全过程可以推导为：\n$$ RL\\circ IRL_{\\psi}(\\pi_E) = \\arg\\min_{\\pi \\in \\Pi} - H(\\pi)+\\psi^*(\\rho_{\\pi} - \\rho_{\\pi_E})$$\n\n　　上式说明，加入了约束项的IRL目标函数，等价于找到一个策略$\\pi$，使得其状态-动作对的出现频率(occupancy measure)尽量接近专家生成的状态-动作对，“接近”的评价标准是$\\psi^*$，即约束函数的共轭函数(convex conjugate)。\n\n- 以上面的发现为基础，作者证明了学徒学习(Apprenticeship learning)的目标函数，实际上对应着一种特殊的约束函数$\\psi$，即indicator function $\\psi=\\delta_{\\mathcal{C}}$，$\\delta_{\\mathcal{C}}(c)=0$ if $c\\in\\mathcal{C}$, and $\\infty$ otherwise。注意学徒学习的目标函数是：\n\n$$ \\min_{\\pi} \\max_{c\\in\\mathcal{C}} \\mathbb{E}_{\\pi} [c(s,a)] - \\mathbb{E}_{\\pi_E} [c(s,a)]$$\n\n　　而使用了$\\delta_{\\mathcal{C}}$的目标函数为：\n\n$$ \\min_{\\pi} -H(\\pi)+ \\max_{c\\in\\mathcal{C}}\\mathbb{E}_{\\pi} [c(s,a)] - \\mathbb{E}_{\\pi_E} [c(s,a)]$$\n\n- 作者的另一篇前作证明了用梯度下降的方式求解上式，可以用一个两步交替迭代的方法去解，而其中的一步正是对应于RL里的<font color=\"#FF0000\">**policy gradient**</font>。简单的推导如下：\n\n　　设学习者用参数$\\theta$做价值函数模拟，故策略可写作$\\pi_{\\theta}$。对$\\theta$求导：\n$$\\nabla_{\\theta} \\max_{c\\in\\mathcal{C}} \\mathbb{E}_{\\pi_{\\theta}}[c(s,a)] - \\mathbb{E}_{\\pi_{E}}[c(s,a)]$$\n　　设两个辅助函数：$\\hat{c}=\\arg\\max_{c\\in\\mathcal{C}}\\mathbb{E}_{\\pi_{\\theta}}[c(s,a)] - \\mathbb{E}_{\\pi_{E}}[c(s,a)]$, 和 $Q_{\\hat{c}}(s,a)=\\mathbb{E_{\\theta}}[\\hat{c}(s,a)|s_0=s,a_0=a]$\n　　之前的梯度可以转化为：\n$$ Left=\\nabla_{\\theta}\\mathbb{E}_{\\pi_{\\theta}}[\\hat{c}(s,a)] = \\mathbb{E}_{\\pi_{\\theta}}[\\nabla_\\theta\\log \\pi_{\\theta}(a|s) Q_{\\hat{c}}(s,a)]$$\n　　其中最后一个等号用到了经典的policy gradient的推导，不熟悉的可以参考这篇 [Karpathy的blog](http://karpathy.github.io/2016/05/31/rl/)。\n\n- 上述的两步迭代求解方法即：\n**1. 使用当前的策略$\\pi_{\\theta_i}$在环境中采样，并依此用上式拟合修正的代价函数 $\\hat{c}$ ;**\n**2. 使用拟合出的代价函数 $\\hat{c}$，根据采样的历史记录，使用policy gradient优化当前策略得到新策略$\\pi_{\\theta_{i+1}}$ .**\n\n> 以上两步中，步骤1对应着找到最能区分expert和learner的cost function的作用，步骤2对应根据cost function，优化learner的策略。\n\n## 引入GAN\n\n　　OK，经过以上一段复杂的数学推导，终于到了在目标函数中引入GAN的时候了。回顾一下，上文分析了每一种约束函数$\\psi$，都可以对应一种新的学徒学习的算法。作者证明了，GAN对应的约束函数是：\n\n![](/images/imitation_learning_paper/equation13.png)\n\n　　可以证明，其目标函数有如下形式：\n$$\\psi^*_{GA}(\\rho_\\pi - \\rho_{\\pi_E})-\\lambda H(\\pi) = \\sup_{D\\in (0,1)^{\\mathcal{S}\\times\\mathcal{A}}} \\mathbb{E}_{\\pi}[\\log(D(s,a))] + \\mathbb{E}_{\\pi_E}[\\log(1-D(s,a))]-\\lambda H(\\pi)$$\n\n　　于是终于，熟悉的式子出现啦~ 由上式可知，GAN对应的cost function $c(s,a)=\\log D(s,a)$，即验钞机的验钞本领。当learner越厉害（可以以假乱真）时，$D(s,a)\\rightarrow 1$，对应$c(s,a)$ 最大，算法约需要*“用力”*地找到一个可以区分expert 和 learner的cost function。\n\n## GAIL算法流程\n作者将GAN+Imitation Learning(IL)命名为GAIL（名字不错）。最终的算法流程其实非常简单，见下图，paper的关键其实在于之前的数学推导。\n\n![](/images/imitation_learning_paper/algorithm_gail.png \"GAIL算法框架\")\n\n　　算法很清晰，第一步用GAN的目标函数拟合cost function，对应于GAN的$D_w(s,a)$ (line4)；第二步根据cost function使用TRPO(Trust Region Policy Optimization，一种policy gradient的改进，用于训练时不容易跑飞)更新策略$\\pi_\\theta$。重复迭代至收敛即可。\n\n\n## 实验与代码实现\n　　OpenAI kindly提供了GAIL的代码，传上了github：https://github.com/openai/imitation 。时间原因我还没有自己跑哈，测试之后再更新这段。\n　　\n---\n\n# Third-Person Imitation Learning\n　　理解了GAIL，我们的这篇正文\"Third-Person Imitation Learning\"就很简单了。个人浅见，这篇文章的主要贡献在于make it possible去做第三人称模仿学习这件事，而算法本身更多的是在现有算法上添砖加瓦，创新点并不是很多。\n\n![](/images/imitation_learning_paper/third_title.png \"Third-Person Imitation Learning 网络结构\")\n\n　　本文基本的想法，即在GAIL的基础上，引入第三人称和第一人称之间的domain difference，用经典的Deep Domain Adaptation的手段来求解GAIL。具体来说，作者将GAN的网络分成了两部分，前一半低层layers用于提取特征（设其为$D_F$），在两个domain共用；后一半的高层layers则分别对应于expert domain的$D_R$ 和 区分expert和learner的分类器$D_D$。作者用了一个BP过程中翻转梯度方向的trick $\\mathcal{G}$，使得$D_R, D_D, D_F$在优化目标中都取$\\min$。最终的算法流程图和GAIL相比变化不大，作者同样使用了TRPO作为policy gradient的解法。\n\n![](/images/imitation_learning_paper/algorithm_third.png \"Third-Person Imitation Learning 算法框架\")\n\n\n---\n\n# 相关paper推荐\n\n由于本文实质上只要是在讲GAIL，故这里只推荐一些后续cite GAIL的文章。\n\n- 最值得一看的是Deepmind的一篇文章[\"Connecting Generative Adversarial Networks\nand Actor-Critic Methods\"](https://arxiv.org/pdf/1610.01945.pdf)，文章中分析了 GAN 和 RL中的Actor-Critic(AC) 算法在方法论上的相似性，并详细分析了两种算法在实际应用中的 stabilizing 方法，至少可以刚做一个实现GAN和AC算法时的trick使用手册。\n\n- 另外有一篇很有意思的paper，关于[multi-agent imitation learning](https://arxiv.org/pdf/1703.03121)，内容大致是让一群机器人学习现实中球员比赛时的行为，以理解multi-agent间的协同合作关系。\n\n---\n\n# 结论\n　　这篇blog借着\"Third-Person Imitation Learning\"这篇文章，回顾了Imitation Learning的问题设定，及前作将GAN和RL结合起来做imitation learning的算法GAIL，并详细解释了部分的理论推导。\n\n从这篇文章我们可以得到的take-away是：\n- Imitation Learning作为目前高速发展的一个领域，其应用场景在不断的扩展。几年之后机器人的模仿、学习能力就能上一个台阶就说不定。\n- 用IRL的方法做Imitation Learning，其特例的IRL过程可以演化为GAN的形式，通过迭代的方式，优化GAN得到代价函数，再根据代价函数使用policy gradient优化策略，直至收敛。\n- GAIL的想法可以看到一定的CV和NLP领域中的应用，如视频/序列的生成。\n\n\n---\n\n** 最近访问 ** \n\n<div class=\"ds-recent-visitors\"\n    data-num-items=\"36\"\n    data-avatar-size=\"42\"\n    id=\"ds-recent-visitors\">\n</div>","source":"_posts/iclr17-imitation-learning.md","raw":"---\ntitle: >-\n  【DRL论文阅读】Third-Person Imitation Learning 和前作Generative Adversarial Imitation\n  Learning\ntags: drl\ncategories: drl\ndate: 2017-04-14 00:10:56\n---\n\n\n# 前言\n\n　　最近终于项目有一定进展，可以有时间看paper啦，撒花~  这个系列扫一下ICLR2017关于DRL的文章，希望能有些insights。\n\n　　首先说个题外话，ICLR的审稿流程真是可怕，reviewer和author来回好几轮，工作量足够赶上一个journal，没有金刚钻是不敢揽这个瓷器活的。DRL的圈子被Deepmind和OpenAI把持，外人想进去很难，不过现阶段用DRL去做一些其他应用的道路还是很宽的，无论是写paper还是工业界都有着很大的价值，值得follow。\n\n---\n\n![](/images/imitation_learning_paper/iclr_imitation_learning_title.png)\n> 解决的问题：第三人称视角模仿学习\n使用方法：GAN、RL (TRPO)\n亮点：GAN与RL结合、模仿学习应用场景扩展\n\n<!--more-->\n\n---\n\n# 文章简介\n\n　　第一篇文章讲一下 [\"Third-Person Imitation Learning\"](https://arxiv.org/pdf/1703.01703.pdf) [1]，出自OpenAI，挂着Pieter Abbeel和Ilya Sutskever两座大神的名字。正如题目所说，这篇文章讲的是以第三人称视角进行模仿学习(imitation learning)，即学习者通过观察老师的行为，进行模仿并最终实现任务，此过程中不需要学习者以第一人称的视角实际体验该任务，这个设定实际上很符合人类婴儿的学习过程。以后imitation learning可能就不需要agent亲自上场试验啦，想想看机器人仅靠眼睛看人类的行为即可做到模仿、学习，既感到fancy又觉得可怕。\n\n![](/images/imitation_learning_paper/title.jpg \"模仿学习\")\n\n　　推荐这篇文章的另外一个原因，其实是延续自其前作的一个很有意思的思想，即将Generative Adversarial Network (GAN)的思想引入Imitation Learning，把目前大火的 <font color=\"#FF0000\">**GAN**</font> 和 <font color=\"#FF0000\">**RL**</font> 有机的结合在一起。个人浅见，这是目前GAN在实际工业应用中最可能实现突破的一点。\n\n　　理解这篇paper首先需要解释两个问题：1) 什么是imitation learning; 2) 理解其前作：[\"Generative Adversarial Imitation Learning\"](http://papers.nips.cc/paper/6391-generative-adversarial-imitation-learning.pdf) (NIPS 16，作者是OpenAI的Jonathan Ho和Stefano Ermon) [2]。\n\n---\n\n# 模仿学习（Imitation Learning）\n\n> 模仿学习，指学习者通过模仿专家的示范动作来完成任务的一种算法。\n\n　　根据[2]的描述，其特点是：\n* 学习者(learner)可以得到专家(expert)行为的轨迹或历史记录(trajactory)；\n* 在学习过程中，learner无法从expert处追加查询更多的数据；\n* Learner在学习过程中无法获得任何显式的奖励信号(reward signal)。\n\n　　公式上，模仿学习一般提供专家的决策数据 {$\\tau_E$}，每个决策包含着状态和动作序列 \n$< s_1^i,a_1^i,s_2^i,a_2^2,...,s_n^i>$。将所有的状态-动作对抽离出来，以状态作为feature，动作作为label进行学习，并使得模型生成的状态-动作与输入轨迹尽可能相近。\n\n　　实现imitation learning的方式主要有两种。最简单直接可以想到的，即用supervised learning的方法直接去拟合专家轨迹的状态-动作对，作为learner的策略(policy)。这种算法被称为 **Behavioral Cloning**。相关的supervised learning算法，可以想到的例子如RNN (LSTM)，Structural SVM, CRF等。由于传统的supervised learning不考虑agent和环境间的交互，Behavioral Cloning存在着序列行为中累积误差逐渐增大的问题。\n\n![](/images/imitation_learning_paper/aggreg_error.png \"误差累积问题\")\n\n　　另一种算法，即 ** IRL (Inverse Reinforcement Learning) **，根据专家的轨迹拟合出其做决策时的cost function，并根据这个cost function进行强化学习(Reinforcement Learning)以实现行为。该算法引入了agent和environment的交互，更适用于时间序列的学习问题。[2]中提到，由于IRL算法在内层循环中需要运行Reinforcement learning这一花费大量时间的过程，其scalability能力很有限，换句话说就是跑的太慢啦，不能解大型问题。\n\n---\n\n# GAIL (Generative Adversarial Imitation Learning)\n　　这篇前作[2]解决的问题，即调过IRL，直接使用RL中的policy gradient算法学习专家的策略。\n\n## IRL 目标函数解释\n　　Inverse Reinforcement Learning，顾名思义，是RL的反过程，即通过行为体某策略的动作轨迹，反推行为体做决策时使用的奖励函数reward或代价函数cost。首先回顾一下符号定义：\n- $\\mathcal{S}$表示状态集，$\\mathcal{A}$表示动作集；\n- 状态转移$s\\rightarrow s'$由环境中的转移概率$P(s'|s,a)$决定；\n- 专家策略由$\\pi_E$表示，学习者策略由$\\pi$表示；\n- 定义cost function $c(s,a)$，其目的是区分专家决策和学习者的决策，也就是说$c(s,a)$会倾向于给learner的策略比较高的cost，而给专家的策略低的cost；\n- 定义 $ \\mathbb{E}_{\\pi}[c(s,a)] \\triangleq \\mathbb{E}[\\sum_{t=0}^\\infty \\gamma^t c(s_t,a_t)] $，描述在策略$\\pi$下的total discounted cost之和的期望，类似于RL里的value function.\n\n　　在此定一下，IRL的目标函数可以定义为:\n\n> $$ \\max_{c\\in \\mathbb{C}} \\big(\\min_{\\pi \\in \\Pi} -H(\\pi) + \\mathbb{E}_{\\pi} [c(s,a)] \\big) - \\mathbb{E}_{\\pi_E} [c(s,a)] $$\n\n　　OK，下面我们来解释一下这个objective function。从外向内看，\n- 首先，cost function $c$的作用是建立一种reward(cost)机制，使得其可以分别expert trajactories和learner trajactories。故公式的最外层是对c求max cost，即Inverse RL里由行为轨迹反求reward(cost)的过程；\n- 向内一层，在第一个括号之内，是优化learner策略的过程，即求策略$\\pi$，使其可最小化长期cost function；\n- 最后，$H(\\pi)$ 定义为 * $\\gamma$-discounted casusal entropy*，$H(\\pi)\\triangleq \\mathbb{E}\\pi [-\\log \\pi(a|s)]$，其作用类似于supervised learning中s为feature，a为label的学习过程。\n\n当代价函数$c$确定后，IRL在inner loop中就可以使用RL算法，利用学习到的cost function做为依据来得到最优策略：\n$$ RL(c)=\\arg\\min_{\\pi \\in \\Pi} -H(\\pi)+\\mathbb{E}_{\\pi}[c(s,a)] $$\n\n> 综上，IRL的求解过程为：计算learner的策略$\\pi$使其尽量模仿expert的行为，并建立代价函数$c$使其尽量区分expert的真实轨迹和learner的模仿轨迹，重复迭代以上两步骤至收敛。\n\n　　*等等，这是不是看起来很熟悉？很像GAN里面的印钞机和验钞机有没有？！* 没错！GAIL这篇文章的想法，就是将IRL改写成GAN的形式，使用neural network的强大特征表征能力来提升Imitation Learning的性能。当然，这篇文章的另一个贡献是将GAN以及 Abbeel and Ng 和 Syed之前的两篇学徒学习(Apprenticeship learning)的算法从数学上统一在了一个框架中，得到了很漂亮的公式表达。\n\n## 公式推导与重要结论\n\n介绍完IRL的主要思路后，文章里做了一系列的数学推导，得到了一个很general的公式来表征模仿学习的问题。我们这里就只提一些重要结论啦，感兴趣的读者可以啃啃原文。\n\n- 由于专家轨迹的量有限，IRL学习cost function $c$时很容易overfit，于是没什么可说的，加个regularizer $\\psi$吧！于是IRL目标函数变为了\n$$ IRL_{\\psi}(\\pi_E) = \\arg\\max_{c\\in\\mathbb{R}^{\\mathcal{S}\\times \\mathcal{A}}} -\\psi(c) + \\big(\\min_{\\pi \\in \\Pi} -H(\\pi) + \\mathbb{E}_{\\pi} [c(s,a)] \\big) - \\mathbb{E}_{\\pi_E}[c(s,a)]$$\n\n- 定义occupancy measure $\\rho_{\\pi}: \\mathcal{S}\\times\\mathcal{A}\\rightarrow\\mathbb{R}$ 为 $\\rho_{\\pi}(s,a)=\\pi(a|s)\\sum_{t=0}^{\\infty}\\gamma^t P(s_t=s|\\pi)$，表示策略$\\pi$下状态-动作对$< s,a>$出现的频率。则使用IRL进行Imitation learning的全过程可以推导为：\n$$ RL\\circ IRL_{\\psi}(\\pi_E) = \\arg\\min_{\\pi \\in \\Pi} - H(\\pi)+\\psi^*(\\rho_{\\pi} - \\rho_{\\pi_E})$$\n\n　　上式说明，加入了约束项的IRL目标函数，等价于找到一个策略$\\pi$，使得其状态-动作对的出现频率(occupancy measure)尽量接近专家生成的状态-动作对，“接近”的评价标准是$\\psi^*$，即约束函数的共轭函数(convex conjugate)。\n\n- 以上面的发现为基础，作者证明了学徒学习(Apprenticeship learning)的目标函数，实际上对应着一种特殊的约束函数$\\psi$，即indicator function $\\psi=\\delta_{\\mathcal{C}}$，$\\delta_{\\mathcal{C}}(c)=0$ if $c\\in\\mathcal{C}$, and $\\infty$ otherwise。注意学徒学习的目标函数是：\n\n$$ \\min_{\\pi} \\max_{c\\in\\mathcal{C}} \\mathbb{E}_{\\pi} [c(s,a)] - \\mathbb{E}_{\\pi_E} [c(s,a)]$$\n\n　　而使用了$\\delta_{\\mathcal{C}}$的目标函数为：\n\n$$ \\min_{\\pi} -H(\\pi)+ \\max_{c\\in\\mathcal{C}}\\mathbb{E}_{\\pi} [c(s,a)] - \\mathbb{E}_{\\pi_E} [c(s,a)]$$\n\n- 作者的另一篇前作证明了用梯度下降的方式求解上式，可以用一个两步交替迭代的方法去解，而其中的一步正是对应于RL里的<font color=\"#FF0000\">**policy gradient**</font>。简单的推导如下：\n\n　　设学习者用参数$\\theta$做价值函数模拟，故策略可写作$\\pi_{\\theta}$。对$\\theta$求导：\n$$\\nabla_{\\theta} \\max_{c\\in\\mathcal{C}} \\mathbb{E}_{\\pi_{\\theta}}[c(s,a)] - \\mathbb{E}_{\\pi_{E}}[c(s,a)]$$\n　　设两个辅助函数：$\\hat{c}=\\arg\\max_{c\\in\\mathcal{C}}\\mathbb{E}_{\\pi_{\\theta}}[c(s,a)] - \\mathbb{E}_{\\pi_{E}}[c(s,a)]$, 和 $Q_{\\hat{c}}(s,a)=\\mathbb{E_{\\theta}}[\\hat{c}(s,a)|s_0=s,a_0=a]$\n　　之前的梯度可以转化为：\n$$ Left=\\nabla_{\\theta}\\mathbb{E}_{\\pi_{\\theta}}[\\hat{c}(s,a)] = \\mathbb{E}_{\\pi_{\\theta}}[\\nabla_\\theta\\log \\pi_{\\theta}(a|s) Q_{\\hat{c}}(s,a)]$$\n　　其中最后一个等号用到了经典的policy gradient的推导，不熟悉的可以参考这篇 [Karpathy的blog](http://karpathy.github.io/2016/05/31/rl/)。\n\n- 上述的两步迭代求解方法即：\n**1. 使用当前的策略$\\pi_{\\theta_i}$在环境中采样，并依此用上式拟合修正的代价函数 $\\hat{c}$ ;**\n**2. 使用拟合出的代价函数 $\\hat{c}$，根据采样的历史记录，使用policy gradient优化当前策略得到新策略$\\pi_{\\theta_{i+1}}$ .**\n\n> 以上两步中，步骤1对应着找到最能区分expert和learner的cost function的作用，步骤2对应根据cost function，优化learner的策略。\n\n## 引入GAN\n\n　　OK，经过以上一段复杂的数学推导，终于到了在目标函数中引入GAN的时候了。回顾一下，上文分析了每一种约束函数$\\psi$，都可以对应一种新的学徒学习的算法。作者证明了，GAN对应的约束函数是：\n\n![](/images/imitation_learning_paper/equation13.png)\n\n　　可以证明，其目标函数有如下形式：\n$$\\psi^*_{GA}(\\rho_\\pi - \\rho_{\\pi_E})-\\lambda H(\\pi) = \\sup_{D\\in (0,1)^{\\mathcal{S}\\times\\mathcal{A}}} \\mathbb{E}_{\\pi}[\\log(D(s,a))] + \\mathbb{E}_{\\pi_E}[\\log(1-D(s,a))]-\\lambda H(\\pi)$$\n\n　　于是终于，熟悉的式子出现啦~ 由上式可知，GAN对应的cost function $c(s,a)=\\log D(s,a)$，即验钞机的验钞本领。当learner越厉害（可以以假乱真）时，$D(s,a)\\rightarrow 1$，对应$c(s,a)$ 最大，算法约需要*“用力”*地找到一个可以区分expert 和 learner的cost function。\n\n## GAIL算法流程\n作者将GAN+Imitation Learning(IL)命名为GAIL（名字不错）。最终的算法流程其实非常简单，见下图，paper的关键其实在于之前的数学推导。\n\n![](/images/imitation_learning_paper/algorithm_gail.png \"GAIL算法框架\")\n\n　　算法很清晰，第一步用GAN的目标函数拟合cost function，对应于GAN的$D_w(s,a)$ (line4)；第二步根据cost function使用TRPO(Trust Region Policy Optimization，一种policy gradient的改进，用于训练时不容易跑飞)更新策略$\\pi_\\theta$。重复迭代至收敛即可。\n\n\n## 实验与代码实现\n　　OpenAI kindly提供了GAIL的代码，传上了github：https://github.com/openai/imitation 。时间原因我还没有自己跑哈，测试之后再更新这段。\n　　\n---\n\n# Third-Person Imitation Learning\n　　理解了GAIL，我们的这篇正文\"Third-Person Imitation Learning\"就很简单了。个人浅见，这篇文章的主要贡献在于make it possible去做第三人称模仿学习这件事，而算法本身更多的是在现有算法上添砖加瓦，创新点并不是很多。\n\n![](/images/imitation_learning_paper/third_title.png \"Third-Person Imitation Learning 网络结构\")\n\n　　本文基本的想法，即在GAIL的基础上，引入第三人称和第一人称之间的domain difference，用经典的Deep Domain Adaptation的手段来求解GAIL。具体来说，作者将GAN的网络分成了两部分，前一半低层layers用于提取特征（设其为$D_F$），在两个domain共用；后一半的高层layers则分别对应于expert domain的$D_R$ 和 区分expert和learner的分类器$D_D$。作者用了一个BP过程中翻转梯度方向的trick $\\mathcal{G}$，使得$D_R, D_D, D_F$在优化目标中都取$\\min$。最终的算法流程图和GAIL相比变化不大，作者同样使用了TRPO作为policy gradient的解法。\n\n![](/images/imitation_learning_paper/algorithm_third.png \"Third-Person Imitation Learning 算法框架\")\n\n\n---\n\n# 相关paper推荐\n\n由于本文实质上只要是在讲GAIL，故这里只推荐一些后续cite GAIL的文章。\n\n- 最值得一看的是Deepmind的一篇文章[\"Connecting Generative Adversarial Networks\nand Actor-Critic Methods\"](https://arxiv.org/pdf/1610.01945.pdf)，文章中分析了 GAN 和 RL中的Actor-Critic(AC) 算法在方法论上的相似性，并详细分析了两种算法在实际应用中的 stabilizing 方法，至少可以刚做一个实现GAN和AC算法时的trick使用手册。\n\n- 另外有一篇很有意思的paper，关于[multi-agent imitation learning](https://arxiv.org/pdf/1703.03121)，内容大致是让一群机器人学习现实中球员比赛时的行为，以理解multi-agent间的协同合作关系。\n\n---\n\n# 结论\n　　这篇blog借着\"Third-Person Imitation Learning\"这篇文章，回顾了Imitation Learning的问题设定，及前作将GAN和RL结合起来做imitation learning的算法GAIL，并详细解释了部分的理论推导。\n\n从这篇文章我们可以得到的take-away是：\n- Imitation Learning作为目前高速发展的一个领域，其应用场景在不断的扩展。几年之后机器人的模仿、学习能力就能上一个台阶就说不定。\n- 用IRL的方法做Imitation Learning，其特例的IRL过程可以演化为GAN的形式，通过迭代的方式，优化GAN得到代价函数，再根据代价函数使用policy gradient优化策略，直至收敛。\n- GAIL的想法可以看到一定的CV和NLP领域中的应用，如视频/序列的生成。\n\n\n---\n\n** 最近访问 ** \n\n<div class=\"ds-recent-visitors\"\n    data-num-items=\"36\"\n    data-avatar-size=\"42\"\n    id=\"ds-recent-visitors\">\n</div>","slug":"iclr17-imitation-learning","published":1,"updated":"2017-04-15T11:22:34.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1jbutd5000c9qs627ld5gts","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><p>　　最近终于项目有一定进展，可以有时间看paper啦，撒花~  这个系列扫一下ICLR2017关于DRL的文章，希望能有些insights。</p>\n<p>　　首先说个题外话，ICLR的审稿流程真是可怕，reviewer和author来回好几轮，工作量足够赶上一个journal，没有金刚钻是不敢揽这个瓷器活的。DRL的圈子被Deepmind和OpenAI把持，外人想进去很难，不过现阶段用DRL去做一些其他应用的道路还是很宽的，无论是写paper还是工业界都有着很大的价值，值得follow。</p>\n<hr>\n<p><img src=\"/images/imitation_learning_paper/iclr_imitation_learning_title.png\" alt=\"\"></p>\n<blockquote>\n<p>解决的问题：第三人称视角模仿学习<br>使用方法：GAN、RL (TRPO)<br>亮点：GAN与RL结合、模仿学习应用场景扩展</p>\n</blockquote>\n<a id=\"more\"></a>\n<hr>\n<h1 id=\"文章简介\"><a href=\"#文章简介\" class=\"headerlink\" title=\"文章简介\"></a>文章简介</h1><p>　　第一篇文章讲一下 <a href=\"https://arxiv.org/pdf/1703.01703.pdf\" target=\"_blank\" rel=\"external\">“Third-Person Imitation Learning”</a> [1]，出自OpenAI，挂着Pieter Abbeel和Ilya Sutskever两座大神的名字。正如题目所说，这篇文章讲的是以第三人称视角进行模仿学习(imitation learning)，即学习者通过观察老师的行为，进行模仿并最终实现任务，此过程中不需要学习者以第一人称的视角实际体验该任务，这个设定实际上很符合人类婴儿的学习过程。以后imitation learning可能就不需要agent亲自上场试验啦，想想看机器人仅靠眼睛看人类的行为即可做到模仿、学习，既感到fancy又觉得可怕。</p>\n<p><img src=\"/images/imitation_learning_paper/title.jpg\" alt=\"\" title=\"模仿学习\"></p>\n<p>　　推荐这篇文章的另外一个原因，其实是延续自其前作的一个很有意思的思想，即将Generative Adversarial Network (GAN)的思想引入Imitation Learning，把目前大火的 <font color=\"#FF0000\"><strong>GAN</strong></font> 和 <font color=\"#FF0000\"><strong>RL</strong></font> 有机的结合在一起。个人浅见，这是目前GAN在实际工业应用中最可能实现突破的一点。</p>\n<p>　　理解这篇paper首先需要解释两个问题：1) 什么是imitation learning; 2) 理解其前作：<a href=\"http://papers.nips.cc/paper/6391-generative-adversarial-imitation-learning.pdf\" target=\"_blank\" rel=\"external\">“Generative Adversarial Imitation Learning”</a> (NIPS 16，作者是OpenAI的Jonathan Ho和Stefano Ermon) [2]。</p>\n<hr>\n<h1 id=\"模仿学习imitation-learning\"><a href=\"#模仿学习（Imitation-Learning）\" class=\"headerlink\" title=\"模仿学习（Imitation Learning）\"></a>模仿学习（Imitation Learning）</h1><blockquote>\n<p>模仿学习，指学习者通过模仿专家的示范动作来完成任务的一种算法。</p>\n</blockquote>\n<p>　　根据[2]的描述，其特点是：</p>\n<ul>\n<li>学习者(learner)可以得到专家(expert)行为的轨迹或历史记录(trajactory)；</li>\n<li>在学习过程中，learner无法从expert处追加查询更多的数据；</li>\n<li>Learner在学习过程中无法获得任何显式的奖励信号(reward signal)。</li>\n</ul>\n<p>　　公式上，模仿学习一般提供专家的决策数据 {$\\tau_E$}，每个决策包含着状态和动作序列<br>$&lt; s_1^i,a_1^i,s_2^i,a_2^2,…,s_n^i&gt;$。将所有的状态-动作对抽离出来，以状态作为feature，动作作为label进行学习，并使得模型生成的状态-动作与输入轨迹尽可能相近。</p>\n<p>　　实现imitation learning的方式主要有两种。最简单直接可以想到的，即用supervised learning的方法直接去拟合专家轨迹的状态-动作对，作为learner的策略(policy)。这种算法被称为 <strong>Behavioral Cloning</strong>。相关的supervised learning算法，可以想到的例子如RNN (LSTM)，Structural SVM, CRF等。由于传统的supervised learning不考虑agent和环境间的交互，Behavioral Cloning存在着序列行为中累积误差逐渐增大的问题。</p>\n<p><img src=\"/images/imitation_learning_paper/aggreg_error.png\" alt=\"\" title=\"误差累积问题\"></p>\n<p>　　另一种算法，即 <strong> IRL (Inverse Reinforcement Learning) </strong>，根据专家的轨迹拟合出其做决策时的cost function，并根据这个cost function进行强化学习(Reinforcement Learning)以实现行为。该算法引入了agent和environment的交互，更适用于时间序列的学习问题。[2]中提到，由于IRL算法在内层循环中需要运行Reinforcement learning这一花费大量时间的过程，其scalability能力很有限，换句话说就是跑的太慢啦，不能解大型问题。</p>\n<hr>\n<h1 id=\"gail-generative-adversarial-imitation-learning\"><a href=\"#GAIL-Generative-Adversarial-Imitation-Learning\" class=\"headerlink\" title=\"GAIL (Generative Adversarial Imitation Learning)\"></a>GAIL (Generative Adversarial Imitation Learning)</h1><p>　　这篇前作[2]解决的问题，即调过IRL，直接使用RL中的policy gradient算法学习专家的策略。</p>\n<h2 id=\"irl-目标函数解释\"><a href=\"#IRL-目标函数解释\" class=\"headerlink\" title=\"IRL 目标函数解释\"></a>IRL 目标函数解释</h2><p>　　Inverse Reinforcement Learning，顾名思义，是RL的反过程，即通过行为体某策略的动作轨迹，反推行为体做决策时使用的奖励函数reward或代价函数cost。首先回顾一下符号定义：</p>\n<ul>\n<li>$\\mathcal{S}$表示状态集，$\\mathcal{A}$表示动作集；</li>\n<li>状态转移$s\\rightarrow s’$由环境中的转移概率$P(s’|s,a)$决定；</li>\n<li>专家策略由$\\pi_E$表示，学习者策略由$\\pi$表示；</li>\n<li>定义cost function $c(s,a)$，其目的是区分专家决策和学习者的决策，也就是说$c(s,a)$会倾向于给learner的策略比较高的cost，而给专家的策略低的cost；</li>\n<li>定义 $ \\mathbb{E}_{\\pi}[c(s,a)] \\triangleq \\mathbb{E}[\\sum_{t=0}^\\infty \\gamma^t c(s_t,a_t)] $，描述在策略$\\pi$下的total discounted cost之和的期望，类似于RL里的value function.</li>\n</ul>\n<p>　　在此定一下，IRL的目标函数可以定义为:</p>\n<blockquote>\n<p>$$ \\max_{c\\in \\mathbb{C}} \\big(\\min_{\\pi \\in \\Pi} -H(\\pi) + \\mathbb{E}_{\\pi} [c(s,a)] \\big) - \\mathbb{E}_{\\pi_E} [c(s,a)] $$</p>\n</blockquote>\n<p>　　OK，下面我们来解释一下这个objective function。从外向内看，</p>\n<ul>\n<li>首先，cost function $c$的作用是建立一种reward(cost)机制，使得其可以分别expert trajactories和learner trajactories。故公式的最外层是对c求max cost，即Inverse RL里由行为轨迹反求reward(cost)的过程；</li>\n<li>向内一层，在第一个括号之内，是优化learner策略的过程，即求策略$\\pi$，使其可最小化长期cost function；</li>\n<li>最后，$H(\\pi)$ 定义为 <em> $\\gamma$-discounted casusal entropy</em>，$H(\\pi)\\triangleq \\mathbb{E}\\pi [-\\log \\pi(a|s)]$，其作用类似于supervised learning中s为feature，a为label的学习过程。</li>\n</ul>\n<p>当代价函数$c$确定后，IRL在inner loop中就可以使用RL算法，利用学习到的cost function做为依据来得到最优策略：<br>$$ RL(c)=\\arg\\min_{\\pi \\in \\Pi} -H(\\pi)+\\mathbb{E}_{\\pi}[c(s,a)] $$</p>\n<blockquote>\n<p>综上，IRL的求解过程为：计算learner的策略$\\pi$使其尽量模仿expert的行为，并建立代价函数$c$使其尽量区分expert的真实轨迹和learner的模仿轨迹，重复迭代以上两步骤至收敛。</p>\n</blockquote>\n<p>　　<em>等等，这是不是看起来很熟悉？很像GAN里面的印钞机和验钞机有没有？！</em> 没错！GAIL这篇文章的想法，就是将IRL改写成GAN的形式，使用neural network的强大特征表征能力来提升Imitation Learning的性能。当然，这篇文章的另一个贡献是将GAN以及 Abbeel and Ng 和 Syed之前的两篇学徒学习(Apprenticeship learning)的算法从数学上统一在了一个框架中，得到了很漂亮的公式表达。</p>\n<h2 id=\"公式推导与重要结论\"><a href=\"#公式推导与重要结论\" class=\"headerlink\" title=\"公式推导与重要结论\"></a>公式推导与重要结论</h2><p>介绍完IRL的主要思路后，文章里做了一系列的数学推导，得到了一个很general的公式来表征模仿学习的问题。我们这里就只提一些重要结论啦，感兴趣的读者可以啃啃原文。</p>\n<ul>\n<li><p>由于专家轨迹的量有限，IRL学习cost function $c$时很容易overfit，于是没什么可说的，加个regularizer $\\psi$吧！于是IRL目标函数变为了<br>$$ IRL_{\\psi}(\\pi_E) = \\arg\\max_{c\\in\\mathbb{R}^{\\mathcal{S}\\times \\mathcal{A}}} -\\psi(c) + \\big(\\min_{\\pi \\in \\Pi} -H(\\pi) + \\mathbb{E}_{\\pi} [c(s,a)] \\big) - \\mathbb{E}_{\\pi_E}[c(s,a)]$$</p>\n</li>\n<li><p>定义occupancy measure $\\rho_{\\pi}: \\mathcal{S}\\times\\mathcal{A}\\rightarrow\\mathbb{R}$ 为 $\\rho_{\\pi}(s,a)=\\pi(a|s)\\sum_{t=0}^{\\infty}\\gamma^t P(s_t=s|\\pi)$，表示策略$\\pi$下状态-动作对$&lt; s,a&gt;$出现的频率。则使用IRL进行Imitation learning的全过程可以推导为：<br>$$ RL\\circ IRL_{\\psi}(\\pi_E) = \\arg\\min_{\\pi \\in \\Pi} - H(\\pi)+\\psi^*(\\rho_{\\pi} - \\rho_{\\pi_E})$$</p>\n</li>\n</ul>\n<p>　　上式说明，加入了约束项的IRL目标函数，等价于找到一个策略$\\pi$，使得其状态-动作对的出现频率(occupancy measure)尽量接近专家生成的状态-动作对，“接近”的评价标准是$\\psi^*$，即约束函数的共轭函数(convex conjugate)。</p>\n<ul>\n<li>以上面的发现为基础，作者证明了学徒学习(Apprenticeship learning)的目标函数，实际上对应着一种特殊的约束函数$\\psi$，即indicator function $\\psi=\\delta_{\\mathcal{C}}$，$\\delta_{\\mathcal{C}}(c)=0$ if $c\\in\\mathcal{C}$, and $\\infty$ otherwise。注意学徒学习的目标函数是：</li>\n</ul>\n<p>$$ \\min_{\\pi} \\max_{c\\in\\mathcal{C}} \\mathbb{E}_{\\pi} [c(s,a)] - \\mathbb{E}_{\\pi_E} [c(s,a)]$$</p>\n<p>　　而使用了$\\delta_{\\mathcal{C}}$的目标函数为：</p>\n<p>$$ \\min_{\\pi} -H(\\pi)+ \\max_{c\\in\\mathcal{C}}\\mathbb{E}_{\\pi} [c(s,a)] - \\mathbb{E}_{\\pi_E} [c(s,a)]$$</p>\n<ul>\n<li>作者的另一篇前作证明了用梯度下降的方式求解上式，可以用一个两步交替迭代的方法去解，而其中的一步正是对应于RL里的<font color=\"#FF0000\"><strong>policy gradient</strong></font>。简单的推导如下：</li>\n</ul>\n<p>　　设学习者用参数$\\theta$做价值函数模拟，故策略可写作$\\pi_{\\theta}$。对$\\theta$求导：<br>$$\\nabla_{\\theta} \\max_{c\\in\\mathcal{C}} \\mathbb{E}_{\\pi_{\\theta}}[c(s,a)] - \\mathbb{E}_{\\pi_{E}}[c(s,a)]$$<br>　　设两个辅助函数：$\\hat{c}=\\arg\\max_{c\\in\\mathcal{C}}\\mathbb{E}_{\\pi_{\\theta}}[c(s,a)] - \\mathbb{E}_{\\pi_{E}}[c(s,a)]$, 和 $Q_{\\hat{c}}(s,a)=\\mathbb{E_{\\theta}}[\\hat{c}(s,a)|s_0=s,a_0=a]$<br>　　之前的梯度可以转化为：<br>$$ Left=\\nabla_{\\theta}\\mathbb{E}_{\\pi_{\\theta}}[\\hat{c}(s,a)] = \\mathbb{E}_{\\pi_{\\theta}}[\\nabla_\\theta\\log \\pi_{\\theta}(a|s) Q_{\\hat{c}}(s,a)]$$<br>　　其中最后一个等号用到了经典的policy gradient的推导，不熟悉的可以参考这篇 <a href=\"http://karpathy.github.io/2016/05/31/rl/\" target=\"_blank\" rel=\"external\">Karpathy的blog</a>。</p>\n<ul>\n<li>上述的两步迭代求解方法即：<br><strong>1. 使用当前的策略$\\pi_{\\theta_i}$在环境中采样，并依此用上式拟合修正的代价函数 $\\hat{c}$ ;</strong><br><strong>2. 使用拟合出的代价函数 $\\hat{c}$，根据采样的历史记录，使用policy gradient优化当前策略得到新策略$\\pi_{\\theta_{i+1}}$ .</strong></li>\n</ul>\n<blockquote>\n<p>以上两步中，步骤1对应着找到最能区分expert和learner的cost function的作用，步骤2对应根据cost function，优化learner的策略。</p>\n</blockquote>\n<h2 id=\"引入gan\"><a href=\"#引入GAN\" class=\"headerlink\" title=\"引入GAN\"></a>引入GAN</h2><p>　　OK，经过以上一段复杂的数学推导，终于到了在目标函数中引入GAN的时候了。回顾一下，上文分析了每一种约束函数$\\psi$，都可以对应一种新的学徒学习的算法。作者证明了，GAN对应的约束函数是：</p>\n<p><img src=\"/images/imitation_learning_paper/equation13.png\" alt=\"\"></p>\n<p>　　可以证明，其目标函数有如下形式：<br>$$\\psi^*_{GA}(\\rho_\\pi - \\rho_{\\pi_E})-\\lambda H(\\pi) = \\sup_{D\\in (0,1)^{\\mathcal{S}\\times\\mathcal{A}}} \\mathbb{E}_{\\pi}[\\log(D(s,a))] + \\mathbb{E}_{\\pi_E}[\\log(1-D(s,a))]-\\lambda H(\\pi)$$</p>\n<p>　　于是终于，熟悉的式子出现啦~ 由上式可知，GAN对应的cost function $c(s,a)=\\log D(s,a)$，即验钞机的验钞本领。当learner越厉害（可以以假乱真）时，$D(s,a)\\rightarrow 1$，对应$c(s,a)$ 最大，算法约需要<em>“用力”</em>地找到一个可以区分expert 和 learner的cost function。</p>\n<h2 id=\"gail算法流程\"><a href=\"#GAIL算法流程\" class=\"headerlink\" title=\"GAIL算法流程\"></a>GAIL算法流程</h2><p>作者将GAN+Imitation Learning(IL)命名为GAIL（名字不错）。最终的算法流程其实非常简单，见下图，paper的关键其实在于之前的数学推导。</p>\n<p><img src=\"/images/imitation_learning_paper/algorithm_gail.png\" alt=\"\" title=\"GAIL算法框架\"></p>\n<p>　　算法很清晰，第一步用GAN的目标函数拟合cost function，对应于GAN的$D_w(s,a)$ (line4)；第二步根据cost function使用TRPO(Trust Region Policy Optimization，一种policy gradient的改进，用于训练时不容易跑飞)更新策略$\\pi_\\theta$。重复迭代至收敛即可。</p>\n<h2 id=\"实验与代码实现\"><a href=\"#实验与代码实现\" class=\"headerlink\" title=\"实验与代码实现\"></a>实验与代码实现</h2><p>　　OpenAI kindly提供了GAIL的代码，传上了github：<a href=\"https://github.com/openai/imitation\" target=\"_blank\" rel=\"external\">https://github.com/openai/imitation</a> 。时间原因我还没有自己跑哈，测试之后再更新这段。</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"　　\"></a>　　</h2><h1 id=\"third-person-imitation-learning\"><a href=\"#Third-Person-Imitation-Learning\" class=\"headerlink\" title=\"Third-Person Imitation Learning\"></a>Third-Person Imitation Learning</h1><p>　　理解了GAIL，我们的这篇正文”Third-Person Imitation Learning”就很简单了。个人浅见，这篇文章的主要贡献在于make it possible去做第三人称模仿学习这件事，而算法本身更多的是在现有算法上添砖加瓦，创新点并不是很多。</p>\n<p><img src=\"/images/imitation_learning_paper/third_title.png\" alt=\"\" title=\"Third-Person Imitation Learning 网络结构\"></p>\n<p>　　本文基本的想法，即在GAIL的基础上，引入第三人称和第一人称之间的domain difference，用经典的Deep Domain Adaptation的手段来求解GAIL。具体来说，作者将GAN的网络分成了两部分，前一半低层layers用于提取特征（设其为$D_F$），在两个domain共用；后一半的高层layers则分别对应于expert domain的$D_R$ 和 区分expert和learner的分类器$D_D$。作者用了一个BP过程中翻转梯度方向的trick $\\mathcal{G}$，使得$D_R, D_D, D_F$在优化目标中都取$\\min$。最终的算法流程图和GAIL相比变化不大，作者同样使用了TRPO作为policy gradient的解法。</p>\n<p><img src=\"/images/imitation_learning_paper/algorithm_third.png\" alt=\"\" title=\"Third-Person Imitation Learning 算法框架\"></p>\n<hr>\n<h1 id=\"相关paper推荐\"><a href=\"#相关paper推荐\" class=\"headerlink\" title=\"相关paper推荐\"></a>相关paper推荐</h1><p>由于本文实质上只要是在讲GAIL，故这里只推荐一些后续cite GAIL的文章。</p>\n<ul>\n<li><p>最值得一看的是Deepmind的一篇文章<a href=\"https://arxiv.org/pdf/1610.01945.pdf\" target=\"_blank\" rel=\"external\">“Connecting Generative Adversarial Networks<br>and Actor-Critic Methods”</a>，文章中分析了 GAN 和 RL中的Actor-Critic(AC) 算法在方法论上的相似性，并详细分析了两种算法在实际应用中的 stabilizing 方法，至少可以刚做一个实现GAN和AC算法时的trick使用手册。</p>\n</li>\n<li><p>另外有一篇很有意思的paper，关于<a href=\"https://arxiv.org/pdf/1703.03121\" target=\"_blank\" rel=\"external\">multi-agent imitation learning</a>，内容大致是让一群机器人学习现实中球员比赛时的行为，以理解multi-agent间的协同合作关系。</p>\n</li>\n</ul>\n<hr>\n<h1 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h1><p>　　这篇blog借着”Third-Person Imitation Learning”这篇文章，回顾了Imitation Learning的问题设定，及前作将GAN和RL结合起来做imitation learning的算法GAIL，并详细解释了部分的理论推导。</p>\n<p>从这篇文章我们可以得到的take-away是：</p>\n<ul>\n<li>Imitation Learning作为目前高速发展的一个领域，其应用场景在不断的扩展。几年之后机器人的模仿、学习能力就能上一个台阶就说不定。</li>\n<li>用IRL的方法做Imitation Learning，其特例的IRL过程可以演化为GAN的形式，通过迭代的方式，优化GAN得到代价函数，再根据代价函数使用policy gradient优化策略，直至收敛。</li>\n<li>GAIL的想法可以看到一定的CV和NLP领域中的应用，如视频/序列的生成。</li>\n</ul>\n<hr>\n<p><strong> 最近访问 </strong> </p>\n<div class=\"ds-recent-visitors\" data-num-items=\"36\" data-avatar-size=\"42\" id=\"ds-recent-visitors\"><br></div>","excerpt":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><p>　　最近终于项目有一定进展，可以有时间看paper啦，撒花~  这个系列扫一下ICLR2017关于DRL的文章，希望能有些insights。</p>\n<p>　　首先说个题外话，ICLR的审稿流程真是可怕，reviewer和author来回好几轮，工作量足够赶上一个journal，没有金刚钻是不敢揽这个瓷器活的。DRL的圈子被Deepmind和OpenAI把持，外人想进去很难，不过现阶段用DRL去做一些其他应用的道路还是很宽的，无论是写paper还是工业界都有着很大的价值，值得follow。</p>\n<hr>\n<p><img src=\"/images/imitation_learning_paper/iclr_imitation_learning_title.png\" alt=\"\"></p>\n<blockquote>\n<p>解决的问题：第三人称视角模仿学习<br>使用方法：GAN、RL (TRPO)<br>亮点：GAN与RL结合、模仿学习应用场景扩展</p>\n</blockquote>","more":"<hr>\n<h1 id=\"文章简介\"><a href=\"#文章简介\" class=\"headerlink\" title=\"文章简介\"></a>文章简介</h1><p>　　第一篇文章讲一下 <a href=\"https://arxiv.org/pdf/1703.01703.pdf\">“Third-Person Imitation Learning”</a> [1]，出自OpenAI，挂着Pieter Abbeel和Ilya Sutskever两座大神的名字。正如题目所说，这篇文章讲的是以第三人称视角进行模仿学习(imitation learning)，即学习者通过观察老师的行为，进行模仿并最终实现任务，此过程中不需要学习者以第一人称的视角实际体验该任务，这个设定实际上很符合人类婴儿的学习过程。以后imitation learning可能就不需要agent亲自上场试验啦，想想看机器人仅靠眼睛看人类的行为即可做到模仿、学习，既感到fancy又觉得可怕。</p>\n<p><img src=\"/images/imitation_learning_paper/title.jpg\" alt=\"\" title=\"模仿学习\"></p>\n<p>　　推荐这篇文章的另外一个原因，其实是延续自其前作的一个很有意思的思想，即将Generative Adversarial Network (GAN)的思想引入Imitation Learning，把目前大火的 <font color=\"#FF0000\"><strong>GAN</strong></font> 和 <font color=\"#FF0000\"><strong>RL</strong></font> 有机的结合在一起。个人浅见，这是目前GAN在实际工业应用中最可能实现突破的一点。</p>\n<p>　　理解这篇paper首先需要解释两个问题：1) 什么是imitation learning; 2) 理解其前作：<a href=\"http://papers.nips.cc/paper/6391-generative-adversarial-imitation-learning.pdf\">“Generative Adversarial Imitation Learning”</a> (NIPS 16，作者是OpenAI的Jonathan Ho和Stefano Ermon) [2]。</p>\n<hr>\n<h1 id=\"模仿学习（Imitation-Learning）\"><a href=\"#模仿学习（Imitation-Learning）\" class=\"headerlink\" title=\"模仿学习（Imitation Learning）\"></a>模仿学习（Imitation Learning）</h1><blockquote>\n<p>模仿学习，指学习者通过模仿专家的示范动作来完成任务的一种算法。</p>\n</blockquote>\n<p>　　根据[2]的描述，其特点是：</p>\n<ul>\n<li>学习者(learner)可以得到专家(expert)行为的轨迹或历史记录(trajactory)；</li>\n<li>在学习过程中，learner无法从expert处追加查询更多的数据；</li>\n<li>Learner在学习过程中无法获得任何显式的奖励信号(reward signal)。</li>\n</ul>\n<p>　　公式上，模仿学习一般提供专家的决策数据 {$\\tau_E$}，每个决策包含着状态和动作序列<br>$&lt; s_1^i,a_1^i,s_2^i,a_2^2,…,s_n^i&gt;$。将所有的状态-动作对抽离出来，以状态作为feature，动作作为label进行学习，并使得模型生成的状态-动作与输入轨迹尽可能相近。</p>\n<p>　　实现imitation learning的方式主要有两种。最简单直接可以想到的，即用supervised learning的方法直接去拟合专家轨迹的状态-动作对，作为learner的策略(policy)。这种算法被称为 <strong>Behavioral Cloning</strong>。相关的supervised learning算法，可以想到的例子如RNN (LSTM)，Structural SVM, CRF等。由于传统的supervised learning不考虑agent和环境间的交互，Behavioral Cloning存在着序列行为中累积误差逐渐增大的问题。</p>\n<p><img src=\"/images/imitation_learning_paper/aggreg_error.png\" alt=\"\" title=\"误差累积问题\"></p>\n<p>　　另一种算法，即 <strong> IRL (Inverse Reinforcement Learning) </strong>，根据专家的轨迹拟合出其做决策时的cost function，并根据这个cost function进行强化学习(Reinforcement Learning)以实现行为。该算法引入了agent和environment的交互，更适用于时间序列的学习问题。[2]中提到，由于IRL算法在内层循环中需要运行Reinforcement learning这一花费大量时间的过程，其scalability能力很有限，换句话说就是跑的太慢啦，不能解大型问题。</p>\n<hr>\n<h1 id=\"GAIL-Generative-Adversarial-Imitation-Learning\"><a href=\"#GAIL-Generative-Adversarial-Imitation-Learning\" class=\"headerlink\" title=\"GAIL (Generative Adversarial Imitation Learning)\"></a>GAIL (Generative Adversarial Imitation Learning)</h1><p>　　这篇前作[2]解决的问题，即调过IRL，直接使用RL中的policy gradient算法学习专家的策略。</p>\n<h2 id=\"IRL-目标函数解释\"><a href=\"#IRL-目标函数解释\" class=\"headerlink\" title=\"IRL 目标函数解释\"></a>IRL 目标函数解释</h2><p>　　Inverse Reinforcement Learning，顾名思义，是RL的反过程，即通过行为体某策略的动作轨迹，反推行为体做决策时使用的奖励函数reward或代价函数cost。首先回顾一下符号定义：</p>\n<ul>\n<li>$\\mathcal{S}$表示状态集，$\\mathcal{A}$表示动作集；</li>\n<li>状态转移$s\\rightarrow s’$由环境中的转移概率$P(s’|s,a)$决定；</li>\n<li>专家策略由$\\pi_E$表示，学习者策略由$\\pi$表示；</li>\n<li>定义cost function $c(s,a)$，其目的是区分专家决策和学习者的决策，也就是说$c(s,a)$会倾向于给learner的策略比较高的cost，而给专家的策略低的cost；</li>\n<li>定义 $ \\mathbb{E}_{\\pi}[c(s,a)] \\triangleq \\mathbb{E}[\\sum_{t=0}^\\infty \\gamma^t c(s_t,a_t)] $，描述在策略$\\pi$下的total discounted cost之和的期望，类似于RL里的value function.</li>\n</ul>\n<p>　　在此定一下，IRL的目标函数可以定义为:</p>\n<blockquote>\n<p>$$ \\max_{c\\in \\mathbb{C}} \\big(\\min_{\\pi \\in \\Pi} -H(\\pi) + \\mathbb{E}_{\\pi} [c(s,a)] \\big) - \\mathbb{E}_{\\pi_E} [c(s,a)] $$</p>\n</blockquote>\n<p>　　OK，下面我们来解释一下这个objective function。从外向内看，</p>\n<ul>\n<li>首先，cost function $c$的作用是建立一种reward(cost)机制，使得其可以分别expert trajactories和learner trajactories。故公式的最外层是对c求max cost，即Inverse RL里由行为轨迹反求reward(cost)的过程；</li>\n<li>向内一层，在第一个括号之内，是优化learner策略的过程，即求策略$\\pi$，使其可最小化长期cost function；</li>\n<li>最后，$H(\\pi)$ 定义为 <em> $\\gamma$-discounted casusal entropy</em>，$H(\\pi)\\triangleq \\mathbb{E}\\pi [-\\log \\pi(a|s)]$，其作用类似于supervised learning中s为feature，a为label的学习过程。</li>\n</ul>\n<p>当代价函数$c$确定后，IRL在inner loop中就可以使用RL算法，利用学习到的cost function做为依据来得到最优策略：<br>$$ RL(c)=\\arg\\min_{\\pi \\in \\Pi} -H(\\pi)+\\mathbb{E}_{\\pi}[c(s,a)] $$</p>\n<blockquote>\n<p>综上，IRL的求解过程为：计算learner的策略$\\pi$使其尽量模仿expert的行为，并建立代价函数$c$使其尽量区分expert的真实轨迹和learner的模仿轨迹，重复迭代以上两步骤至收敛。</p>\n</blockquote>\n<p>　　<em>等等，这是不是看起来很熟悉？很像GAN里面的印钞机和验钞机有没有？！</em> 没错！GAIL这篇文章的想法，就是将IRL改写成GAN的形式，使用neural network的强大特征表征能力来提升Imitation Learning的性能。当然，这篇文章的另一个贡献是将GAN以及 Abbeel and Ng 和 Syed之前的两篇学徒学习(Apprenticeship learning)的算法从数学上统一在了一个框架中，得到了很漂亮的公式表达。</p>\n<h2 id=\"公式推导与重要结论\"><a href=\"#公式推导与重要结论\" class=\"headerlink\" title=\"公式推导与重要结论\"></a>公式推导与重要结论</h2><p>介绍完IRL的主要思路后，文章里做了一系列的数学推导，得到了一个很general的公式来表征模仿学习的问题。我们这里就只提一些重要结论啦，感兴趣的读者可以啃啃原文。</p>\n<ul>\n<li><p>由于专家轨迹的量有限，IRL学习cost function $c$时很容易overfit，于是没什么可说的，加个regularizer $\\psi$吧！于是IRL目标函数变为了<br>$$ IRL_{\\psi}(\\pi_E) = \\arg\\max_{c\\in\\mathbb{R}^{\\mathcal{S}\\times \\mathcal{A}}} -\\psi(c) + \\big(\\min_{\\pi \\in \\Pi} -H(\\pi) + \\mathbb{E}_{\\pi} [c(s,a)] \\big) - \\mathbb{E}_{\\pi_E}[c(s,a)]$$</p>\n</li>\n<li><p>定义occupancy measure $\\rho_{\\pi}: \\mathcal{S}\\times\\mathcal{A}\\rightarrow\\mathbb{R}$ 为 $\\rho_{\\pi}(s,a)=\\pi(a|s)\\sum_{t=0}^{\\infty}\\gamma^t P(s_t=s|\\pi)$，表示策略$\\pi$下状态-动作对$&lt; s,a&gt;$出现的频率。则使用IRL进行Imitation learning的全过程可以推导为：<br>$$ RL\\circ IRL_{\\psi}(\\pi_E) = \\arg\\min_{\\pi \\in \\Pi} - H(\\pi)+\\psi^*(\\rho_{\\pi} - \\rho_{\\pi_E})$$</p>\n</li>\n</ul>\n<p>　　上式说明，加入了约束项的IRL目标函数，等价于找到一个策略$\\pi$，使得其状态-动作对的出现频率(occupancy measure)尽量接近专家生成的状态-动作对，“接近”的评价标准是$\\psi^*$，即约束函数的共轭函数(convex conjugate)。</p>\n<ul>\n<li>以上面的发现为基础，作者证明了学徒学习(Apprenticeship learning)的目标函数，实际上对应着一种特殊的约束函数$\\psi$，即indicator function $\\psi=\\delta_{\\mathcal{C}}$，$\\delta_{\\mathcal{C}}(c)=0$ if $c\\in\\mathcal{C}$, and $\\infty$ otherwise。注意学徒学习的目标函数是：</li>\n</ul>\n<p>$$ \\min_{\\pi} \\max_{c\\in\\mathcal{C}} \\mathbb{E}_{\\pi} [c(s,a)] - \\mathbb{E}_{\\pi_E} [c(s,a)]$$</p>\n<p>　　而使用了$\\delta_{\\mathcal{C}}$的目标函数为：</p>\n<p>$$ \\min_{\\pi} -H(\\pi)+ \\max_{c\\in\\mathcal{C}}\\mathbb{E}_{\\pi} [c(s,a)] - \\mathbb{E}_{\\pi_E} [c(s,a)]$$</p>\n<ul>\n<li>作者的另一篇前作证明了用梯度下降的方式求解上式，可以用一个两步交替迭代的方法去解，而其中的一步正是对应于RL里的<font color=\"#FF0000\"><strong>policy gradient</strong></font>。简单的推导如下：</li>\n</ul>\n<p>　　设学习者用参数$\\theta$做价值函数模拟，故策略可写作$\\pi_{\\theta}$。对$\\theta$求导：<br>$$\\nabla_{\\theta} \\max_{c\\in\\mathcal{C}} \\mathbb{E}_{\\pi_{\\theta}}[c(s,a)] - \\mathbb{E}_{\\pi_{E}}[c(s,a)]$$<br>　　设两个辅助函数：$\\hat{c}=\\arg\\max_{c\\in\\mathcal{C}}\\mathbb{E}_{\\pi_{\\theta}}[c(s,a)] - \\mathbb{E}_{\\pi_{E}}[c(s,a)]$, 和 $Q_{\\hat{c}}(s,a)=\\mathbb{E_{\\theta}}[\\hat{c}(s,a)|s_0=s,a_0=a]$<br>　　之前的梯度可以转化为：<br>$$ Left=\\nabla_{\\theta}\\mathbb{E}_{\\pi_{\\theta}}[\\hat{c}(s,a)] = \\mathbb{E}_{\\pi_{\\theta}}[\\nabla_\\theta\\log \\pi_{\\theta}(a|s) Q_{\\hat{c}}(s,a)]$$<br>　　其中最后一个等号用到了经典的policy gradient的推导，不熟悉的可以参考这篇 <a href=\"http://karpathy.github.io/2016/05/31/rl/\">Karpathy的blog</a>。</p>\n<ul>\n<li>上述的两步迭代求解方法即：<br><strong>1. 使用当前的策略$\\pi_{\\theta_i}$在环境中采样，并依此用上式拟合修正的代价函数 $\\hat{c}$ ;</strong><br><strong>2. 使用拟合出的代价函数 $\\hat{c}$，根据采样的历史记录，使用policy gradient优化当前策略得到新策略$\\pi_{\\theta_{i+1}}$ .</strong></li>\n</ul>\n<blockquote>\n<p>以上两步中，步骤1对应着找到最能区分expert和learner的cost function的作用，步骤2对应根据cost function，优化learner的策略。</p>\n</blockquote>\n<h2 id=\"引入GAN\"><a href=\"#引入GAN\" class=\"headerlink\" title=\"引入GAN\"></a>引入GAN</h2><p>　　OK，经过以上一段复杂的数学推导，终于到了在目标函数中引入GAN的时候了。回顾一下，上文分析了每一种约束函数$\\psi$，都可以对应一种新的学徒学习的算法。作者证明了，GAN对应的约束函数是：</p>\n<p><img src=\"/images/imitation_learning_paper/equation13.png\" alt=\"\"></p>\n<p>　　可以证明，其目标函数有如下形式：<br>$$\\psi^*_{GA}(\\rho_\\pi - \\rho_{\\pi_E})-\\lambda H(\\pi) = \\sup_{D\\in (0,1)^{\\mathcal{S}\\times\\mathcal{A}}} \\mathbb{E}_{\\pi}[\\log(D(s,a))] + \\mathbb{E}_{\\pi_E}[\\log(1-D(s,a))]-\\lambda H(\\pi)$$</p>\n<p>　　于是终于，熟悉的式子出现啦~ 由上式可知，GAN对应的cost function $c(s,a)=\\log D(s,a)$，即验钞机的验钞本领。当learner越厉害（可以以假乱真）时，$D(s,a)\\rightarrow 1$，对应$c(s,a)$ 最大，算法约需要<em>“用力”</em>地找到一个可以区分expert 和 learner的cost function。</p>\n<h2 id=\"GAIL算法流程\"><a href=\"#GAIL算法流程\" class=\"headerlink\" title=\"GAIL算法流程\"></a>GAIL算法流程</h2><p>作者将GAN+Imitation Learning(IL)命名为GAIL（名字不错）。最终的算法流程其实非常简单，见下图，paper的关键其实在于之前的数学推导。</p>\n<p><img src=\"/images/imitation_learning_paper/algorithm_gail.png\" alt=\"\" title=\"GAIL算法框架\"></p>\n<p>　　算法很清晰，第一步用GAN的目标函数拟合cost function，对应于GAN的$D_w(s,a)$ (line4)；第二步根据cost function使用TRPO(Trust Region Policy Optimization，一种policy gradient的改进，用于训练时不容易跑飞)更新策略$\\pi_\\theta$。重复迭代至收敛即可。</p>\n<h2 id=\"实验与代码实现\"><a href=\"#实验与代码实现\" class=\"headerlink\" title=\"实验与代码实现\"></a>实验与代码实现</h2><p>　　OpenAI kindly提供了GAIL的代码，传上了github：<a href=\"https://github.com/openai/imitation\">https://github.com/openai/imitation</a> 。时间原因我还没有自己跑哈，测试之后再更新这段。</p>\n<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"　　\"></a>　　</h2><h1 id=\"Third-Person-Imitation-Learning\"><a href=\"#Third-Person-Imitation-Learning\" class=\"headerlink\" title=\"Third-Person Imitation Learning\"></a>Third-Person Imitation Learning</h1><p>　　理解了GAIL，我们的这篇正文”Third-Person Imitation Learning”就很简单了。个人浅见，这篇文章的主要贡献在于make it possible去做第三人称模仿学习这件事，而算法本身更多的是在现有算法上添砖加瓦，创新点并不是很多。</p>\n<p><img src=\"/images/imitation_learning_paper/third_title.png\" alt=\"\" title=\"Third-Person Imitation Learning 网络结构\"></p>\n<p>　　本文基本的想法，即在GAIL的基础上，引入第三人称和第一人称之间的domain difference，用经典的Deep Domain Adaptation的手段来求解GAIL。具体来说，作者将GAN的网络分成了两部分，前一半低层layers用于提取特征（设其为$D_F$），在两个domain共用；后一半的高层layers则分别对应于expert domain的$D_R$ 和 区分expert和learner的分类器$D_D$。作者用了一个BP过程中翻转梯度方向的trick $\\mathcal{G}$，使得$D_R, D_D, D_F$在优化目标中都取$\\min$。最终的算法流程图和GAIL相比变化不大，作者同样使用了TRPO作为policy gradient的解法。</p>\n<p><img src=\"/images/imitation_learning_paper/algorithm_third.png\" alt=\"\" title=\"Third-Person Imitation Learning 算法框架\"></p>\n<hr>\n<h1 id=\"相关paper推荐\"><a href=\"#相关paper推荐\" class=\"headerlink\" title=\"相关paper推荐\"></a>相关paper推荐</h1><p>由于本文实质上只要是在讲GAIL，故这里只推荐一些后续cite GAIL的文章。</p>\n<ul>\n<li><p>最值得一看的是Deepmind的一篇文章<a href=\"https://arxiv.org/pdf/1610.01945.pdf\">“Connecting Generative Adversarial Networks<br>and Actor-Critic Methods”</a>，文章中分析了 GAN 和 RL中的Actor-Critic(AC) 算法在方法论上的相似性，并详细分析了两种算法在实际应用中的 stabilizing 方法，至少可以刚做一个实现GAN和AC算法时的trick使用手册。</p>\n</li>\n<li><p>另外有一篇很有意思的paper，关于<a href=\"https://arxiv.org/pdf/1703.03121\">multi-agent imitation learning</a>，内容大致是让一群机器人学习现实中球员比赛时的行为，以理解multi-agent间的协同合作关系。</p>\n</li>\n</ul>\n<hr>\n<h1 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h1><p>　　这篇blog借着”Third-Person Imitation Learning”这篇文章，回顾了Imitation Learning的问题设定，及前作将GAN和RL结合起来做imitation learning的算法GAIL，并详细解释了部分的理论推导。</p>\n<p>从这篇文章我们可以得到的take-away是：</p>\n<ul>\n<li>Imitation Learning作为目前高速发展的一个领域，其应用场景在不断的扩展。几年之后机器人的模仿、学习能力就能上一个台阶就说不定。</li>\n<li>用IRL的方法做Imitation Learning，其特例的IRL过程可以演化为GAN的形式，通过迭代的方式，优化GAN得到代价函数，再根据代价函数使用policy gradient优化策略，直至收敛。</li>\n<li>GAIL的想法可以看到一定的CV和NLP领域中的应用，如视频/序列的生成。</li>\n</ul>\n<hr>\n<p><strong> 最近访问 </strong> </p>\n<div class=\"ds-recent-visitors\"\n    data-num-items=\"36\"\n    data-avatar-size=\"42\"\n    id=\"ds-recent-visitors\"><br></div>"}],"PostAsset":[],"PostCategory":[{"post_id":"cj1jbutcj00009qs6nenhcxjm","category_id":"cj1jbutct00049qs61tk2nqqp","_id":"cj1jbutdo000d9qs622n6m2sy"},{"post_id":"cj1jbutcp00029qs6g6sdv3x0","category_id":"cj1jbutct00049qs61tk2nqqp","_id":"cj1jbutdq000h9qs60k62x9x0"},{"post_id":"cj1jbutcv00069qs62fcdhslk","category_id":"cj1jbutct00049qs61tk2nqqp","_id":"cj1jbutds000l9qs6g1qpq2dk"},{"post_id":"cj1jbutcz00079qs6nra6q4fq","category_id":"cj1jbutdq000i9qs6pcmgaz5c","_id":"cj1jbutdu000o9qs6heros9sr"},{"post_id":"cj1jbutd100089qs6mxj50ssp","category_id":"cj1jbutds000m9qs6qe54szej","_id":"cj1jbutdw000r9qs6xbxgwsdy"},{"post_id":"cj1jbutd5000c9qs627ld5gts","category_id":"cj1jbutds000m9qs6qe54szej","_id":"cj1jbute1000u9qs6w4lt3iwq"}],"PostTag":[{"post_id":"cj1jbutcj00009qs6nenhcxjm","tag_id":"cj1jbutcu00059qs6osl9fy4i","_id":"cj1jbutd5000b9qs6u7swslsd"},{"post_id":"cj1jbutcp00029qs6g6sdv3x0","tag_id":"cj1jbutcu00059qs6osl9fy4i","_id":"cj1jbutdq000g9qs6rkpbmv4f"},{"post_id":"cj1jbutcv00069qs62fcdhslk","tag_id":"cj1jbutcu00059qs6osl9fy4i","_id":"cj1jbutds000k9qs615cud19g"},{"post_id":"cj1jbutcz00079qs6nra6q4fq","tag_id":"cj1jbutdr000j9qs65qvn7jrg","_id":"cj1jbute1000t9qs6je5b7rur"},{"post_id":"cj1jbutcz00079qs6nra6q4fq","tag_id":"cj1jbutdt000n9qs62h4gjtum","_id":"cj1jbute1000v9qs6imtlca68"},{"post_id":"cj1jbutcz00079qs6nra6q4fq","tag_id":"cj1jbutdu000q9qs6kxzmerf0","_id":"cj1jbute2000x9qs6h26k7uyo"},{"post_id":"cj1jbutd100089qs6mxj50ssp","tag_id":"cj1jbute0000s9qs6dgmclcwe","_id":"cj1jbute3000y9qs6f3hrlbwj"},{"post_id":"cj1jbutd5000c9qs627ld5gts","tag_id":"cj1jbute0000s9qs6dgmclcwe","_id":"cj1jbute4000z9qs6rx6jezx1"}],"Tag":[{"name":"didi","_id":"cj1jbutcu00059qs6osl9fy4i"},{"name":"test","_id":"cj1jbutdr000j9qs65qvn7jrg"},{"name":"RL","_id":"cj1jbutdt000n9qs62h4gjtum"},{"name":"code","_id":"cj1jbutdu000q9qs6kxzmerf0"},{"name":"drl","_id":"cj1jbute0000s9qs6dgmclcwe"}]}}