<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>xz3030&#39;s Blog</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://xz3030.github.io/"/>
  <updated>2017-04-15T13:57:39.000Z</updated>
  <id>http://xz3030.github.io/</id>
  
  <author>
    <name>xuzhejesse</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>【DRL论文阅读】Multi-Agent Cooperation and the Emergence of (Natural) Language Learning</title>
    <link href="http://xz3030.github.io/2017/04/15/iclr17-multi-agent-conversation/"/>
    <id>http://xz3030.github.io/2017/04/15/iclr17-multi-agent-conversation/</id>
    <published>2017-04-15T13:57:39.000Z</published>
    <updated>2017-04-15T13:57:39.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/multi_agent_conversation/title.png" alt=""></p>
<blockquote>
<p>解决的问题：两个机器人之间玩“你画我猜”<br>使用方法：RL (Reinforce), t-SNE可视化分析<br>亮点：很有意思的应用，让两个agent玩游戏，观察agents间的交互方式</p>
</blockquote>
<a id="more"></a>
<hr>
<h1 id="文章简介"><a href="#文章简介" class="headerlink" title="文章简介"></a>文章简介</h1><p>　　这两天尝试啃了几篇非常理论的Paper之后，发现还是找一些更简单、应用更有意思的paper看起来比较轻松~ 于是今天来聊一下一篇很有意思的论文：<a href="https://arxiv.org/pdf/1612.07182.pdf" target="_blank" rel="external">“Multi-Agent Cooperation and the Emergence of (Natural) Language”</a>，来自Deepmind和Facebook AI Research的合作。</p>
<p>　　这篇文章大致做了这么一件事情：让两个agent分别扮演 <em>描述者</em> 和 <em>猜题者</em> 两个角色共同玩一个游戏。描述者看到两张不同concept的图片（分别是目标图片和干扰图片），并得知哪张图是目标图片。游戏的目标是让描述者提供一个词（词库由两个agent提前商量好，类似于暗号），使得猜题者可以通过这个词成功的分别两张图片，找到目标图片。设定有点像“你画我猜”或者是“看动作猜词语”这种游戏。</p>
<p>　　之前我们在研究中更多地都是focus在supervised learning上，而却忽视了通往通用AI之路的一个重要问题：AI和他人协同、交互的能力。这篇文章用很简单的方法，测试了AI间协作交互的能力，并试图分析这种交互，乃至干预AI间交互的形式变成人类可理解的语言，非常有意思。</p>
<blockquote>
<p>AI能否学到如何与同伴通信并互相理解？</p>
</blockquote>
<hr>
<h1 id="游戏流程与算法"><a href="#游戏流程与算法" class="headerlink" title="游戏流程与算法"></a>游戏流程与算法</h1><p>　　这篇文章的想法非常简单，就是一个常用的Reinforce搞定。于是我们重点来看一下作者是如何构造的游戏使得AI间可以协同合作。文章里使用了一种叫做 <em>referential games</em> 的设定：</p>
<ul>
<li><p>首先建立一个图片集$i_1,…,i_N$，它们分别来自于463个基础概念（concept，如猫、苹果、汽车等）、20个大类，每个concept从ImageNet库里拉了100张图片（<em>论万能的ImageNet…</em>）。每一次游戏开始时，系统随机的选择两个concept，并分别选择一张图片 $(i_L,i_R)$，把其中的一个作为目标 $t\in ${$ L,R $}，另一个作为伪装。</p>
</li>
<li><p>游戏中共有两个角色：<em>描述者</em> 和 <em>猜题者</em> 。他们都能看到两张输入图片，但是只有描述者知道哪个是目标图片，即收到了输入 $\theta_S(i_L,i_R,t)$。</p>
</li>
<li><p>描述者和猜题者共同维护一个暗号集 $V$（size为$K$）。描述者选择 $V$ 中的一个暗号 $s$ 传给猜题者。描述者的策略记为：$s(\theta_S(i_L,i_R,t)) \in V$。</p>
</li>
<li><p>猜题者并不清楚哪张图是目标图片，于是他根据描述者传来的信息和两张图片本身来猜测目标图片。猜题者的策略记为：$r\big(i_L,i_R,s(\theta_S(i_L,i_R,t))\big) \in ${$ L,R $} 。</p>
</li>
<li><p>如果猜题者猜对了目标图片，即$r\big(i_L,i_R,s(\theta_S(i_L,i_R,t))\big) = t$，描述者和猜题者同时加一分（胜利）；否则都不加分（失败）。</p>
</li>
</ul>
<p>　　作者建立了两种简单的图像embedding方式，分别是（如下图所示）<br>1) 直接将 VGG的softmax层输出（或fc层输出）embedding。称为 <em>agnostic</em> 方法。<br>2) 将输出再过一层2x1的卷积层降维，把目标图和伪装图的输出混合起来，最后再embedding。称为 <em>informed</em> 方法。</p>
<p><img src="/images/multi_agent_conversation/architecture.png" alt=""></p>
<p>　　建立了policy和reward，玩游戏的问题自然地被转化成了RL过程，作者使用了Reinforce（最简单的policy gradient方法之一）算法来求解。结果很愉快，所有的agent都在10k轮之内成功收敛了，且答对率接近100%！其中，<em>informed</em> 方法相比于 <em>agnostic</em> 方法由于使用了更多的信息，可以收敛更快并更准。</p>
<p>　　当然了，这个游戏本身其实挺简单的（concept之间差的挺大的，imagenet的vgg特征也足够强），所以performance倒并不是最重要的，更有意义的是去分析 agent之间到底是如何(chuan)交(an)互(hao)的，以为这种交互方式是否能被我们人类所理解？</p>
<blockquote>
<p>试想如果有一天 AI 之间发明了一种特殊的暗号，人类完全破译不能。。。 orz </p>
</blockquote>
<hr>
<h1 id="了解ai间的交互方式"><a href="#了解AI间的交互方式" class="headerlink" title="了解AI间的交互方式"></a>了解AI间的交互方式</h1><p>　　为了潜入AI的脑内，分析其与同伴打招呼的方式，作者设计了几种trick来证明或可视化他们的假设。下面我们简单看看作者们的发现：</p>
<blockquote>
<p>AI 有丰富的描述语言，并能抓住重点。</p>
</blockquote>
<p>　　作者的第一个发现，就是 <em>informed sender</em> 使用了较多种的词语作为暗号来通信，且这些暗号中至少有很大一部分代表着不同的意义。同时，这些暗号的意义与人类定义的20种大类有着一定的相关性（最高的一种算法的聚类purity有46%）。<br>　　另一方面，<em>agnostic sender</em> 弱弱地只使用过两个词作为暗号。作者肉眼看的结果是这个暗号类似于 “能喘气的 vs. 不动换的” (living-vs-non-living)，也确实符合imagenet“动物园”的特性。</p>
<blockquote>
<p>可以通过“去常识化”使得 AI 的词语与人类的认知进一步相似</p>
</blockquote>
<p>　　用博弈论的调调来说，常识指所有人都知道的，以及所有人都知道所有人都知道的。。。把输入中的常识分量去掉，可以使得agent学会更本质的东西。作者的实现方法很tricky，给描述者和猜题者看了两张很相近但确不完全相同的图片（比如目标concept是狗，两个agent一个看到的是吉娃娃，另一个则看到的是波士顿小狗）。结果是agent的暗号与人类定义的类别更相近了。这点其实我没太理解。。。</p>
<p><img src="/images/multi_agent_conversation/tsne.png" alt="" title="去常识化之前（左）和之后（右）的AI暗号聚类示意图，同样颜色对应着人类标识的相同大类"></p>
<blockquote>
<p>可以通过加入监督学习的方式使得 AI 用人类的词语通信</p>
</blockquote>
<p>　　最后，当然我们可以用监督学习的方式，显式地让 AI 学会人类的语言来通信。作者在最后一个实验中，迭代地进行增强学习来优化agent和监督学习来让agent学会人类语言。评价时，作者雇了一群真正的人来当猜题者，并由AI生成的词语（监督学习中对应着imagenet的label）作为提示。结果是68%的人能猜到正确的图片（心疼这些人几秒钟。。。），似乎比预想的要低一些，但至少能大幅超过random guess。</p>
<hr>
<h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><p>　　总体来说，这篇文章提出了一个很有意思的应用，依此研究了AI间交互的可能性与其形式。理论算法和实验的结果其实都并不是很强，相信还有很大的提升空间。</p>
<hr>
<p><strong> 最近访问 </strong> </p>
<div class="ds-recent-visitors" data-num-items="36" data-avatar-size="42" id="ds-recent-visitors"><br></div>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/images/multi_agent_conversation/title.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;解决的问题：两个机器人之间玩“你画我猜”&lt;br&gt;使用方法：RL (Reinforce), t-SNE可视化分析&lt;br&gt;亮点：很有意思的应用，让两个agent玩游戏，观察agents间的交互方式&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="drl" scheme="http://xz3030.github.io/categories/drl/"/>
    
    
      <category term="drl" scheme="http://xz3030.github.io/tags/drl/"/>
    
  </entry>
  
  <entry>
    <title>【DRL论文阅读】Third-Person Imitation Learning 和前作Generative Adversarial Imitation Learning</title>
    <link href="http://xz3030.github.io/2017/04/14/iclr17-imitation-learning/"/>
    <id>http://xz3030.github.io/2017/04/14/iclr17-imitation-learning/</id>
    <published>2017-04-13T16:10:56.000Z</published>
    <updated>2017-04-15T11:22:34.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>　　最近终于项目有一定进展，可以有时间看paper啦，撒花~  这个系列扫一下ICLR2017关于DRL的文章，希望能有些insights。</p>
<p>　　首先说个题外话，ICLR的审稿流程真是可怕，reviewer和author来回好几轮，工作量足够赶上一个journal，没有金刚钻是不敢揽这个瓷器活的。DRL的圈子被Deepmind和OpenAI把持，外人想进去很难，不过现阶段用DRL去做一些其他应用的道路还是很宽的，无论是写paper还是工业界都有着很大的价值，值得follow。</p>
<hr>
<p><img src="/images/imitation_learning_paper/iclr_imitation_learning_title.png" alt=""></p>
<blockquote>
<p>解决的问题：第三人称视角模仿学习<br>使用方法：GAN、RL (TRPO)<br>亮点：GAN与RL结合、模仿学习应用场景扩展</p>
</blockquote>
<a id="more"></a>
<hr>
<h1 id="文章简介"><a href="#文章简介" class="headerlink" title="文章简介"></a>文章简介</h1><p>　　第一篇文章讲一下 <a href="https://arxiv.org/pdf/1703.01703.pdf" target="_blank" rel="external">“Third-Person Imitation Learning”</a> [1]，出自OpenAI，挂着Pieter Abbeel和Ilya Sutskever两座大神的名字。正如题目所说，这篇文章讲的是以第三人称视角进行模仿学习(imitation learning)，即学习者通过观察老师的行为，进行模仿并最终实现任务，此过程中不需要学习者以第一人称的视角实际体验该任务，这个设定实际上很符合人类婴儿的学习过程。以后imitation learning可能就不需要agent亲自上场试验啦，想想看机器人仅靠眼睛看人类的行为即可做到模仿、学习，既感到fancy又觉得可怕。</p>
<p><img src="/images/imitation_learning_paper/title.jpg" alt="" title="模仿学习"></p>
<p>　　推荐这篇文章的另外一个原因，其实是延续自其前作的一个很有意思的思想，即将Generative Adversarial Network (GAN)的思想引入Imitation Learning，把目前大火的 <font color="#FF0000"><strong>GAN</strong></font> 和 <font color="#FF0000"><strong>RL</strong></font> 有机的结合在一起。个人浅见，这是目前GAN在实际工业应用中最可能实现突破的一点。</p>
<p>　　理解这篇paper首先需要解释两个问题：1) 什么是imitation learning; 2) 理解其前作：<a href="http://papers.nips.cc/paper/6391-generative-adversarial-imitation-learning.pdf" target="_blank" rel="external">“Generative Adversarial Imitation Learning”</a> (NIPS 16，作者是OpenAI的Jonathan Ho和Stefano Ermon) [2]。</p>
<hr>
<h1 id="模仿学习imitation-learning"><a href="#模仿学习（Imitation-Learning）" class="headerlink" title="模仿学习（Imitation Learning）"></a>模仿学习（Imitation Learning）</h1><blockquote>
<p>模仿学习，指学习者通过模仿专家的示范动作来完成任务的一种算法。</p>
</blockquote>
<p>　　根据[2]的描述，其特点是：</p>
<ul>
<li>学习者(learner)可以得到专家(expert)行为的轨迹或历史记录(trajactory)；</li>
<li>在学习过程中，learner无法从expert处追加查询更多的数据；</li>
<li>Learner在学习过程中无法获得任何显式的奖励信号(reward signal)。</li>
</ul>
<p>　　公式上，模仿学习一般提供专家的决策数据 {$\tau_E$}，每个决策包含着状态和动作序列<br>$&lt; s_1^i,a_1^i,s_2^i,a_2^2,…,s_n^i&gt;$。将所有的状态-动作对抽离出来，以状态作为feature，动作作为label进行学习，并使得模型生成的状态-动作与输入轨迹尽可能相近。</p>
<p>　　实现imitation learning的方式主要有两种。最简单直接可以想到的，即用supervised learning的方法直接去拟合专家轨迹的状态-动作对，作为learner的策略(policy)。这种算法被称为 <strong>Behavioral Cloning</strong>。相关的supervised learning算法，可以想到的例子如RNN (LSTM)，Structural SVM, CRF等。由于传统的supervised learning不考虑agent和环境间的交互，Behavioral Cloning存在着序列行为中累积误差逐渐增大的问题。</p>
<p><img src="/images/imitation_learning_paper/aggreg_error.png" alt="" title="误差累积问题"></p>
<p>　　另一种算法，即 <strong> IRL (Inverse Reinforcement Learning) </strong>，根据专家的轨迹拟合出其做决策时的cost function，并根据这个cost function进行强化学习(Reinforcement Learning)以实现行为。该算法引入了agent和environment的交互，更适用于时间序列的学习问题。[2]中提到，由于IRL算法在内层循环中需要运行Reinforcement learning这一花费大量时间的过程，其scalability能力很有限，换句话说就是跑的太慢啦，不能解大型问题。</p>
<hr>
<h1 id="gail-generative-adversarial-imitation-learning"><a href="#GAIL-Generative-Adversarial-Imitation-Learning" class="headerlink" title="GAIL (Generative Adversarial Imitation Learning)"></a>GAIL (Generative Adversarial Imitation Learning)</h1><p>　　这篇前作[2]解决的问题，即调过IRL，直接使用RL中的policy gradient算法学习专家的策略。</p>
<h2 id="irl-目标函数解释"><a href="#IRL-目标函数解释" class="headerlink" title="IRL 目标函数解释"></a>IRL 目标函数解释</h2><p>　　Inverse Reinforcement Learning，顾名思义，是RL的反过程，即通过行为体某策略的动作轨迹，反推行为体做决策时使用的奖励函数reward或代价函数cost。首先回顾一下符号定义：</p>
<ul>
<li>$\mathcal{S}$表示状态集，$\mathcal{A}$表示动作集；</li>
<li>状态转移$s\rightarrow s’$由环境中的转移概率$P(s’|s,a)$决定；</li>
<li>专家策略由$\pi_E$表示，学习者策略由$\pi$表示；</li>
<li>定义cost function $c(s,a)$，其目的是区分专家决策和学习者的决策，也就是说$c(s,a)$会倾向于给learner的策略比较高的cost，而给专家的策略低的cost；</li>
<li>定义 $ \mathbb{E}_{\pi}[c(s,a)] \triangleq \mathbb{E}[\sum_{t=0}^\infty \gamma^t c(s_t,a_t)] $，描述在策略$\pi$下的total discounted cost之和的期望，类似于RL里的value function.</li>
</ul>
<p>　　在此定一下，IRL的目标函数可以定义为:</p>
<blockquote>
<p>$$ \max_{c\in \mathbb{C}} \big(\min_{\pi \in \Pi} -H(\pi) + \mathbb{E}_{\pi} [c(s,a)] \big) - \mathbb{E}_{\pi_E} [c(s,a)] $$</p>
</blockquote>
<p>　　OK，下面我们来解释一下这个objective function。从外向内看，</p>
<ul>
<li>首先，cost function $c$的作用是建立一种reward(cost)机制，使得其可以分别expert trajactories和learner trajactories。故公式的最外层是对c求max cost，即Inverse RL里由行为轨迹反求reward(cost)的过程；</li>
<li>向内一层，在第一个括号之内，是优化learner策略的过程，即求策略$\pi$，使其可最小化长期cost function；</li>
<li>最后，$H(\pi)$ 定义为 <em> $\gamma$-discounted casusal entropy</em>，$H(\pi)\triangleq \mathbb{E}\pi [-\log \pi(a|s)]$，其作用类似于supervised learning中s为feature，a为label的学习过程。</li>
</ul>
<p>当代价函数$c$确定后，IRL在inner loop中就可以使用RL算法，利用学习到的cost function做为依据来得到最优策略：<br>$$ RL(c)=\arg\min_{\pi \in \Pi} -H(\pi)+\mathbb{E}_{\pi}[c(s,a)] $$</p>
<blockquote>
<p>综上，IRL的求解过程为：计算learner的策略$\pi$使其尽量模仿expert的行为，并建立代价函数$c$使其尽量区分expert的真实轨迹和learner的模仿轨迹，重复迭代以上两步骤至收敛。</p>
</blockquote>
<p>　　<em>等等，这是不是看起来很熟悉？很像GAN里面的印钞机和验钞机有没有？！</em> 没错！GAIL这篇文章的想法，就是将IRL改写成GAN的形式，使用neural network的强大特征表征能力来提升Imitation Learning的性能。当然，这篇文章的另一个贡献是将GAN以及 Abbeel and Ng 和 Syed之前的两篇学徒学习(Apprenticeship learning)的算法从数学上统一在了一个框架中，得到了很漂亮的公式表达。</p>
<h2 id="公式推导与重要结论"><a href="#公式推导与重要结论" class="headerlink" title="公式推导与重要结论"></a>公式推导与重要结论</h2><p>介绍完IRL的主要思路后，文章里做了一系列的数学推导，得到了一个很general的公式来表征模仿学习的问题。我们这里就只提一些重要结论啦，感兴趣的读者可以啃啃原文。</p>
<ul>
<li><p>由于专家轨迹的量有限，IRL学习cost function $c$时很容易overfit，于是没什么可说的，加个regularizer $\psi$吧！于是IRL目标函数变为了<br>$$ IRL_{\psi}(\pi_E) = \arg\max_{c\in\mathbb{R}^{\mathcal{S}\times \mathcal{A}}} -\psi(c) + \big(\min_{\pi \in \Pi} -H(\pi) + \mathbb{E}_{\pi} [c(s,a)] \big) - \mathbb{E}_{\pi_E}[c(s,a)]$$</p>
</li>
<li><p>定义occupancy measure $\rho_{\pi}: \mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}$ 为 $\rho_{\pi}(s,a)=\pi(a|s)\sum_{t=0}^{\infty}\gamma^t P(s_t=s|\pi)$，表示策略$\pi$下状态-动作对$&lt; s,a&gt;$出现的频率。则使用IRL进行Imitation learning的全过程可以推导为：<br>$$ RL\circ IRL_{\psi}(\pi_E) = \arg\min_{\pi \in \Pi} - H(\pi)+\psi^*(\rho_{\pi} - \rho_{\pi_E})$$</p>
</li>
</ul>
<p>　　上式说明，加入了约束项的IRL目标函数，等价于找到一个策略$\pi$，使得其状态-动作对的出现频率(occupancy measure)尽量接近专家生成的状态-动作对，“接近”的评价标准是$\psi^*$，即约束函数的共轭函数(convex conjugate)。</p>
<ul>
<li>以上面的发现为基础，作者证明了学徒学习(Apprenticeship learning)的目标函数，实际上对应着一种特殊的约束函数$\psi$，即indicator function $\psi=\delta_{\mathcal{C}}$，$\delta_{\mathcal{C}}(c)=0$ if $c\in\mathcal{C}$, and $\infty$ otherwise。注意学徒学习的目标函数是：</li>
</ul>
<p>$$ \min_{\pi} \max_{c\in\mathcal{C}} \mathbb{E}_{\pi} [c(s,a)] - \mathbb{E}_{\pi_E} [c(s,a)]$$</p>
<p>　　而使用了$\delta_{\mathcal{C}}$的目标函数为：</p>
<p>$$ \min_{\pi} -H(\pi)+ \max_{c\in\mathcal{C}}\mathbb{E}_{\pi} [c(s,a)] - \mathbb{E}_{\pi_E} [c(s,a)]$$</p>
<ul>
<li>作者的另一篇前作证明了用梯度下降的方式求解上式，可以用一个两步交替迭代的方法去解，而其中的一步正是对应于RL里的<font color="#FF0000"><strong>policy gradient</strong></font>。简单的推导如下：</li>
</ul>
<p>　　设学习者用参数$\theta$做价值函数模拟，故策略可写作$\pi_{\theta}$。对$\theta$求导：<br>$$\nabla_{\theta} \max_{c\in\mathcal{C}} \mathbb{E}_{\pi_{\theta}}[c(s,a)] - \mathbb{E}_{\pi_{E}}[c(s,a)]$$<br>　　设两个辅助函数：$\hat{c}=\arg\max_{c\in\mathcal{C}}\mathbb{E}_{\pi_{\theta}}[c(s,a)] - \mathbb{E}_{\pi_{E}}[c(s,a)]$, 和 $Q_{\hat{c}}(s,a)=\mathbb{E_{\theta}}[\hat{c}(s,a)|s_0=s,a_0=a]$<br>　　之前的梯度可以转化为：<br>$$ Left=\nabla_{\theta}\mathbb{E}_{\pi_{\theta}}[\hat{c}(s,a)] = \mathbb{E}_{\pi_{\theta}}[\nabla_\theta\log \pi_{\theta}(a|s) Q_{\hat{c}}(s,a)]$$<br>　　其中最后一个等号用到了经典的policy gradient的推导，不熟悉的可以参考这篇 <a href="http://karpathy.github.io/2016/05/31/rl/" target="_blank" rel="external">Karpathy的blog</a>。</p>
<ul>
<li>上述的两步迭代求解方法即：<br><strong>1. 使用当前的策略$\pi_{\theta_i}$在环境中采样，并依此用上式拟合修正的代价函数 $\hat{c}$ ;</strong><br><strong>2. 使用拟合出的代价函数 $\hat{c}$，根据采样的历史记录，使用policy gradient优化当前策略得到新策略$\pi_{\theta_{i+1}}$ .</strong></li>
</ul>
<blockquote>
<p>以上两步中，步骤1对应着找到最能区分expert和learner的cost function的作用，步骤2对应根据cost function，优化learner的策略。</p>
</blockquote>
<h2 id="引入gan"><a href="#引入GAN" class="headerlink" title="引入GAN"></a>引入GAN</h2><p>　　OK，经过以上一段复杂的数学推导，终于到了在目标函数中引入GAN的时候了。回顾一下，上文分析了每一种约束函数$\psi$，都可以对应一种新的学徒学习的算法。作者证明了，GAN对应的约束函数是：</p>
<p><img src="/images/imitation_learning_paper/equation13.png" alt=""></p>
<p>　　可以证明，其目标函数有如下形式：<br>$$\psi^*_{GA}(\rho_\pi - \rho_{\pi_E})-\lambda H(\pi) = \sup_{D\in (0,1)^{\mathcal{S}\times\mathcal{A}}} \mathbb{E}_{\pi}[\log(D(s,a))] + \mathbb{E}_{\pi_E}[\log(1-D(s,a))]-\lambda H(\pi)$$</p>
<p>　　于是终于，熟悉的式子出现啦~ 由上式可知，GAN对应的cost function $c(s,a)=\log D(s,a)$，即验钞机的验钞本领。当learner越厉害（可以以假乱真）时，$D(s,a)\rightarrow 1$，对应$c(s,a)$ 最大，算法约需要<em>“用力”</em>地找到一个可以区分expert 和 learner的cost function。</p>
<h2 id="gail算法流程"><a href="#GAIL算法流程" class="headerlink" title="GAIL算法流程"></a>GAIL算法流程</h2><p>作者将GAN+Imitation Learning(IL)命名为GAIL（名字不错）。最终的算法流程其实非常简单，见下图，paper的关键其实在于之前的数学推导。</p>
<p><img src="/images/imitation_learning_paper/algorithm_gail.png" alt="" title="GAIL算法框架"></p>
<p>　　算法很清晰，第一步用GAN的目标函数拟合cost function，对应于GAN的$D_w(s,a)$ (line4)；第二步根据cost function使用TRPO(Trust Region Policy Optimization，一种policy gradient的改进，用于训练时不容易跑飞)更新策略$\pi_\theta$。重复迭代至收敛即可。</p>
<h2 id="实验与代码实现"><a href="#实验与代码实现" class="headerlink" title="实验与代码实现"></a>实验与代码实现</h2><p>　　OpenAI kindly提供了GAIL的代码，传上了github：<a href="https://github.com/openai/imitation" target="_blank" rel="external">https://github.com/openai/imitation</a> 。时间原因我还没有自己跑哈，测试之后再更新这段。</p>
<h2 id=""><a href="#" class="headerlink" title="　　"></a>　　</h2><h1 id="third-person-imitation-learning"><a href="#Third-Person-Imitation-Learning" class="headerlink" title="Third-Person Imitation Learning"></a>Third-Person Imitation Learning</h1><p>　　理解了GAIL，我们的这篇正文”Third-Person Imitation Learning”就很简单了。个人浅见，这篇文章的主要贡献在于make it possible去做第三人称模仿学习这件事，而算法本身更多的是在现有算法上添砖加瓦，创新点并不是很多。</p>
<p><img src="/images/imitation_learning_paper/third_title.png" alt="" title="Third-Person Imitation Learning 网络结构"></p>
<p>　　本文基本的想法，即在GAIL的基础上，引入第三人称和第一人称之间的domain difference，用经典的Deep Domain Adaptation的手段来求解GAIL。具体来说，作者将GAN的网络分成了两部分，前一半低层layers用于提取特征（设其为$D_F$），在两个domain共用；后一半的高层layers则分别对应于expert domain的$D_R$ 和 区分expert和learner的分类器$D_D$。作者用了一个BP过程中翻转梯度方向的trick $\mathcal{G}$，使得$D_R, D_D, D_F$在优化目标中都取$\min$。最终的算法流程图和GAIL相比变化不大，作者同样使用了TRPO作为policy gradient的解法。</p>
<p><img src="/images/imitation_learning_paper/algorithm_third.png" alt="" title="Third-Person Imitation Learning 算法框架"></p>
<hr>
<h1 id="相关paper推荐"><a href="#相关paper推荐" class="headerlink" title="相关paper推荐"></a>相关paper推荐</h1><p>由于本文实质上只要是在讲GAIL，故这里只推荐一些后续cite GAIL的文章。</p>
<ul>
<li><p>最值得一看的是Deepmind的一篇文章<a href="https://arxiv.org/pdf/1610.01945.pdf" target="_blank" rel="external">“Connecting Generative Adversarial Networks<br>and Actor-Critic Methods”</a>，文章中分析了 GAN 和 RL中的Actor-Critic(AC) 算法在方法论上的相似性，并详细分析了两种算法在实际应用中的 stabilizing 方法，至少可以刚做一个实现GAN和AC算法时的trick使用手册。</p>
</li>
<li><p>另外有一篇很有意思的paper，关于<a href="https://arxiv.org/pdf/1703.03121" target="_blank" rel="external">multi-agent imitation learning</a>，内容大致是让一群机器人学习现实中球员比赛时的行为，以理解multi-agent间的协同合作关系。</p>
</li>
</ul>
<hr>
<h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><p>　　这篇blog借着”Third-Person Imitation Learning”这篇文章，回顾了Imitation Learning的问题设定，及前作将GAN和RL结合起来做imitation learning的算法GAIL，并详细解释了部分的理论推导。</p>
<p>从这篇文章我们可以得到的take-away是：</p>
<ul>
<li>Imitation Learning作为目前高速发展的一个领域，其应用场景在不断的扩展。几年之后机器人的模仿、学习能力就能上一个台阶就说不定。</li>
<li>用IRL的方法做Imitation Learning，其特例的IRL过程可以演化为GAN的形式，通过迭代的方式，优化GAN得到代价函数，再根据代价函数使用policy gradient优化策略，直至收敛。</li>
<li>GAIL的想法可以看到一定的CV和NLP领域中的应用，如视频/序列的生成。</li>
</ul>
<hr>
<p><strong> 最近访问 </strong> </p>
<div class="ds-recent-visitors" data-num-items="36" data-avatar-size="42" id="ds-recent-visitors"><br></div>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;　　最近终于项目有一定进展，可以有时间看paper啦，撒花~  这个系列扫一下ICLR2017关于DRL的文章，希望能有些insights。&lt;/p&gt;
&lt;p&gt;　　首先说个题外话，ICLR的审稿流程真是可怕，reviewer和author来回好几轮，工作量足够赶上一个journal，没有金刚钻是不敢揽这个瓷器活的。DRL的圈子被Deepmind和OpenAI把持，外人想进去很难，不过现阶段用DRL去做一些其他应用的道路还是很宽的，无论是写paper还是工业界都有着很大的价值，值得follow。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img src=&quot;/images/imitation_learning_paper/iclr_imitation_learning_title.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;解决的问题：第三人称视角模仿学习&lt;br&gt;使用方法：GAN、RL (TRPO)&lt;br&gt;亮点：GAN与RL结合、模仿学习应用场景扩展&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="drl" scheme="http://xz3030.github.io/categories/drl/"/>
    
    
      <category term="drl" scheme="http://xz3030.github.io/tags/drl/"/>
    
  </entry>
  
</feed>
